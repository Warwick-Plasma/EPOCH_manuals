\documentclass[12pt,a4paper]{article}
\usepackage{url,graphicx,tabularx,array}
\usepackage[margin=.5in,nohead]{geometry}
\usepackage{color,verbatim,framed}
\usepackage{moreverb}
\usepackage{fancyvrb}
\usepackage{amsmath}
\definecolor{warwickdark}{cmyk}{1.0,0.6,0.0,0.06}
\definecolor{warwickmid}{cmyk}{0.69,0.34,0.0,0.0}
\definecolor{warwicklight}{cmyk}{1.0,0.0,0.0,0.0}
\definecolor{warwickred}{cmyk}{0.0,1.0,0.65,0.15}
\newcommand{\HRule}{\rule[0.3cm]{\linewidth}{0.5mm}}
\newcommand{\emphtext}{\color{warwickdark} \fontfamily{phv}\selectfont\Large\bf}
\newcommand{\inlinecode}[1]{{\color{warwickred} \bf\texttt{#1}}}
\newcommand{\code}[1]{{\texttt{#1}}}
\newcommand{\qtt}[1]{``{\code{#1}}''}
\newcommand{\inlineemph}[1]{{\color{warwicklight} \bf{#1}}}
\newcommand{\cemph}[1]{{\inlineemph{#1}}}
\newcommand{\EPOCH}{{\color{warwickdark}\fontfamily{phv}\selectfont{EPOCH}}}
% Caption before Label to fix strange problem with not putting in subsection
% numbers. DO NOT CHANGE.
\newcommand{\captionedimage}[3]
  {{\begin{figure}[hbt!]\centering\includegraphics{#1}\caption{#3}\label{#2}
    \end{figure}}}
\newcommand{\scaledcapimage}[4]
  {{\begin{figure}[hbt!]\centering\includegraphics[scale=#4]{#1}\caption{#3}
    \label{#2} \end{figure}}}
\definecolor{shadecolor}{cmyk}{0.2,0.05,0.0,0.0}
\setlength{\FrameRule}{0.6mm}

\newenvironment{lboxverbatim}[1]{
\setlength{\FrameSep}{0pt}
%\topsep=0ex\relax
\def\FrameCommand{\fboxsep=0pt \colorbox{shadecolor}}
\MakeFramed{\FrameRestore}
\vspace{-13.5pt}
\fvset{label=#1}
\boxverb
}{
\endboxverb
\vspace{-13.5pt}
\endMakeFramed
}
\newenvironment{boxverbatim}{\lboxverbatim{none}}{\endlboxverbatim}

\newenvironment{nbboxverbatim}[1]{
\noindent\minipage{\textwidth}
\setlength{\FrameSep}{0pt}
%\topsep=0ex\relax
\def\FrameCommand{\fboxsep=0pt \colorbox{shadecolor}}
\MakeFramed{\FrameRestore}
\fvset{label=#1}
\boxverb
}{
\endboxverb
\vspace{-13.5pt}
\endMakeFramed
\endminipage
\vspace{5pt}
}

\DefineVerbatimEnvironment{boxverb}{Verbatim}
  {frame=single,framerule=0.5mm,rulecolor=\color{warwickmid},
   formatcom=\color{black}}

\DefineVerbatimEnvironment{codedef}{Verbatim}
  {formatcom=\color{warwickred},fontsize=\Large,commandchars=\\\{\}}

\begin{document}
{
  \fontfamily{phv}\selectfont\input{./title.tex}
}
\fontfamily{garamond}\selectfont%
\tableofcontents%
\newpage%
\DefineShortVerb{\#}
\fvset{formatcom=\color{warwickred}}
\section{FAQs}

\subsection{What is {\EPOCH}?}
{\EPOCH} is a plasma physics simulation code which uses the Particle in Cell
(PIC) method. In this method, collections of physical particles are represented
using a smaller number of pseudoparticles, and the fields generated by the
motion of these pseudoparticles are calculated using a finite difference time
domain technique on an underlying grid of fixed spatial resolution. The forces
on the pseudoparticles due to the calculated fields are then used to update the
pseudoparticle velocities, and these velocities are then used to update the
pseudoparticle positions. This leads to a scheme which can reproduce the full
range of classical micro-scale behaviour of a collection of charged particles,
but shows statistical noise if one pseudoparticle represents many real
particles (which is common in real problems), and also shows artificial
smoothing of currents and fields due to the finite spatial resolution of the
underlying grid.\\

This approach can alternatively be thought of as the pseudoparticles being
Monte Carlo sampling points of the full (2N)D phase space of an (N)D
problem. This has significant memory advantages over (2N)D direct Vlasov
solvers at the expense of lower accuracy and higher noise.

\subsubsection{Features of {\EPOCH}}
\begin{itemize}
  \item MPI parallelised explicit 2nd order relativistic PIC code.
  \item Dynamic MPI load balancing option.
  \item MPI-IO based output, allowing restart on arbitrary number of processors.
  \item Data analysis and visualisation options include ITT IDL, LLNL VisIt
    and Mathworks MatLab.
\end{itemize}

\subsection{The origins of the code}
The {\EPOCH} family of PIC codes is based on the older code PSC by Hartmut Ruhl,
and retains almost the same core algorithm for the field updates and particle
advance routines. {\EPOCH} was written to add more modern features, and to
structure the code in such a way that future expansion of the code is made as
easy as possible.

\subsection{What normalisations are used in {\EPOCH}?}
Since the idea from the start was that {\EPOCH} would be used by a large number
of different users, and that it should be as easy as possible to ``plug in''
different modules from different people into a given copy of the code, it was
decided to write {\EPOCH} in SI units. There are a few places in the code where
some quantities are given in other units for convenience (for example charges
are specified in multiples of the electron charge), but the entire of the core
code is written in SI units.

\subsection{What are those \code{\_num} things doing everywhere}
Historically using the compiler auto-promotion of \code{REAL} to
\code{DOUBLE PRECISION} was unreliable, so {\EPOCH} uses kind tags to specify
the precision of the code. The \code{\_num} suffixes and the associated
definition of \code{REAL}s as \code{REAL(num)} are these kind tags in
operation. The \code{\_num} tags force numerical constants to match the
precision of the code preventing errors due to precision conversion. The
important thing is that all numerical constants should be tagged with an
\code{\_num} tag, and all \code{REAL}s should be defined as \code{REAL(num)}.

\subsection{I just want to use the code as a black box, or I'm just
  starting. How do I do that?}
See the section titled {\bf Getting started guides}.

\subsection{I've been through the getting started guides but want more detail
  on using the code. How do I do that?}
See the section titled {\bf {\EPOCH} for end users}.

\subsection{I can set up the code using the autoloaders, but I want more
  control. How do I do that?}
See the section titled {\bf {\EPOCH} for advanced end users}.

\subsection{I am an advanced user, but I want to set up the code so that less
  experienced users can use it. How do I do that?}
See the section titled {\bf Customising {\EPOCH}}.

\subsection{I want to develop an addition to {\EPOCH}. How do I do that?}
Basically read the entire of this manual. The second part of the manual details
the internals of the code but it expects you to already be familiar with all
the material in the first part of the manual.

\subsection{I want to have a full understanding of how {\EPOCH} works. How do I
  do that?}
You're insane. Seek professional help. If you really want to understand {\EPOCH}
in full, the only thing that I can recommend is that you should read all of
this manual and then read through the code. Most of it is commented.
\pagebreak

\section{Getting Started Guides}

\subsection{History of these guides}
These guides are based on material which was used as handouts at an {\EPOCH}
introductory workshop held in February 2010 at the University of Warwick. The
purpose of these guides is to quickly get a new user to the point of being able
to use {\EPOCH} without getting bogged down in the details of how the code
works. Some of the material covered in these getting started guides will also
appear in the body of the manual.

\subsection{Running {\EPOCH} and basic control of EPOCH1D}
When the code is run, the output is
\begin{lboxverbatim}{Command line output}
@@@@@@@@  @@@@@@      @@@@    @@@@@@@@  @@    @@    @@@@@@    @@@@@@
@@@@@@@@  @@@@@@      @@@@    @@@@@@@@  @@    @@    @@@@@@    @@@@@@
@@        @@    @@  @@    @@  @@        @@    @@       @@@    @@  @@
@@        @@    @@  @@    @@  @@        @@    @@       @@@    @@  @@
@@        @@    @@  @@    @@  @@        @@    @@       @@@    @@    @@
@@        @@    @@  @@    @@  @@        @@    @@       @@@    @@    @@
@@@@@@@@  @@@@@@    @@    @@  @@        @@@@@@@@       @@@    @@    @@
@@@@@@@@  @@@@@@    @@    @@  @@        @@@@@@@@       @@@    @@    @@
@@        @@        @@    @@  @@        @@    @@       @@@    @@    @@
@@        @@        @@    @@  @@        @@    @@       @@@    @@    @@
@@        @@        @@    @@  @@        @@    @@       @@@    @@  @@
@@        @@        @@    @@  @@        @@    @@       @@@    @@  @@
@@@@@@@@  @@          @@@@    @@@@@@@@  @@    @@    @@@@@@@@  @@@@@@
@@@@@@@@  @@          @@@@    @@@@@@@@  @@    @@    @@@@@@@@  @@@@@@

Welcome to EPOCH1D Version 2.1

 The code was compiled with the following compile time options
 *************************************************************
 Per particle weighting -DPER_PARTICLE_WEIGHT
 Tracer particle support -DTRACER_PARTICLES
 Particle probe support -DPARTICLE_PROBES
 Particle ionisation model -DPARTICLE_IONISE
 *************************************************************

 Specify output directory
\end{lboxverbatim}

At which point the end user should simply type in the name of the directory
where the code output is to be placed. This directory must also include the
file \qtt{input.deck} which controls the code setup, specifies how to set the
initial conditions and controls the I/O. Writing an input deck for {\EPOCH} is
fairly time consuming and so the code is supplied with an example input deck
which includes all the necessary sections for the code to run. This section of
the manual describes how to set up the basic code and a few simple problems in
EPOCH1D.

\subsection{\code{input.deck}}
The input deck file resides in the output directory and contains all the basic
information which is needed to set up the code, including the size and
subdivision of the domain, the boundary conditions, the species of particles to
simulate and the output settings for the code.

\subsubsection{\cemph{control} block}
The \cemph{control} block sets up the basic code properties for the
domain, the end time of the code, the load balancer and the types of initial
conditions to use.

\begin{lboxverbatim}{control block}
begin:control
   nx=2000 # in x
   npart=200*200*40

   nsteps=-1

   t_end=0.3e-12

   x_min=-5e-6
   x_max=5e-6

   dt_multiplier=0.8

   dlb_threshold=1.0

   # restart_snapshot=98
   neutral_background=T
end:control
\end{lboxverbatim}

As illustrated in the above code block, the ``\#'' symbol is treated as a
comment character and the code ignores everything on a line following this
character.\\

{\emphtext nx} - Number of grid points in the x direction. There must be
sufficient gridpoints so that (x\_max-x\_min)/nx $< \lambda_d$, where
$\lambda_d$ is the Debye length.\\

{\emphtext npart} - The global number of pseudoparticles in the
simulation. This parameter does not need to be given if a specific number
of particles is supplied for each particle species. Specifying both will just
use the number of particles supplied for each species.\\

{\emphtext nsteps} - The number of iterations of the core solver before the
code terminates. Negative numbers instruct the code to only terminate at
\inlineemph{t\_end}.\\

{\emphtext t\_end} - The final simulation time in simulation seconds before the
code terminates.\\

{\emphtext NOTE: The code will terminate if EITHER of the above conditions on
nsteps or t\_end is satisfied.}\\

{\emphtext x\_min} - The position of the left hand edge of the domain in
metres. Can be negative. ``x\_start'' is accepted as a synonym.\\

{\emphtext x\_max} - The position of the right hand edge of the domain in
metres. Must be greater than \inlineemph{x\_min}.
``x\_end'' is accepted as a synonym.\\

{\emphtext dt\_multiplier} - Factor by which the timestep is multiplied before
it is applied in the code. Must be less than one.\\

{\emphtext dlb\_threshold} - Number indicating the minimum ratio between the
load on the least loaded processor to the most loaded processor. Set to 1 means
always balance, set to 0 means never balance. If this parameter is not
specified then the code will only be load balanced at initialisation time.\\

{\emphtext restart\_snapshot} - The number of a previously written restart
dump to restart the code from.\\

{\emphtext neutral\_background} - Logical flag indicating whether to
solve Poisson's equation at t=0 or assume that there is a neutralising
background field. If ``T'' then Poisson's equation is solved.\\

\subsubsection{\inlineemph{boundaries} block}
The {\emphtext boundaries} block sets the boundary conditions of each boundary
of the domain. Some types of boundaries allow EM wave sources (lasers) to be
attached to a boundary. Lasers are attached at the initial conditions
stage.

\begin{lboxverbatim}{boundaries block}
begin:boundaries
   bc_x_min=simple_laser
   bc_x_max=simple_outflow
end:boundaries
\end{lboxverbatim}

{\emphtext bc\_x\_min} - The condition for the left boundary.
``xbc\_left'' is accepted as a synonym.\\

{\emphtext bc\_x\_max} - The condition for the right boundary.
``xbc\_right'' is accepted as a synonym.\\

There are four boundary types in {\EPOCH} and each boundary of the domain can
have one and only one of these boundaries attached to it. These boundary types
are:\\

{\emphtext periodic} - A simple periodic boundary condition. Fields and
particles reaching one edge of the domain are wrapped round to the opposite
boundary.\\

{\emphtext other} - A generic boundary condition which in the default {\EPOCH}
version is perfectly reflecting for both particles and EM waves.\\

{\emphtext simple\_laser} - A characteristic based boundary condition to which
one or more EM wave sources can be attached. Since the wave sources require the
domain to be completely set up before they can be calculated the wave sources
are attached when the initial conditions are set.  EM waves impinging on a
\inlineemph{simple\_laser} boundary are transmitted with as little reflection
as possible. Particles are fully transmitted. The field boundary condition
works by allowing outflowing characteristics to propagate through the boundary
while using the attached lasers to specify the inflowing characteristics. The
particles are simply removed from the simulation when they reach the
boundary.\\

{\emphtext simple\_outflow} - A simplified version of \inlineemph{simple\_laser}
which has the same properties of transmitting incident waves and
particles, but which cannot have EM wave sources attached to it. These
boundaries are about 5\% more computationally efficient than
\inlineemph{simple\_laser boundaries} with no attached sources. This boundary
condition again allows outflowing characteristics to flow unchanged, but this
time the inflowing characteristics are set to zero. The particles are again
simply removed from the simulation when they reach the boundary.\\

{\emphtext NOTE: If either simple\_laser or simple\_outflow are specified on
one or more boundaries then the code will no longer necessarily conserve mass.}

\subsubsection{\inlineemph{species} block}

\begin{lboxverbatim}{species block}
begin:species
   n_species=2

   # electrons
   charge1=-1.0
   mass1=1.0
   npart1=2000*100
   frac1=0.5
   name1=Electron
   dump1=T

   # carbon4+
   charge2=4.0
   mass2=1836.0*12
   npart2=2000*100
   frac2=0.5
   name2=Carbon
   dump2=T
end:species
\end{lboxverbatim}

{\emphtext n\_species} - This MUST be the first entry in the species block and
specifies the number of species in the simulation. After this line, the
properties of any numbered species may be set in any order. The species are
numbered from 1 to \inlineemph{n\_species}.\\

{\emphtext charge\inlinecode{n}} - This sets the charge of species n in
multiples of the electron charge. Negative numbers are used for negatively
charged particles.\\

{\emphtext mass\inlinecode{n}} - This sets the mass of species n in multiples
of the electron mass. Negative numbers are not trapped and effectively swap the
charge of the species. This is not recommended since it breaks the
mass\_density diagnostic.\\

{\emphtext npart\inlinecode{n}} - This specifies the number of pseudoparticles
which should be loaded into the simulation domain for species n. This is the
most convenient way of loading particles for simulations containing multiple
species with different number densities. If these are specified then
\inlineemph{npart} (the global number of particles specified in the control
block) is ignored for this species. It should not be specified at the same time
as fracn for a given species.\\

{\emphtext frac\inlinecode{n}} - This specifies what fraction of npart (the
global number of particles specified in the control block) should be assigned
to species n. \\

{\emphtext NOTE: fracn should not be specified at the same time as npartn for a
given species.}\\

{\emphtext name\inlinecode{n}} - This specifies the name of particle species
n. This name can include any alphanumeric characters in the basic ASCII
set. The name is used to identify the species in any species specific output
information. \\

{\emphtext NOTE: DO NOT SET TWO SPECIES WITH THE SAME NAME!}\\

{\emphtext dump\inlinecode{n}} - Logical flag detailing whether or not to dump
the information about species n. If set to ``F'', then the species information
is not dumped when writing ANY species specific diagnostics, although it is
included in global diagnostics (i.e in the example if ``dump2=F'' were specified
then there would be no ``mass\_density\_carbon'', but the mass of the carbon
would be considered when calculating ``mass\_density'') and restart dumps.\\

{\emphtext tracer\inlinecode{n}} - Logical flag switching particle species n
into tracer particles. Tracer particles are enabled with the correct
precompiler option, and when set for a given species make that species move
correctly for its charge and mass, but contribute no current. This means that
these particles are passive tracers in the plasma.\\

\subsubsection{\inlineemph{output} block}
Output in {\EPOCH} is handled using the custom designed CFD (\inlineemph{Compact
Flexible Datatype}) self defining file format which will be covered in more
detail in a later set of notes and comes with readers for ITT IDL and LLNL
VisIt, with additional readers for Mathworks Matlab and Yorick under
development. What the code should output and when it should output it is
specified in the ``output'' block of the input deck which will be covered over
the next few pages, but a quick note should be made on the types of output and
on how to specify when a given variable should be dumped.\\

There are three types of output dump in {\EPOCH} which are used for different
purposes. These types are:\\

{\emphtext normal} - The most frequent type of output dump in {\EPOCH} is a
normal dump. Only the variables required for science output should be written
at a normal dump.\\

{\emphtext full} - A full dump is usually written every 10 or so normal
dumps. A full dump contains all the data that a normal dump contains and should
also contain any information which is needed only infrequently, whether this is
the full particle information or a large distribution function. It is possible
to turn off full dumps completely.\\

{\emphtext restart} - A restart dump is a dump where the code guarantees to
write enough data to allow the code to restart from the output. Output dumps
are guaranteed to contain all the information in a normal dump and, if they
coincide with the timing for a full dump, will also contain the full dump
information.\\

Information will never be written into a file twice, even if two conditions for
it being written are satisfied (i.e even if px should be dumped both because it
is a full dump and a restart dump, px will only be written once).\\

When specifying which type of output dump to write a variable at there are four
options which are specified for each variable and can be combined by
addition. Obviously some combinations make no sense but are formally
valid. These are:\\

{\emphtext never} - If the variable is not a required restart variable then it
will never be written. If it is a required restart variable then it will be
written only at restart dumps.\\

{\emphtext always} - This variable will be written at full, normal and restart
dumps.\\

{\emphtext full} - This variable will be written at full dumps only.\\

{\emphtext restart} - This variable will be written at restart dumps only.\\

There is also a fifth parameter which can be specified for some variables.\\

{\emphtext species} - The output for this variable should be broken down on a
species by species basis. This only applies to certain kinds of derived field
variable, such as mass\_density. It is combined with a restart frequency code
by addition as in: \inlinecode{px = always + species}.\\

When applied to a variable, these codes are referred to as a
\inlineemph{dumpmask}.\\

\begin{lboxverbatim}{output block}
begin:output
   # If use_offset_grid is true then the code dumps a grid which
   # displays positions relative to the left hand edge of the window
   use_offset_grid=F
   # number of timesteps between output dumps
   dt_snapshot=1.0e-14
   # Number of dt_snapshot between full dumps
   full_dump_every=10
   restart_dump_every=-1
   force_final_to_be_restartable=T

   # Properties at particle positions
   particles=never
   px=never
   py=never
   pz=never
   vx=never
   vy=never
   vz=never
   charge=never
   mass=never
   particle_weight=never
   species_id=never

   # Properties on grid
   grid=always
   ex=always
   ey=always
   ez=always
   bx=always
   by=always
   bz=always
   jx=always
   jy=always
   jz=never
   ekbar=always+species
   mass_density=never+species
   charge_density=always
   number_density=always+species
   temperature=always+species

   distribution_functions=always
   particle_probes=never
end:output
\end{lboxverbatim}

{\emphtext use\_offset\_grid} - When using moving windows some visualisation
programs (notably VisIt) show the motion of the window by moving the
visualisation window rather than by changing the x-axis. Setting this option to
``T'' causes the code to write another grid which always gives the offset
relative to the left hand edge of the window rather than the true origin.
Performs no function when not using the moving window.\\

{\emphtext dt\_snapshot} - Sets the interval between normal output dumps in
simulation seconds. Setting zero or negative means that the code will output
every step of the core solver. The code does NOT guarantee that outputs will be
exactly \inlineemph{dt\_snapshot} apart, what is guaranteed is that the next
output will be after the first iteration which takes the simulation to a time
$\ge$ \inlineemph{dt\_snapshot} from the last output.\\

{\emphtext full\_dump\_every} - The number of normal output dumps between full
output dumps.\\

{\emphtext restart\_dump\_every} - The number of normal output dumps between
restart dumps. Setting to zero makes every dump a restart dump, setting
negative sets the code to not produce restart dumps.\\

{\emphtext force\_final\_to\_be\_restartable} - Force the code to override
other output settings and make the last output dump it writes be a restart
dump. Any internal condition which causes the code to terminate will make the
code write a restart dump, but code crashes or scheduler terminations will not
cause the code to write a restart dump.\\

{\emphtext particles} - The dumpmask for the particle positions. Restart
variable. No particle variables can be plotted in VisIt unless this is
dumped.\\

{\emphtext p\{x,y,z\}} - The dumpmasks for the particle momenta. Restart
variable.\\

{\emphtext v\{x,y,z\}} - The dumpmasks for the particle velocities.\\

{\emphtext charge} - The dumpmask for the charge of a given particle. This
has no effect if the code is not compiled with the option for per particle
charge.\\

{\emphtext mass} - The dumpmask for the mass of a given particles. This
has no effect if the code is not compiled with the option for particle mass.\\

{\emphtext particle\_weight} - The dumpmask for the weighting function which
describes how many real particles each pseudoparticle represents. Restart
variable.\\

{\emphtext species\_id} - The dump mask for the number representing which
particle species a given particle is. This is the same as the number assigned
to that particle species in the species block. Restart variable.\\

{\emphtext grid} - The dumpmask for the Cartesian grid which defines the
locations of the grid variables. No grid variables can be plotted in VisIt
unless this variable is output.\\

{\emphtext e\{x,y,z\}} - The electric field vectors pointing in all three
directions. Restart variables.\\

{\emphtext b\{x,y,z\}} - The magnetic field vectors pointing in all three
directions. Restart variables. In 1D bx is a trivial variable because of the
Solenoidal condition. It is included simply for symmetry with higher dimension
codes.\\

{\emphtext j\{x,y,z\}} - The currents pointing in all three directions. Restart
variables.\\

{\emphtext ekbar} - Mean kinetic energy on grid. Can have species dumpmask.\\

{\emphtext mass\_density} - Mass density on grid. Can have species dumpmask.\\

{\emphtext charge\_density} - Charge density on grid. Can have species
dumpmask.\\

{\emphtext number\_density} - Number density on grid. Can have species
dumpmask.\\

{\emphtext temperature} - Temperature on grid. Can have species dumpmask. The
exact way that temperature is calculated in the code is likely to vary in the
near future.\\

{\emphtext distribution\_function} - Dumpmask for outputting distribution
functions specified in the input deck. Each individual distribution function
can have its own dumpmask, but this one is more restrictive.\\

\subsection{External initial conditions}

The initial conditions in {\EPOCH} can be specified in various ways, but the
easiest way is to specify the initial conditions in the input deck file. This
allows any initial condition which can be specified everywhere in space by a
number density and a drifting Maxwellian distribution function. This section
details the blocks that can be set to set up the initial conditions.

\subsubsection{\inlineemph{constants} and \inlineemph{DEO} blocks}

\begin{lboxverbatim}{constant block}
begin:constant
   lambda=1.0e-6 # 1 micron wavelength
   omega=2.0*pi*c/lambda
   den_crit=critical(omega)
   scale=3.5e-6 # microns
   den_max=5.0*den_crit
   thick=300e-9
   pplength=6000e-9
   widscale=5.0e-6

   t_wid=(10.0e-6)/c
   amax=1.0
   wy=1e-6
   y=0.0
end:constant
\end{lboxverbatim}

\begin{lboxverbatim}{deo block}
begin:deo
   slope=exp(-2.0*(y/wy)^2)
   blob=gauss(sqrt(x^2+y^2),0.0,1.0e-6)
end:deo
\end{lboxverbatim}

The initial conditions deck in {\EPOCH} is set up, using the normal maths
expressions, by setting the density and temperature for each species which is
then used by the autoloader to actually position the particles.\\

Before moving on to the blocks which actually set up the initial conditions
there are other helpful blocks which should be explained. These are called
\inlineemph{constants} and \inlineemph{DEO}s which allow you to define
constants and maths parser expressions which can be used by name later in the
deck.\\

Constant and DEO blocks can be used anywhere in the input deck, and the
assigned values and expressions carry over to other deck files, allowing
constants defined in the main input deck to be used in the initial conditions
etc. Constants are simply maths parser expressions which are evaluated when
defined and assigned to a name as shown in the panel to the left. When the name
is used on the right hand side of a deck expression, it is simply replaced with
the numerical value calculated when the constant is created.  If a constant
name is reused in a constant block then the old constant is deleted and
replaced with the new one to conserve memory. This happens without warning.\\

However, since constants are created once and never change, they cannot be used
for spatially varying quantities, and this is where DEOs come in. DEO is short
for deferred execution object and as the name implies, a DEO is a deck
expression that is not evaluated when it is created, but rather when it is used
on the right hand side of a deck expression. This allows DEOs to contain
spatially varying information without having to pre-calculate them at every
location in the domain. However, DEOs require significantly more memory than
constants and should not be used unless needed. As with constants, when a DEO
name is reused in a DEO block the original DEO is deleted and replaced with the
new DEO. When created, DEOs are given a basic sanity test and should cause the
code to exit if they are invalid, but for sufficiently subtle errors, this is
not guaranteed. Use DEOs with caution and test very carefully if errors
appear.\\

Using constants and DEOs can be very helpful when dealing with long,
complicated expressions since they allow the expression to be broken down into
much simpler parts. They can also be used to get around the Fortran string
length limitation built into many compilers which prevents deck lines being
longer then 512 characters long. As a general rule, it is a good idea to break
down complicated expressions, either using constants and DEOs or by other
means, in order to make the deck look more readable.\\

Both constants and DEOs are persistent for the entire runtime of the code,
allowing them to be used when specifying time profiles for lasers, and also
allowing developers to use maths parser expressions for other internal parts of
the code where needed.

\subsubsection{\inlineemph{laser} blocks}
Laser blocks attach an EM wave source to a boundary which is set as
\inlineemph{simple\_laser}.

\begin{lboxverbatim}{laser block}
begin:laser
   boundary=x_min
   amp=3.222499e13
   irradiance=1.0e19
   id=1
   freq=omega
   pol_angle=0.0
   phase=0.0
   t_profile=gauss(time,40.0e-15,40.0e-15)
   t_start=0.0
   t_end=80.0e-15
end:laser
\end{lboxverbatim}

As already mentioned in the discussion of laser boundaries in the boundaries
block, lasers are attached to compatible boundaries here in the initial
conditions deck. In 1D, laser blocks are slightly simpler than their 2D and 3D
equivalents, but most of the important features will be mentioned here. The
only significant difference is that there is also a spatial profile for
multidimensional lasers. This is covered in detail in the section on
multidimensional {\EPOCH}.\\

{\emphtext boundary} - The boundary on which to attach the laser.
In 1D, the directions can be either x\_min or x\_max.  ``left'' and ``right''
are accepted as a synonyms for ``x\_min'' and ``x\_max''.\\

{\emphtext amp} - The amplitude of the E field of the laser.\\

{\emphtext irradiance} - The irradiance (intensity) of the laser in $W/m^2$. It
may sometimes be more convenient to use either irradiance or amp but the two
are equivalent in effect.\\

{\emphtext id} - An id code for the laser. Used if you specify the laser time
profile in the {\EPOCH} source rather than in the input deck. Does not have to
be unique, but all lasers with the same id will have the same time profile.\\

{\emphtext freq} - Angular frequency (rad/s not Hz) for the laser.\\

{\emphtext pol\_angle} - Polarisation angle for the laser in radians.\\

{\emphtext phase} - phase shift for the laser in radians.\\

{\emphtext t\_profile} - Used to specify the time profile for the laser. Should
use the parser variable time to specify the time profile. Since this is applied
after the laser amplitude, it should run between 0 and 1. Setting values
greater than 1 is possible but will cause the maximum laser intensity to grow
beyond amp.\\

{\emphtext t\_start} - Start time for the laser in seconds. Can be set to the
string ``start'' to start at the beginning of the simulation. When using this
parameter, the laser start is hard. To get a soft start use the
\inlineemph{t\_profile} parameter to ramp the laser up to full strength.\\

{\emphtext t\_end} - End time for the laser in seconds, can be set to the
string ``end'' to end at the end of the simulation. When using this parameter,
the laser end is clipped straight to zero at t$>$\inlineemph{t\_end}. To get a
soft end use the \inlineemph{t\_profile} parameter to ramp the laser down to
zero.\\

{\emphtext NOTE: If amp and irradiance are both specified in the same block
then the last set of the two will take precedence.}\\

The laser time profiles specified in the laser input block represent only the
temporal envelope for the wave. The wave itself is introduced as a sinusoidal
variation using the frequency specified in the laser block.

In theory, any laser time profile required is possible, but the core FDTD
solver for the EM fields in {\EPOCH} produces spurious results if sudden
changes in the field intensity occur. This is shown in Figures~\ref{badpulse}
and \ref{smoothpulse}. The pulse shown in Figure~\ref{badpulse} used a constant
\inlineemph{t\_profile} and used \inlineemph{t\_end} to stop the laser after
8fs. Since the stopping time was not an exact multiple of the period, the
result was to introduce spurious oscillations behind the pulse. If the laser
had a finite phase shift so that the amplitude did not start at zero, a
similar effect would be observed on the front of the pulse.

\captionedimage{./images/pulse2}{badpulse}{A laser pulse with a sharp
  cutoff shows numerical artifacts behind the pulse.}
\captionedimage{./images/pulse1}{smoothpulse}{A laser pulse with a smooth
  temporal profile shows no artifacts.}

Figure~\ref{smoothpulse} instead used a Gaussian window function with a
characteristic width of 8fs as well as using \inlineemph{t\_end} to introduce
a hard cutoff. It can clearly be seen that there are no spurious oscillations
and the wave packet propagates correctly, showing only some dispersive
features.

There is no hard and fast rule as to how rapid the rise or fall for a laser can
be, and the best advice is to simply test the problem and see whether any
problems occur. If they do then there are various solutions. Essentially, the
timestep must be reduced to the point where the sharp change in amplitude can
be accommodated. The best solution for this is to increase the spatial
resolution (with a comparable increase in the number of pseudoparticles), thus
causing the timestep to drop via the CFL condition.

This is computationally expensive, and so a cheaper option is simply to
decrease the input.deck option \inlineemph{dt\_multiplier}. This artificially
decreases the timestep below the timestep calculated from the internal
stability criteria and allows the resolution of sharp temporal gradients. This
is an inferior solution since the FDTD scheme has increased error as the
timestep is reduced from that for EM waves. The latest version of {\EPOCH}
includes a high order field solver to attempt to reduce this.

\subsubsection{\inlineemph{species\inlinecode{n}} block}

\begin{lboxverbatim}{species{\it n} block}
begin:species2
   rho=if(abs(x) lt thick,den_max,0.0)
   rho=if((x gt -thick) and (abs(y) gt 2e-6),0.0,rho(2))
   temp_x=0.0
   temp_y=temp_x(1)
   minrho=0.1 * den_max
end:species2

begin:species1
   rho=4.0*rho(2)
   temp_x=temp_x(1)
   temp_y=temp_x(1)
   minrho=0.1 * den_max
end:species1
\end{lboxverbatim}

The actual initial conditions are specified in one or more blocks called
species blocks. There should be one of these blocks for each species which is
to have its initial conditions set from the external initial conditions.\\

Each block is started with a\\

{\emphtext begin:species\inlinecode{n}}\\
\\
line, and ends with an associated\\

{\emphtext end:species\inlinecode{n}}\\
\\
line. The species number n is the same species number which is assigned
properties in the species block of input.deck. So if you assigned species 1 the
properties of an electron then the initial conditions given in the block
species1 would be the initial conditions used to set up the electrons. Note
that as in the example above the species do not have to be specified in any
given numerical order. The above example positions carbon 4+ ions (species2)
and then positions four times the number of electrons with the same
profile. The elements of the species block are:\\

{\emphtext rho} - Particle number density in $m^{-3}$. Despite the name, this
is NOT the mass density. As soon as a rho= line has been parsed, the values are
calculated for the whole domain and are available for reuse on the right hand
side of an expression. This is seen in the above example in the first two lines
for species2, where the density is first set and then corrected.\\

{\emphtext temp\_\{x,y,z\}} - The temperature in each direction for a thermal
distribution in K. To specify a temperature in ev, simply use the deck
parser. So for example temp\_x=4*kev*kb gives a 4kev temperature.\\

{\emphtext temp} - Sets an isotropic temperature distribution in K. Does not
give thermal distribution in ignorable directions. If both temp and a specific
temp\_x, temp\_y, temp\_z parameter is specified then the last to appear in the
deck has precedence.\\

{\emphtext minrho} - Minimum particle number density in $m^{-3}$. When the
number density in a cell falls below minrho the autoloader does not load any
pseudoparticles into that cell to minimize the number of low weight,
unimportant particles. If set to 0 then all cells are loaded with particles.\\

{\emphtext maxrho} - Maximum particle number density in $m^{-3}$. When the
number density in a cell rises above maxrho the autoloader clips the density to
maxrho allowing easy implementation of exponential rises to plateaus.\\

{\emphtext drift\_\{x,y,z\}} - Specifies a momentum space offset in kgms-2 to
the distribution function for this species. At present the drift\_\{x,y,z\}
parameter CANNOT vary in space and is just a single constant.

\subsubsection{\inlineemph{species\_external\inlinecode{n}} block}
\begin{lboxverbatim}{species\_external{\it n} block}
begin:species_external2
   rho=Data/ic.dat
   offset=80000
   temp_x=Data/ic.dat
end:species_external2
\end{lboxverbatim}

The penultimate type of block in the initial conditions deck is the
species\_external\inlinecode{n} block. It is very similar to the ic.deck
\inlineemph{species\inlinecode{n}} block, but instead of specifying the
initial conditions mathematically in the input deck, you specify the filename
of a simple binary file containing the information required. The use of this
block is very simple and is shown in the example above.\\

All the elements which exist in the simple \inlineemph{species\inlinecode{n}}
block also exist in the \inlineemph{species\_external\inlinecode{n}} block,
and the value on the right hand side should be the filename which holds the
data wanted. An additional element is also introduced, the offset element which
is the offset in bytes from which the start of the file where the data should
be read from. As a given line in the block executes, the file is opened, the
file handle is moved to the point specified by the offset parameter, the data
is read and the file is then closed. Therefore, unless the offset value is
changed between data reading lines the same data will be read into all the
variables. The data is read in as soon as a line is executed, and so it is
perfectly possible to load data from a file with the
\inlineemph{species\_external\inlinecode{n}} block and then modify the data
with a \inlineemph{species\inlinecode{n}} block.\\

The file should be a simple binary file consisting of floating point numbers of
the same precision as \inlinecode{\_num} in the core {\EPOCH} code.\\

{\emphtext NOTE: The files that are expected by this block are SIMPLE BINARY
files, NOT Fortran unformatted files. It is possible to read Fortran
unformatted files using the offset element, but care must be taken!}\\

\subsubsection{\inlineemph{fields} block}
\begin{lboxverbatim}{fields block}
begin:fields
   ex=sin(pi*x/length_x)
   ey=cos(pi*x/length_x)
   ez=0
   bx=1.0
   by=-1.0
   bz=0
end:fields
\end{lboxverbatim}

The final type of block in the {\EPOCH} input deck is the fields block. This
allows you to specify the electric and magnetic fields at any point in the
domain. Once again, this is a very simple block needing only limited
explanation. All field variables are accessible by name and can be read back
using the appropriate commands from the maths parser. \\

Any valid maths parser expression can be used to set up the fields, and no
check is made to ensure that the solenoidal condition is satisfied (as can be
seen in the above example).\\

\subsection{Moving to EPOCH2D and EPOCH3D}
EPOCH1D is the simplest version of {\EPOCH} and, although feature complete,
lacks many of the elements which are required for multidimensional operation.
These elements are mainly small modifications to the concepts already
introduced in EPOCH1D but are significant enough to need at least some
discussion.\\

The higher dimension versions of the code go by the uninspired names of EPOCH2D
and EPOCH3D. Most of the input decks are equivalent to those already covered in
EPOCH1D, with a few added elements for setting the number of grid points in the
additional dimensions, setting the start and end points for the axes in the new
dimensions, and setting boundary conditions on the new boundaries. These will
be covered in more detail over the next few pages.\\

The laser blocks become slightly more complicated by the addition of spatial
profiles for the laser front, and the addition of the ability to have spatially
dependent phase profiles across the laser front. This will be covered later in
this document.\\

The data loading routines are identical to those for EPOCH1D, although
obviously new routines are required to actually visualise 2D and 3D data. 2D
and 3D data is much larger than 1D, and a much larger number of particles are
required to adequately resolve the physics in multidimensional simulations. As
a consequence, the data files generated by higher dimensionality versions of
{\EPOCH} will be much larger, making the more advanced I/O routines available
in {\EPOCH} more useful. These will be covered in the following sections.\\

\subsubsection{Changes to the \inlineemph{control} block}

\begin{lboxverbatim}{Changed control block}
begin:control
   .
   .
   ny=200
   nz=200
   .
   y_min=-10e-6
   y_max=10e-6
   z_min=-10e-6
   z_max=10e-6
   .
   .
end:control
\end{lboxverbatim}

The modified control block in EPOCH2D and 3D is very similar to the control
block already introduced for EPOCH1D. All that is added are new elements
describing the number of gridpoints in y and z (\inlineemph{ny \& nz}), and
new start and end lines for the length of the domain in the new directions
(\inlineemph{y\_min $\rightarrow$ y\_max} \&
\inlineemph{z\_min $\rightarrow$ z\_max}). These operate in exactly the same
way as those already introduced with EPOCH1D.  However, care must be taken to
increase the number of particles, ensuring that the number of particles per
cell remains high enough to accurately resolve the distribution function.\\

Also note that the amount of memory required for multidimensional simulations
will generally be hundreds of times larger than that required for 1D
simulations.\\

\subsubsection{Changes to the \inlineemph{boundaries} block}

\begin{lboxverbatim}{Changed boundaries block}
begin:boundaries
   .
   .
   bc_y_min=periodic
   bc_y_max=periodic
   bc_z_min=simple_laser
   bc_z_max=simple_outflow
end:boundaries
\end{lboxverbatim}

The modified boundary block includes new boundary conditions for the additional
boundaries that are introduced in higher dimensions. These available boundaries
are exactly the same as in 1D, with the additional boundaries being:\\

\begin{tabular}{rcl}
\inlineemph{bc\_y\_min} &-& bottom of domain. ``ybc\_down'' is also accepted.\\
\inlineemph{bc\_y\_max} &-& top of domain. ``ybc\_up'' is also accepted.\\
\inlineemph{bc\_z\_min} &-& back of domain. ``zbc\_back'' is also accepted.\\
\inlineemph{bc\_z\_max} &-& front of domain. ``zbc\_front'' is also accepted.\\
\end{tabular} \\

These are equivalent to the existing 1D simulations except that the moving
window always moves parallel to the x-axis when activated.\\
This concludes all
the modifications to input.deck that are required to use {\EPOCH} in multiple
dimensions. There are some changes that are required to allow
multidimensional initial conditions, but a surprising number of initial
conditions are in fact 1D even when running in multi-dimensional versions of
{\EPOCH}. The next few pages cover the modifications to the initial conditions
file which are needed for true multidimensional initial conditions with
examples.

\subsubsection{Multidimensional initial conditions}

The basic modification to the {\EPOCH} initial conditions deck is the addition
of the new variables describing positions and length in the new directions. The
names of these variables are given in the full manual and will be used without
introduction here.

\scaledcapimage{./images/gaussic}{gaussblob}{A Gaussian density blob at the
  centre of the domain.}{0.4}

A very simple problem to set up using EPOCH2D is a Gaussian blob which is
shown in Figure~\ref{gaussblob}, in the form both of the ic.deck needed to
create it and the output from VisIt that is produced from running this deck.

\scaledcapimage{./images/invgaussic}{inversegaussblob}{A Gaussian density
  deficit at the centre of the domain.}{0.4}

In Figure~\ref{inversegaussblob}, a very small modification to this initial
conditions deck is shown which creates the a case with a Gaussian density
minimum at the centre. This makes it clear that multidimensional initial
conditions are only a little more complex than 1D initial conditions, but at
present these are not very interesting initial conditions.

The next example extends this to a more useful initial condition.\\

\begin{nbboxverbatim}{species block to set up Gaussian density blob}
begin:constant
   width=2.5e-6 # 2.5 microns
end:constant

begin:deo
   r=sqrt(x^2+y^2)
end:deo

begin:species1
   rho=1.0e19 * gauss(r,0.0,width)
end:species1
\end{nbboxverbatim}
%
%\begin{minipage}{\textwidth}
\begin{lboxverbatim}{species block to set up Gaussian density blob}
begin:constant
   width=2.5e-6 # 2.5 microns
end:constant

begin:deo
   r=sqrt(x^2+y^2)
end:deo

begin:species1
   rho=1.0e19 * (1.0-gauss(r,0.0,width))
end:species1
\end{lboxverbatim}
%\end{minipage}

\subsubsection{\inlineemph{laser} blocks in multiple dimensions.}

\scaledcapimage{./images/profile_flat}{flatlaser}{Constant laser profile}{0.4}
\scaledcapimage{./images/profile_gauss}{gausslaser}{Gaussian laser profile}{0.4}
\scaledcapimage{./images/profile_diff_gauss}{diffgausslaser}{Differential
  Gaussian laser profile}{0.4}

The laser block already introduced in the EPOCH1D handouts are still present in
2D and 3D, but have one additional feature and a modification to one existing
feature. The new feature is \inlineemph{profile} which is defined by:\\

{\emphtext profile} - The spatial profile for the laser. This is
essentially an array defined along the edge (or surface) that the laser is
attached to. It is clear that the spatial profile is only meaningful
perpendicular to the laser's direction of travel and so it simply does not
exist in 1D. The laser profile is evaluated while the initial conditions deck
is evaluated and so cannot include any temporal information which must be
encoded in \inlineemph{t\_profile}.  The spatial profile is evaluated at the
boundary where the laser is attached and so only spatial information in the
plane of the boundary is significant. This is most clearly explained through a
few examples. In these examples the spatial profile of the laser is set to vary
between a flat uniform profile (\inlineemph{profile=1}), a Gaussian profile in
y (\inlineemph{profile=gauss(y,0,2.5e-6)}) and a differentiated Gaussian
profile in y (\inlineemph{profile=(y/2.5e-6) * gauss(y,0,2.5e-6)}). The
difference between these profiles is obvious but the important point is that a
laser travelling parallel to the x-direction has a profile in the y
direction. Similarly a laser propagating in the y-direction has a profile in
the x direction. In 3D this is extended so that a laser propagating in a
specified direction has a profile in both orthogonal directions. So a laser
travelling parallel to the x axis in 3D would have a profile in y and z. Since
3D lasers are very similar to 2D lasers, they will not be considered here in
greater detail, but in 3D, it is possible to freely specify the laser profile
across the entire face where a laser is attached.\\

The modified parameter in
multidimensional lasers is the phase parameter which is also modified to
be spatially dependent. This allows a user to add a laser travelling at an
angle to a boundary although no examples of this will be given. It is not
currently possible to automatically link lasers on two boundaries which
would be required to allow a laser to propagate in from a corner.\\

\subsubsection{Distribution functions}
%
\begin{nbboxverbatim}{dist\_fn block}
begin:dist_fn
   name=x_px
   ndims=2
   dumpmask=always

   direction1=dir_x
   direction2=dir_px

   # range is ignored for spatial coordinates
   range1=(1,1)
   range2=(-50.0e-20,50.0e-20)

   # resolution is ignored for spatial coordinates
   resolution1=1
   resolution2=5000

   include_species_1=F
   include_species_2=T
   include_species_3=T
end:dist_fn
\end{nbboxverbatim}

One way of considering the PIC methodology is based on the idea of using the
simulation pseudoparticles as Monte-Carlo sampling points of the phase space
of Vlasov's equation. While direct simulation of Vlasov's equation leads to
much lower noise than using this Monte-Carlo approach it is much more
computationally expensive and is currently only just possible in 2D and
completely impossible in 3D. However, sometimes it is useful to be able to
reconstruct at least some of the phase space for one or more particle species,
and this option is provided through a \inlineemph{dist\_fn} block. The
distribution function is integrated over all dimensions which are not axes of
the distribution function.\\

This block allows the user to specify information about
setting up additional diagnostics including phase space reconstructions. It is
possible to set up as many 2D and 3D distribution functions as required by
simply specifying multiple \inlineemph{dist\_fn} blocks. The layout of these
blocks is as follows:\\

{\emphtext name} - The name of the distribution function when it is
output. This name is appended with the name of each species for which the data
is output and so, for example, when applied to a species named
carbon the output is called \inlineemph{x\_px\_carbon}. The Cartesian grid
which describes the axes of the distribution function would then be called
\inlineemph{grid\_x\_px\_carbon}.\\

{\emphtext ndims} - The number of dimensions in this phase space
reconstruction. Due to difficulties in visualising data in more than three
dimensions, this is restricted to being either 2 or 3.\\

{\emphtext dumpmask} - At which type of output should this distribution
function be dumped. Whether or not the distribution function is actually
dumped is defined by the more restrictive of this parameter and the setting for
distribution functions in the output section of input.deck.\\

{\emphtext direction\inlinecode{n}} - This is a code representing the direction
which is calculated to run along axis \inlinecode{n}. This can be any one of:
dir\_x, dir\_y, dir\_z, dir\_px, dir\_py, dir\_pz, with spatial codes only
being available in dimensionalities of the code which have that
direction. Therefore dir\_z does not exist in EPOCH1D or EPOCH2D.\\

{\emphtext range\inlinecode{n}} - The range between which this axis should
run. This is in the form of (minimum, maximum). The rangen parameter is ignored
when applied to a spatial direction since all spatial directions run over
the whole domain. For momentum directions this parameter is specified in
$kgms^{-2}$. If the range of a momentum direction is set so that the maximum
and the minimum are both zero then the code will automatically set the range to
exactly span the range of particle momenta at the point of writing the dump.\\

{\emphtext NOTE: Currently the range parameters have to be simple floating
point numbers and NOT maths parser expressions.}\\

{\emphtext resolution\inlinecode{n}} - The number of gridpoints in a given
direction. Once again this is ignored for spatial dimensions where the
resolution is always the same as the resolution of the underlying simulation.\\

{\emphtext include\_species\_\inlinecode{n}} - Logical flag indicating whether
the species defined in the input.deck as species number n should be included
in the output. This is useful since it is rare that momentum limits are
appropriate for both electrons and ions, so usually for a given dist\_fn block
only electrons or ions are considered. It is possible to have two dist\_fn
blocks with the same name but different momentum ranges and different
include\_species\_n settings produce the effect of a single diagnostic for
all species in the output file.\\

{\emphtext NOTE: If using multiple dist\_fn blocks with the same name MAKE SURE
that the same species is not included in both blocks, since the data for both
would be written but only the first block would be available in the reader
routines.}\\

There are a few additional commands which are less commonly used but can be
useful in some circumstances. These commands allow a user to restrict which
particles should be included in the integration over directions which are not
axes of the distribution function. It allows the user to specify minimum and
maximum values for each spatial and momentum direction and use particles which
fall within this range when calculating the distribution function. The
restrictions are specified in the same (minimum,maximum) form as ranges. These
commands are:\\

{\emphtext restrict\_\{x,y,z\}} - Restricts over spatial dimensions. Only
spatial dimensions which exist in the code being run are available. Therefore,
attempting to set restrict\_z in EPOCH1D will produce a warning.\\

{\emphtext restrict\_p\{x,y,z\}} - Restricts over momentum directions. All
momentum directions exist in all versions of the code, so it is possible to set
restrictions in any momentum dimension in any dimensionality of the code.\\

{\emphtext NOTE: It is possible to restrict in dimensions which are included in
the distribution function and these restrictions are honoured. This means that
there will be empty sections of the distribution function plot.}

\subsection{Basic examples of using {\EPOCH}}

\subsubsection{Electron two stream instability}

An obvious simple test problem to do with {\EPOCH} is the electron two stream
instability. An example of a nice dramatic two stream instability can be
obtained using EPOCH1D by setting the code with the following input deck
files.
\begin{lboxverbatim}{input.deck}
begin:control
   # global number of gridpoints
   nx=400 # in x
   npart=3200

   # maximum number of iterations
   # set to -1 to run until finished
   nsteps=-1

   # final time of simulation
   t_end=1.5e-1

   # size of domain
   x_min=0
   x_max=5.0e5

   # Don't know if this should be in the deck or not
   dt_multiplier=0.9

   # Setting neutral_background to F makes the code solve
   # Poisson's equation for the initial electric field
   # Setting it to T makes the initial electric field 0
   neutral_background=T
end:control

begin:boundaries
   bc_x_min=periodic
   bc_x_max=periodic
end:boundaries

begin:species
   n_species=2

   # Rightwards travelling electrons
   charge1=-1
   mass1=1.0
   frac1=0.5
   name1=Right
   dump1=T

   # Leftwards travelling electrons
   charge2=-1
   mass2=1.0
   frac2=0.5
   name2=Left
   dump2=T
end:species

begin:output
   # If use_offset_grid is true then the code dumps a
   # grid which displays positions relative to the
   # Left hand edge of the window
   use_offset_grid=F
   # number of timesteps between output dumps
   dt_snapshot=1.5e-3
   # Number of dt_snapshot between full dumps
   full_dump_every=1
   restart_dump_every=-1
   force_final_to_be_restartable=T

   # Properties at particle positions
   particles=always
   px=always
   py=never
   pz=never
   vx=never
   vy=never
   vz=never
   charge=never
   mass=never
   particle_weight=never
   species_id=always

   # Properties on grid
   grid=always
   ex=always
   ey=always
   ez=always
   bx=always
   by=always
   bz=always
   jx=always
   jy=never
   jz=never
   ekbar=always
   mass_density=never+species
   charge_density=never
   number_density=always+species
   temperature=never
end:output

include:ic.deck
\end{lboxverbatim}

\begin{lboxverbatim}{ic.deck}
begin:constant
   drift_p=2.5e-24
   temp=273
   dens=10
end:constant

begin:species1
   temp_x=temp
   drift_x=drift_p
   rho=dens
end:species1

begin:species2
   temp_x=temp
   drift_x=-drift_p
   rho=dens
end:species2
\end{lboxverbatim}

\scaledcapimage{./images/late}{tsilate}{The final state of the electron
  phase space for the two stream instability example.}{0.4}

While the \inlineemph{input.deck} file is rather long, most of it is the basic
standard input deck that is supplied with {\EPOCH}, with only the length of the
domain, the final time and the time between snapshots specific to this
problem. \inlineemph{ic.deck}, the initial conditions file, is very simple
indeed. The first block sets up constants for the momentum space drift, the
temperature and the electron number density. The second and third blocks set up
the two drifting Maxwellian distributions and the constant density profile.
Note that we have written this example as two separate files for clarity. The
same result would be obtained by appending the contents of ``ic.deck'' to
the end of ``input.deck'' and removing the ``include'' line.
The final output from this simulation is shown in Figure~\ref{tsilate}.

\subsubsection{Structured density profile in EPOCH2D}
A simple but useful example for EPOCH2D is to have a highly structured initial
condition to show that this is still easy to implement in {\EPOCH}. A good
example initial condition would be:
\begin{lboxverbatim}{ic.deck}
begin:constant
  den_peak=1.0e19
end:constant

begin:species1
  rho=den_peak*(sin(4.0*pi*x/length_x+pi/4))*(sin(8.0*pi*y/length_y)+1)
  minrho=0.1*den_peak
end:species1

begin:species2
  rho=rho(1)
end:species2
\end{lboxverbatim}

Although not included here, the input deck associated with these initial
conditions sets \inlineemph{species1} to be electrons and \inlineemph{species2}
to be protons. The species block for \inlineemph{species1} is specified
first, setting up the ion density to be a highly structured 2D sinusoidal
profile. The species block for \inlineemph{species2} is then set to simply
match the density of \inlineemph{species1}, enforcing charge neutrality. On
its own this initial condition does nothing and so only needs to run for 0
timesteps (\inlineemph{nsteps=0} in input.deck). The resulting electron number
density should look like Figure~\ref{densitycomplex}


\scaledcapimage{./images/shapetest}{densitycomplex}{Complex 2D density
  structure}{0.4}

\subsubsection{A hollow cone in 3D}
A more useful example of an initial condition is to create a hollow cone. This
is easy to do in both 2D and 3D, but is presented here in 3D form.
\begin{lboxverbatim}{ic.deck}
begin:constants
   den_cone=1.0e22
end:constants

begin:deo
   ri=abs(x-5.0e-6)
   ro=ri+1.0e-6
   r=sqrt(y^2+z^2)
end:deo

begin:species2
   rho=if((r gt ri) and(r lt ro),den_cone,0.0)
   rho=if((x gt 3.0e-6) and (x lt 4.0e-6) and (r lt ri),den_cone,rho(2))
   rho=if(x gt 4e-6,0.0,rho(2))
end:species2

begin:species1
   rho=rho(2) * 22.0
end:species1
\end{lboxverbatim}

To convert this to 2D, simply replace the line
\inlinecode{r=sqrt(y\^{}2+z\^{}2)} with the line \inlinecode{r=abs(y)}. The
actual work in these initial conditions is done by the three lines inside the
block for \inlineemph{species2}. Each of these lines performs a very specific
function:

\begin{enumerate}
\item Creates the outer cone. Simply tests whether \inlinecode{r} is within
  the range of radii which corresponds to the thickness of the cone and if so
  fills it with the given density. Since the inner radius is x dependent this
  produces a cone rather than a cylinder. On its own, this line produces a
  pair of cones joined at the tip.
\item Creates the solid tip of the cone. This line just tests whether the
  point in space is within the outer radius of the cone and within a given
  range in x, and fills it with the given density if true.
\item Cuts off all of the cone beyond the solid tip. Simply tests if x is
  greater than the end of the cone tip and sets the density to zero if so.
\end{enumerate}

\scaledcapimage{./images/3dcone}{3dcone}{Cone initial conditions in 3D}{0.4}
\scaledcapimage{./images/2dcone}{2dcone}{Cone initial conditions in 2D}{0.4}

This deck produces and initial condition which looks like Figure~\ref{3dcone}
and Figure~\ref{2dcone} in 3D and 2D respectively.
\pagebreak

\section{{\EPOCH} for end users}

\subsection{Structure of the {\EPOCH} codes}
When obtained, the {\EPOCH} codes all have a similar structure. Inside the
epoch{n}d directory, there are 3 subdirectories:

\begin{itemize}
\item src - The {\EPOCH} source code.
\item IDL - The IDL routines needed to open the Compact Flexible Datatype
  (CFD) files which the code outputs.
\item VisIt - The files for creating a plug-in for the LLNL VisIt parallel
  visualisation tool for reading CFD files.
\item Data - A sample data directory containing example input deck files.
\end{itemize}
there are also 10 files:

\begin{itemize}
\item cleandir - A script which deletes the output files generated by the
  {\EPOCH} code from an output directory while leaving the input files needed to
  run the code.
\item copydir - A script which copies the input files used by {\EPOCH} from one
  directory to another without copying any of the output files.
\item deck.file - An example file showing how to run the code without user
  interaction. It will be explained in more depth later.
\item epoch{n}d.pbs - An example PBS batch submission script showing how to use
  the code in a parallel environment. In general, it probably will not be
  suitable for your particular machine, please consult the documentation for
  the facility that you are working on.
\item gen\_commit\_string - This script is used during the build process to
  generate a commit id string which can later be used to identify the version
  number of the code binary and output files.
\item gen\_src\_module - This script is used during the build process to
  embed the source code into the code binary, enabling it to be written into
  restart dumps.
\item Makefile - A standard makefile.
\item pack\_for\_transport - A script which is used in the event of bug
  reporting. pack\_for\_transport packs up all the relevant parts of a
  standard {\EPOCH} installation into a single tgz file. The file is then sent
  when reporting the bug.
\item Start.pro - An IDL script which starts the IDL visualisation
  routines. Execute it using ``idl Start''.
\item unpack\_source\_from\_restart - All restart dumps are written to contain
  a copy of the input decks and source code used to generate them. This script
  can be used to unpack that information from a given restart dump.
\end{itemize}

There is also a single directory in the parent directory of epoch{1,2,3}d:

\begin{itemize}
\item VisIt - The files for creating a plug-in for the LLNL VisIt parallel
  visualisation tool for reading CFD files.
\end{itemize}

\subsection{Libraries and requirements}
The {\EPOCH} codes are written using MPI for parallelism, but have no other
libraries or dependencies. Currently, the codes are written to only require
MPI1.2 compatible libraries, although this may change to require full MPI2
compliance in the future. Current versions of both MPICH and OpenMPI implement
the MPI2 standard and are known to work with this code. The SCALI MPI
implementation is only compliant with the MPI1.2 specification and may loose
support soon.
There are no plans to write a version of {\EPOCH} which does not require
the MPI libraries.

The code is supplied with a standard GNU make makefile, which is also
compatible with most other forms of the {\bf make} utility. In theory it is
possible to compile the code without a {\bf make} utility, but it is much
easier to compile the code using the supplied makefile.

\subsection{Compiling and running {\EPOCH}}

To compile {\EPOCH} in the supplied state, just type\\
\indent\inlinecode{make}\\
and the code will compile. There are certain options within the code which are
controlled by compiler preprocessors which are described in the next
section. When the code is compiled, it creates a new directory called ``bin''
containing the compiled binary which will be called \inlinecode{epoch1d},
\inlinecode{epoch2d} or \inlinecode{epoch3d}. To run the code, just execute the
binary file by typing:\\
\indent\inlinecode{./bin/epoch2d}\\
or whatever the correct binary is for the dimensionality of the code that you
have. You should be given a screen which begins with the {\EPOCH} logo, and then
reads:
\begin{boxverbatim}
Welcome to EPOCH2D Version 2.1

 The code was compiled with the following compile time options
 *************************************************************
 Per particle weighting -DPER_PARTICLE_WEIGHT
 Tracer particle support -DTRACER_PARTICLES
 Particle probe support -DPARTICLE_PROBES
 Particle ionisation model -DPART_IONISE
 *************************************************************

 Specify output directory
\end{boxverbatim}

At this point, the user simply types in the name of the (already existing)
output directory and the code will read the input deck files inside the
specified directory and start running. To run the code in parallel, just use
the normal mpirun or mpiexec scripts supplied by your MPI implementation. If
you want the code to run unattended, then you will need to specify the output
directory by piping the directory name in from a file. An example of such a
file is supplied as ``deck.file'' with the standard distribution of {\EPOCH}. To
use it, just run the code as\\
\indent\inlinecode{mpirun -np 2 ./bin/epoch2d < deck.file}\\
and the code will run without user input. Some cluster queueing systems do not
allow the use of input pipes to mpirun. In this case, there is usually a
``-stdin'' command line option to specify an input file, see your cluster
documentation for more details.

\subsection{Compiler flags and preprocessor defines}
As already stated, some features of the code are controlled by compiler
preprocessor directives. The flags for these preprocessor directives are
specified in ``Makefile'' and are placed on the line which reads:
\begin{boxverbatim}
DEFINES = -DPER_PARTICLE_WEIGHT
\end{boxverbatim}
To turn on the effect given by a given preprocessor directive, just add the
command \inlinecode{-D\{directive\}} to the \inlinecode{DEFINES} line. The
options currently controlled by the preprocessor are:\\
\begin{itemize}
\item PER\_PARTICLE\_WEIGHT - Instead of running the code where each
  pseudoparticle represents the same number of real particles, each
  pseudoparticle can represent a different number of real particles. Many of
  the codes more advanced features require this and it is turned on by
  default. It can be turned off to save on memory, but this is recommended
  only for advanced users.
\item PARTICLE\_CELL\_DIVISION - After the code has updated the particle
  positions, it splits the particles into separate lists for each grid
  cell. Some features of the code (like collision operators) require this
  feature to be on, but it is off by default.
\item PARTICLE\_DEBUG - Each particle is additionally tagged with information
  about which processor it is currently on, and which processor it starts
  on. This is a debug mode for code development.
\item FIELD\_DEBUG - The code also outputs information about where the
  processor boundaries are in space. This is a debug mode for code development.
\item PARTICLE\_IONISE - Activate the particle ionisation code (BETA).
\item PARTICLE\_COUNT\_UPDATE - Makes the code keep global particle counts for
  each species on each processor. This information isn't needed by the core
  algorithm, but can be useful for developing some types of additional physics
  packages. It does require one additional MPI\_ALL\_REDUCE per species per
  timestep, so it is not activated by default.
\item TRACER\_PARTICLES - Gives the option to specify one or more species as
  tracer particles. Tracer particles are specified like normal particles, and
  move about as would a normal particle with the same charge and mass, but
  tracer particles do not generate any current and are therefore passive
  elements in the simulation. Any attempt to add particle collision effects
  should remember that tracer species should not interact through collisions.
  The implementation of tracer particles requires an additional ``IF'' clause
  in the particle push, so it is not activated by default.
\item PARTICLE\_PROBES - For laser plasma interaction studies it can sometimes
  be useful to be able to record information about particles which cross a
  plane in the simulation. Since this requires the code to check whether each
  particles has crossed the plane in the particles pusher and also to store
  copies of particles until the next output dump, it is a heavyweight
  diagnostic. Therefore, this diagnostic is only enabled when the code is
  compiled with this directive.
\item SPLINE\_FOUR - Swap the code from a 2nd order cloud in cell PIC code to
  using 4th order spline particle shape functions.
\end{itemize}

So to turn on per particle weighting and particle debugging, the line would
look like:
\begin{boxverbatim}
DEFINES = -DPER_PARTICLE_WEIGHT -DPARTICLE_DEBUG
\end{boxverbatim}

If a user requests an option which the code has not been compiled to support
then the code will give an error message as follows:
\begin{boxverbatim}
 ***WARNING***
 The element "particle_probes" of block "output" cannot be set
 because the code has not been compiled with the correct preprocessor options.
 Code will continue, but to use selected features, please recompile with the
 -DPARTICLE_PROBES option
\end{boxverbatim}

It is also possible to pass other flags to the compiler. In ``Makefile'' there
is a line which reads\\
\indent\inlinecode{FFLAGS = -O3 -fast}\\
the two commands to the right are compiler flags and are passed unaltered to
the fortran compiler. Change this line to add any additional flags required by
your compiler.

\subsection{The {\EPOCH} input deck}
The input deck files describe the setup of the code and
include almost all the controllable parameters for the code. The input deck is
contained in a file called ``input.deck'' which must be present in the output
directory that is given to the code at runtime. It is a structured
file which is split into separate blocks, with each block containing several
``parameter''=``value'' pairs. The pairs can be present in any order, and not
all possible pairs must be present in any given input deck. If a required pair
is missing the code will exit with an error message. The input deck is case
sensitive, so true is always ``T'', false is always ``F'' and the names of
the parameters are always lower case.\\

There are three {\it input deck directive} commands, which are:
\begin{itemize}
\item begin:{\it block} - Begin the block named {\it block}.
\item end:{\it block} - Ends the block named {\it block}.
\item include:{\it filename} - Includes another file (called {\it filename})
  into the input deck at the point where the directive is encountered. The
  input deck parser reads the included file exactly as if the contents of the
  included file were pasted directly at the position of the include directive.
\end{itemize}
Each block must be surrounded by valid {\it begin:} and {\it end:} directives
or the input deck will fail. There are currently eleven valid blocks hard
coded into the input deck reader, but it is possible for end users to extend the
input deck. If the input deck has been extended then you must contact the
writer of the extension for assistance. The eleven built in blocks are
\begin{itemize}
\item control - Contains information about the general code setup.
\item boundaries - Contains information about the boundary conditions for this
  run.
\item species - Contains information about the species of particles which are
  used in the code.
\item output - Contains information about when and how to dump output files.
\item window - Contains information about the moving window if the code is
  used in that fashion.
\item constant - Contains information about user defined constants.
\item species\_external - Contains information about particle species to be
  read from external binary input files.
\item deo - Contains information about deferred execution objects.
\item laser - Contains information about laser sources.
\item dist\_fn - Contains information about distribution functions.
\item probe - Contains information about particle probes.
\end{itemize}

\subsubsection{The control block}
The control block of a valid input deck for EPOCH2D reads as follows:
\begin{boxverbatim}
begin:control
   # global number of gridpoints
   nx=512 # in x
   ny=512 # in y
   # global number of particles
   npart=1000000

   # maximum number of iterations
   # set to -1 to run until finished
   nsteps=-1

   # final time of simulation
   t_end=1.0e-12

   # size of domain
   x_min=-0.1e-6
   x_max=400.0e-6
   y_min=-400.0e-6
   y_max=400.0e-6

   # Don't know if this should be in the deck or not
   dt_multiplier=0.8

   dlb_threshold=0.5

   # Setting neutral_background to F makes the code solve
   # Poisson's equation for the initial electric field
   # Setting it to T makes the initial electric field 0
   neutral_background=T
end:control
\end{boxverbatim}

Most of the control block is self explanatory, but there are two parts which
need further description. \\
``dlb'' stands for Dynamic Load Balancing and, when turned on, it allows the
code to rearrange the internal domain boundaries to try and balance the
workload on each processor. This rearrangement is an expensive operation, so
it is only performed when the maximum load imbalance reaches a given critical
point. This critical point is given by the parameter ``dlb\_threshold'' which
is the ratio of the workload on the least loaded processor to the most loaded
processor. When the calculated load imbalance is less than ``dlb\_threshold''
the code performs a re-balancing sweep, so if ``dlb\_threshold=1.0'' is set
then the code will keep trying to re-balance the workload at almost every
timestep. At present the workload on each processor is simply calculated from
the number of particles on each processor, but this will probably change in
future. If the ``dlb\_threshold'' parameter is not specified then the code
will only be load balanced at initialisation time.\\

Finally, ``neutral\_background'' is a parameter which changes the initial
behaviour of the code. If ``neutral\_background'' is set to ``T'' then the code
assumes that there exists sufficient particles to mean the initial condition
has an electric field of exactly zero. This is appropriate when the number of
real particles per pseudoparticle is very large, meaning that the electric
fields due to the pseudoparticles will be artificially large. Setting
``neutral\_background'' to ``F'' means that the code will solve Poisson's
equation for the initial conditions, leading to a finite electric field at
t=0. This is appropriate when the number of real particles per pseudoparticle
is small, or when the initial conditions include a region which has a charge
excess or depletion at t=0.\\

\subsubsection{The boundaries block}
The next section of the input deck describes the boundary conditions. The
boundaries block for EPOCH3D is as follows:
\begin{boxverbatim}
begin:boundaries
   bc_x_min=other
   bc_x_max=other
   bc_y_min=periodic
   bc_y_max=periodic
   bc_z_min=other
   bc_z_max=other
end:boundaries
\end{boxverbatim}

This block is fairly self explanatory and describes the boundary conditions
applied to each of the 6 faces of the cuboid which represents the extents of
the simulation. In 1D only xbc\_ appears and in 2D only xbc\_ and ybc\_
appear. The possible boundary conditions are:\\
\begin{itemize}
\item periodic - Particles and fields crossing this boundary wrap back round
  to the opposite boundary. If either boundary condition is set to periodic
  then the boundary condition on the matching boundary at the other side of
  the box is also assumed periodic.
\item other - Particles are perfectly reflected from this boundary
  type. Electric and magnetic fields are clamped to zero at the boundary.
\item simple\_laser - Particles and outwardly propagating EM waves travel
  through the boundary. One or more inwardly propagating EM wave sources may
  be specified either in the input deck or in the initial conditions section.
\item simple\_outflow - Particles and outwardly propagating EM waves travel.
  through the boundary. It is not possible to attach a wave source to this
  type of boundary, which makes it simpler and faster than simple\_laser.
\end{itemize}
Other boundary types will appear as the code matures, probably including laser
driven boundaries and absorbing boundaries.\\

\subsubsection{The species block}
The next section of the input deck describes the particle species used in the
code. An example species block for any {\EPOCH} code is given below.
\begin{boxverbatim}
begin:species
   n_species=2

   # H+ ions
   charge1=1.0
   mass1=1800.0
   frac1=0.5
   name1=Proton
   dump1=T

   # electrons
   charge2=-1.0
   mass2=1.0
   frac2=0.5
   name2=Electron
   dump2=T
end:species
\end{boxverbatim}

The species block is slightly more complex than preceding blocks in that the
number of species described in a block must be specified in advance. While
for most input deck blocks the structure is completely free-form, the first
thing specified in the species block must be ``n\_species'' which tells the
code how many species of particle are present in the code. After that, the
block is once again free-form, although it makes sense to keep the information
for each species together. Each species has the following data which must be
specified in the input deck:\\
\begin{itemize}
\item charge - The charge on the particle in units of the electron charge.
\item mass - The mass of the particle in units of the electron mass.
\item frac - The fraction of the total number of particles in the simulation
  which are of this species.
\item name - The name of this particle species (as written to the output
  dumps).
\item dump - Whether or not to dump this particle species in normal outputs
  (this is ignored for restart dumps when enough information is dumped to
  restart the code).
\item npart - This can be used instead of \inlinecode{frac} to explicitly set
  the number of particles for a species. If this element is set then the code
  effectively ignores the values of \inlinecode{npart} set in the control
  block.
\item tracer - If the code is compiled with tracer particle support then
  setting this logical element to ``\inlinecode{T}'' makes the code treat this
  species as a tracer species. Tracer species have the correct charge and mass
  but do not contribute any current.
\end{itemize}

The particle species which a given property refers to is simply set by a
number after the property name, starting with 1 for the first species and
ending at n\_species for the final species. A final note: if the values of
frac for all species don't add up to one then there will be some particles
requested which are never assigned a species. These particles are destroyed
before the code runs to save memory and compute time, but it means that the
number of particles in the simulation will be lower than expected.\\

\subsubsection{The output block}
The next section of the input deck is the output section. It describes the
data that the user wants dumped from the code and an example block from any
version of {\EPOCH} is given below.
\begin{boxverbatim}
begin:output
   # If use_offset_grid is true then the code dumps a
   # grid which displays positions relative to the
   # Left hand edge of the window
   use_offset_grid=T

   # number of timesteps between output dumps
   dt_snapshot=1.0e-14
   # Number of dt_snapshot between full dumps
   full_dump_every=1
   restart_dump_every=1
   force_final_to_be_restartable=T

   # Properties at particle positions
   particles=full
   px=never
   py=never
   pz=never
   vx=full
   vy=full
   vz=never
   charge=full
   mass=full
   particle_weight=always
   species_id=always

   # Properties on grid
   grid=always
   ex=always
   ey=always
   ez=always
   bx=always
   by=always
   bz=always
   jx=always
   jy=always
   jz=always
   temperatures=always
   mass_density=always
   charge_density=always
end:output
\end{boxverbatim}

The first set of options control the type and frequency of output dumps. They
are used as follows\\
\begin{itemize}
\item use\_offset\_grid - Causes the code to output a special grid which moves
  with a moving window if one is specified. This is needed to allow
  visualisation packages like LLNL VisIt, which cannot cope with moving axes, to
  work properly. If you are not using a moving window then setting this option
  to true merely wastes space as both the normal and the special grid are the
  same.
\item dt\_snapshot - Time (in internal seconds) between performing a basic
  output dump.
\item full\_dump\_every - Number of basic output dumps between full output
  dumps.
\item restart\_dump\_every - Number of basic output dumps between restart
  output dumps. Set to -1 or lower to not produce restart dumps.
\item force\_final\_to\_be\_restartable - Whether or not the code should force
  the last output dump to be a restart dump. This ensures that it is possible
  to restart the code from the last output dump.
\end{itemize}

The remaining items control what data should be dumped at which type of
output. There are three possible values\\
\begin{itemize}
\item never - This variable will never be dumped unless it is a required restart
  variable in which case it will be dumped at a restart dump.
\item full - Dump only at a full output dump.
\item always - Dump at a basic output dump.
\item species - When applied to a grid variable which can meaningfully be
  calculated on a per species level, this causes the code to dump per species
  information about that variable. This is simply added to the frequency code,
  i.e. ``mass\_density = always + species'' will cause the mass density to be
  output at every dump for each species and also globally.
\end{itemize}

The options are fairly self explanatory, but they are given in more detail
below. The first set are per particle properties which must be plotted at the
individual particle positions to make sense.\\
\begin{itemize}
\item particles - Dump particle position data.
\item px, py, pz - Dump particle momentum in x, y, z direction.
\item vx, vy, vz - Dump particle velocity in x, y, z direction.
\item charge - Dump particle charge.
\item mass - Dump particle mass.
\item particle\_weight - Dump the weight value for each particle. (The weight
  is the number of real particles represented by a given pseudoparticle).
\item species\_id - Dump the species number for each particle.
\end{itemize}

There are also variables which are defined on the underlying grid.\\
\begin{itemize}
\item grid - Dump the grid underlying the simulation.
\item ex, ey, ez - Dump the electric field in x, y, z.
\item bx, by, bz - Dump the magnetic field in x, y, z.
\item jx, jy, jz - Dump the current in x, y, z.
\item temperature - Dump the mean particle kinetic energy at each gridpoint
  for each species.
\item mass\_density - Dump the mass density.
\item charge\_density - Dump the charge density.
\end{itemize}

\subsection{The window block}
{\EPOCH} can include an optional block which causes the simulation domain to
operate as a moving window. At present, it is only possible to have the window
moving at a constant speed parallel to the x direction, although the window
does not have to start moving at t=0. When the window moves, the code removes
particles from the left hand edge of the domain and introduces new particles
at the right hand edge. The code does not only reintroduce the same number of
particles at the right hand edge as are removed at the left hand edge, but
introduces new particles so that for each species the new particles have a
given number density, temperature and number of pseudoparticles per cell. It
is not currently possible to turn off the reintroduction of particles to allow
a pulse to travel into a vacuum region, although this is being developed. The
block looks like:
\begin{boxverbatim}
begin:window
   move_window=T
   window_v_x=3.0e8
   window_start_time=7.0e-13
   bc_x_min_after_move=simple_outflow
   bc_x_max_after_move=simple_outflow
end:window
\end{boxverbatim}

\begin{itemize}
\item move\_window - Logical flag determining whether or not to move the
  window. If the window block is absent then this is the same as setting
  move\_window to ``F''.
\item window\_v\_x - The speed in m/s of the window.
\item window\_start\_time - The time in seconds at which the window should
  start moving.
\item bc\_x\_min\_after\_move - The boundary condition which should apply to
  the left boundary after the window has started moving. This is to allow the
  swapping of a laser boundary to a simple outflow boundary. Boundary codes
  are the same as when just specifying normal boundaries. If a boundary value
  isn't specified then it is assumed that the boundary isn't changed when the
  window starts moving. ``xbc\_left\_after\_move'' is accepted as a synonym.
\item bc\_x\_max\_after\_move - The boundary condition which should apply to
  the right boundary after the window has started moving.
  ``xbc\_right\_after\_move'' is accepted as a synonym.
\end{itemize}

The basic input deck has now been considered fully, but it remains to point
out that it is possible for an end user to add new blocks to the input deck,
so a version of the code which you have obtained from a source other than
CCPForge may include other input deck blocks. These should be described in
additional documentation provided with the version of the code that you have.

\subsection{The maths parser}
A discussion of the input deck for {\EPOCH} would not be complete without
consideration of the maths parser. This means that any parameter taking a
numerical value (integer or real) can be input as a mathematical expression
rather than as a numerical constant. The maths parser is fairly extensive and
includes a range of mathematical functions, physical and simulation constants
and appropriately prioritised mathematical operators. Some features of the
deck only make sense in the context of the external initial conditions file,
and will be described in that section.

\subsubsection{Constants}
The maths parser in {\EPOCH}  has the following constants
\begin{itemize}
\item pi - The ratio of the circumference of a circle to its diameter.
\item kb - Boltzmann's constant.
\item me - Mass of an electron.
\item qe - Charge of an electron.
\item epsilonnought - Permeability of free space.
\item munought - Permittivity of free space.
\item length\_x - The length of the simulation box in the x direction.
\item length\_y - The length of the simulation box in the y direction (2D and 3D
  only).
\item length\_z - The length of the simulation box in the z direction (3D only).
\end{itemize}

It is also possible for an end user to specify custom constants both within
the code and from the input deck. These topics are covered later in this
subsection. An example of using a constant would be:\\
\indent\inlinecode{length\_x = pi}\\

\subsubsection{Functions}
The maths parser in {\EPOCH} has the following functions
\begin{itemize}
\item sqrt(a) - Square root.
\item sin(a) - Sine.
\item cos(a) - Cosine.
\item tan(a) - Tangent.
\item exp(a) - Exponential.
\item asin(a) - Arcsine.
\item acos(a) - Arccosine.
\item atan(a) - Arctangent.
\item if(a,b,c) - Conditional function. If a != 0 the function returns b,
  otherwise the function returns c.
\item floor(a) - Convert real to integer rounding down.
\item ceil(a) - Convert real to integer rounding up.
\item nint(a) - Convert real to integer rounding to nearest integer.
\item tanh(a) - Hyperbolic tangent.
\item sinh(a) - Hyperbolic sine.
\item cosh(a) - Hyperbolic cosine.
\end{itemize}

It is also possible for an end user to specify custom functions within the
code. An example of using a function would be:\\
\indent\inlinecode{length\_x = exp(pi)}\\

\subsubsection{Operators}
The maths parser in {\EPOCH} allows the following operators
\begin{itemize}
\item a + b - Addition operator.
\item a - b - Subtraction operator or unary negation operator (auto-detected).
\item a * b - Multiplication operator.
\item a / b - Division operator.
\item a\^{}b - Power raise operator.
\item a e b - Power of ten operator (1.0e3 = 1000).
\item a lt b - Less than operator. Returns 1 if a $<$ b, otherwise returns
  0. Intended for use with if.
\item a gt b - Greater than operator. Returns 1 if a $>$ b, otherwise returns 0.
\item a eq b - Equality operator. Returns 1 if a==b, otherwise returns 0.
\item a and b - Logical and operator. Returns 1 if a != 0 and b != 0,
  otherwise returns 0.
\item a or b - Logical or operator. Returns 1 if a != 0 or b != 0, otherwise
  returns 0.
\end{itemize}

It is not possible at this time to specify custom operators without major
changes to the code. An example of using an operator would be:\\
\indent\inlinecode{length\_x = 10.0 + 12.0}\\

\subsection{Creating custom constants within the input deck}
Setting up the custom extensions to the input deck are discussed in the
section {\bf Customising {\EPOCH}} later in the manual, but it is simple to set
up custom constants from within the input deck. There is a special input deck
block called {\it constant} which simply contains the constants which you want
to set up. Suppose, for example, that you have a problem in 1D where the Debye
length is known and you want the box to be a fixed number of Debye lengths,
and the grid spacing to always be at least 0.5 Debye lengths, you would set
that up as follows:
\begin{boxverbatim}
begin:constant
  l_debye=1.0e-6 # Debye length is 1 micrometer
  n_debye_in_box = 100 # number of debye lengths in simulation
  n_gridpoints=100 # number of grid points requested
  l_metres=l_debye * n_debye_in_box # length of simulation box in metres
  n_gp_per_debye=n_gridpoints/n_debye_in_box # number of gridpoints per debye length
end:constant

begin:control
  # if the number of gridpoints per debye length is greater than 2
  # use the requested number of gridpoints, otherwise use enough
  # that there are two gridpoints per debye length
  nx = if(n_gp_per_debye gt 2, n_gridpoints, n_debye_in_box * 2)
  length_x = l_metres
end:control
\end{boxverbatim}

Note that in this case, it is still up to the user to ensure that the Debye
length is actually the value given in \inlinecode{l\_debye} by setting the
initial conditions. The named constants are created the first time they are
specified, and can be reset at will. It is possible to have several instances
of the constant block, either creating new constants or resetting existing
ones. It is possible to end other blocks, define constants, and then
restart the block, so this is valid:
\begin{boxverbatim}
begin:control
   # some of control block
end:control

begin:constant
   # set constants
end:constant

begin:control
   # rest of control block
end:control
\end{boxverbatim}

\subsection{MPI in {\EPOCH}}
{\EPOCH} is a parallel PIC code using the standard MPI 2.0 library. However, the
code has been written in such a way that it is possible to use most of the code
with no knowledge of MPI at all. However, this does mean that there are some
quantities which MUST be read from specific variables in the code rather than
hard coding quantities in. These variables are listed below

\subsubsection{Important values in the {\EPOCH} code}
When setting up initial conditions within the {\EPOCH} source (rather than using
the external initial conditions) there are several constants that you need to
know about. These constants are:
\begin{itemize}
\item nx - Number of gridpoints on the local processor in the x direction.
\item ny - Number of gridpoints on the local processor in the y direction (2D
  and 3D).
\item nz - Number of gridpoints on the local processor in the z direction (3D).
\item length\_x - Length of domain in the x direction.
\item length\_y - Length of domain in the y direction (2D and 3D).
\item length\_z - Length of domain in the z direction (3D).
\item n\_species - The number of species in the code.
\end{itemize}

There are also up to three arrays which are available for use.
\begin{itemize}
\item x(-1:nx+2) - Position of a given gridpoint in real units in the x
  direction.
\item y(-1:ny+2) - Position of a given gridpoint in real units in the y
  direction (2D and 3D).
\item z(-1:nz+2) - Position of a given gridpoint in read units in the z
  direction (3D).
\end{itemize}

When using the autoloader you shouldn't need to read particle properties
directly at all. If you do then please read the next section on using the
manual particle loader.

\subsection{{\EPOCH} initial conditions using the autoloader}
The {\EPOCH} autoloader is a feature which means that if your initial condition
can be specified as a number density and temperature profile on the underlying
grid then you don't have to deal with the details of the internal
representation of particles in {\EPOCH}. There are three phases of the
autoloader in {\EPOCH}

\begin{itemize}
\item ic\_early - Initial conditions which are specified in the {\EPOCH}
  source and are executed before the external section.
\item external - Initial conditions which are specified in an external input
  deck file. It is not necessary to recompile the code, or even have
  access to the source to change the external initial conditions.
\item manual\_load - Once particles have been allocated they can have their
  properties altered in this routine which is executed after the external
  section.
\end{itemize}

The purpose of this separation into three phases is to allow maximum
flexibility with the minimum need for access to source code. It is therefore
possible to use the code in developer mode where initial conditions are
specified entirely within the {\EPOCH} source, to use a completely external
method where no initial conditions are specified in the {\EPOCH} source at all,
or a hybrid approach where a base initial condition is specified internally,
then modified by the external conditions, and then finally modified again
internally.

\subsubsection{Setting autoloader parameters within {\EPOCH}}
Within {\EPOCH}, there are Fortran90 structures which represent the information
needed by the autoloader for each species. In 2D, the definition of these
structures is:
\begin{boxverbatim}
TYPE :: initial_condition_block

  REAL(num), DIMENSION(:,:), ALLOCATABLE :: rho ! Number density
  REAL(num), DIMENSION(:,:,:), ALLOCATABLE :: temp ! Temperature
  REAL(num) :: minrho ! Minimum density
  REAL(num) :: maxrho ! Maximum density

END TYPE initial_condition_block
\end{boxverbatim}

In 1D, there is one fewer dimension to both arrays, and in 3D there is one
more dimension to both arrays. An instance of the structure is created for
each particle species and is named ``initial\_conditions(ispecies)''. The
elements of this structure have the following definitions (again in 2D)\\
\begin{itemize}
\item rho(-1:nx+2,-1:ny+2) - Particle number density at all spatial
  positions in the grid. Must be defined fully between -1:nx+2 and -1:ny+2.
\item temp(-1:nx+2,-1:ny+2,1:3) - Temperature of a thermal distribution at all
  spatial positions in the grid, and independently in each dimension
  (including ignorable directions). The final dimension is the direction in
  which to apply the specified thermal distribution.
\item minrho - The minimum density at which to load particles. If the initial
  particle number density at any point is lower than minrho then particles are
  not loaded there by the autoloader.
\item maxrho - The maximum density. If the initial particle number density
  exceeds maxrho then the density is clipped to maxrho.
\end{itemize}

So an example of setting up a cold plasma block for all species in 2D would
look like:
\begin{boxverbatim}
SUBROUTINE ic_early

  INTEGER :: ispecies
  REAL(num) :: slab

  DO ispecies = 1, n_species
    initial_conditions(ispecies)%temp = 0.0_num
    initial_conditions(ispecies)%minrho = 1.4_num
    DO iy = -1, ny+2
      DO ix = -1, nx+2
        slab = 0.0_num
        IF (x(ix) .LT. 1.0_num .AND. x(ix) .GT. -1.0_num) slab = 100.0
        initial_conditions(ispecies)%rho(ix,iy) = 1.0_num + slab
      ENDDO
    ENDDO
  ENDDO

END SUBROUTINE ic_early
\end{boxverbatim}

This sets up a plasma slab with a density of 100 particles per square metre
between x=-1 and x=+1 with no restriction on y. Note that density is never set
to zero anywhere, the lowest value of rho is 1.0. However, since the minrho
value is set to 1.4 then particles are only loaded inside the slab where the
density is greater than 1.4. If rho had simply been set to zero (or worse,
some small but non-zero number) the code would have positioned particles in
that cell anyway, but would have assigned the particles very low or zero
weight functions meaning that they would have barely contributed to the
solution while at the same time taking up compute resources. If you have
localised density slabs in a vacuum, you should always use the minrho
parameter to set a cutoff below which particles are not loaded. Running this
example gives the output shown in Figure~\ref{slab}. This picture was produced
using LLNL visit to plot the particles.\\
%Example image from visit for these initial conditions
\scaledcapimage{./images/example}{slab}{Plasma slab setup using the
  ``ic\_early'' subroutine call}{0.4}

If we wish to alter the properties of the particles after they have been
allocated then we can do so in the ``manual\_load'' routine. This behaves
in much the same way as the ``ic\_early'' routine except that in
``ic\_early'' the temperature and number density are guaranteed to be zero,
whereas in ``manual\_load'' they will contain any changes made in ``ic\_early''
and in the external initial conditions. So to take a (very silly) example, I
could use the following code to change the slab to have a finite width in y.
\begin{boxverbatim}
  SUBROUTINE manual_load

    INTEGER :: ispecies

    DO ispecies = 1, n_species
      DO iy = -1, ny+2
        DO ix = -1, nx+2
          IF (y(iy) .GT. 1.0_num .OR. y(iy) .LT. -1.0_num) &
              initial_conditions(ispecies)%rho(ix,iy) = 1.0_num
        ENDDO
      ENDDO
    ENDDO

  END SUBROUTINE manual_load
\end{boxverbatim}

Once this has been added, the output becomes that shown in Figure~\ref{square}.
Both Figures~\ref{slab} and \ref{square} are from runs with the same number of
particles and the same fraction of particles displayed in VisIt. This shows
why the use of the minrho field is so useful, because the second figure
obviously has a higher particles density than the first, since the particles
are concentrated into a smaller region.
%Example image from visit for the second set of initial conditions
\scaledcapimage{./images/example2}{square}{Plasma square setup using the
  ``manual\_load'' subroutine call}{0.4}

\subsubsection{Setting autoloader properties from the input deck}
There are several blocks which allow the user to specify initial conditions
from the input deck. These blocks have the same maths capabilities as other
input deck blocks, but with some additional constants and functions which
only make sense when specifying initial conditions.\\

\begin{nbboxverbatim}{none}
begin:constant
   partdens=1.0e25
   wpe=sqrt(partdens * qe^2/(me * epsilonnought))
end:constant

begin:species1
   rho=interpolate(x,-(6.0e-6),0.2,(10.0e-6),0.2,(11.0e-6),
            1.0,(150.0e-6),1.0,(150.5e-6),0.2,(420.0e-6),0.2,6)
   rho=rho(1)*partdens
   temp_x=300.0
   temp_y=temp_x(1)
   minrho=0.3*partdens
end:species1

begin:species2
   rho=rho(1)
   temp_x=temp_x(1)
   temp_y=temp_x(1)
   minrho=0.3*partdens
end:species2
\end{nbboxverbatim}

In the above example we have made use of the ``species'' block for specifying
initial conditions. There is one species block for each species given in
the global species block, and each species is identified by the addition of the
species number after the word ``species''.
The ``species'' block has the following possible elements. If an element
is not specified then it is left with the same value that it was specified
either at code startup (everything zero), or at the end of the ``ic\_early''
routine.\\
\begin{itemize}
\item rho - Particle number density in $m^{-3}$. Despite the name, this is
  NOT the mass density. As soon as a rho= line has been parsed, the values are
  calculated for the whole domain and are available for reuse on the right hand
  side of an expression.
\item temp\_\{x,y,z\} - The temperature in each direction for a thermal
  distribution in K. To specify a temperature in ev, simply use the deck
  parser. So for example temp\_x=4*kev*kb gives a 4kev temperature.
\item temp - Sets an isotropic temperature distribution in K. Does not give
  thermal distribution in ignorable directions. If both temp and a specific
  temp\_x, temp\_y, temp\_z parameter is specified then the last to appear in
  the deck has precedence.
\item minrho - Minimum particle number density in $m^{-3}$. When the number
  density in a cell falls below minrho the autoloader does not load any
  pseudoparticles into that cell to minimize the number of low weight,
  unimportant particles. If set to 0 then all cells are loaded with particles.
\item maxrho - Maximum particle number density in $m^{-3}$. When the number
  density in a cell rises above maxrho the autoloader clips the density to
  maxrho allowing easy implementation of exponential rises to plateaus.
\item drift\_\{x,y,z\} - Specifies a momentum space offset in kgms-2 to
  the distribution function for this species. At present the drift\_\{x,y,z\}
  parameter CANNOT vary in space and is just a single constant.
\end{itemize}

The maths parser now has the following new constants:
\begin{itemize}
\item x - X coordinate in metres.
\item y - Y coordinate in metres (2D and 3D only).
\item z - Z coordinate in metres (3D only).
\item dx - Grid spacing in the x direction.
\item dy - Grid spacing in the y direction (2D and 3D only).
\item dz - Grid spacing in the z direction (3D only).
\item \{x,y,z\}\_start - \{x,y,z\}\_start specified in the input deck.
\item \{x,y,z\}\_end - \{x,y,z\}\_end specified in the input deck.
\item ix - X coordinate in grid points.
\item iy - Y coordinate in grid points (2D and 3D only).
\item iz - Z coordinate in grid points (3D only).
\item time - Returns current simulation time (used in the laser boundaries).
\end{itemize}

The maths parser now also has the following new functions:
\begin{itemize}
\item rho(a) - Returns the density for species a.
\item temp\_x(a) - Returns temperature in the x direction for species a.
\item temp\_y(a) - Returns temperature in the y direction for species a.
\item temp\_z(a) - Returns temperature in the z direction for species a.
\item gauss(var,centre,fwhm) - Calculate a Gaussian profile in variable {\it
    var} centred on {\it centre} with a characteristic width {\it fwhm}.
\item interpolate(interp\_var,....,n\_pairs) - Linear interpolation function,
  explained later.
\end{itemize}

The use of most of the new functions and constants is fairly simple, but
``interpolate'' requires some additional explanation. This function allows a
user to specify a set of position,value pairs and have the code linearly
interpolate the values between these control points. This function is mainly
intended for ease of converting initial conditions from other existing PIC
codes, and the same effect can usually be obtained more elegantly using the
``if'' command. The structure of the ``interpolate'' command is as follows:
The first parameter is the variable which is to be used as the axis over which
to interpolate the values. This can in general be any valid expression, but
will normally just be a coordinate axis. The next 2n entries are the
position,value pairs and the final parameter is the number of position,value
pairs. The slightly clunky syntax of this command is unfortunately necessary to
allow it to work with some fairly fundamental features of the maths parser
used in {\EPOCH}.\\
All that is required now is to give an example. A good example is to reproduce
the previous example of an isolated plasma block in 2D. This would look like:
\begin{boxverbatim}
begin:species1
   # first set density in the range 0->1
   # cut down density in x direction
   rho=if ((x gt -1) and (x lt 1),1.0,0.2)
   # cut down density in y direction
   rho=if ((y gt -1) and (y lt 1),rho(1),0.2)

   # multiply density by real particle density
   rho=rho(1)*100.0

   # Set the temperature to be zero
   temp_x=0.0
   temp_y=temp_x(1)

   # Set the minimum density for this species
   minrho=0.3*100.0
end:species1

begin:species2
   # Just copy the density for species 1
   rho=rho(1)

   # Just copy the temperature from species 1
   temp_x=temp_x(1)
   temp_y=temp_y(1)

   # Set the minimum density for this species
   minrho=0.3*100.0
end:species2
\end{boxverbatim}

This produces the same output as that shown in Figure~\ref{square}, since
the two sets of initial conditions are exactly equivalent to each other.

An important point to notice is that the two parts of the logical expressions
in the input deck are enclosed within their own brackets. This helps to remove
some ambiguities in the functioning of the input deck parser. It is hoped that
this will soon be fixed, but at present ALWAYS enclose logical expressions in
brackets.

\subsubsection{Setting autoloader properties from an external binary file}
It is possible to load external data files into variables used by the
autoloader. In the input deck this is done as follows:
\begin{boxverbatim}
begin:species_external1
   rho=Data/data.file
   offset=80000
   temp_x=Data/data.file
   temp_y=Data/data.file
end:species_external1
\end{boxverbatim}

The species\_external block is very similar to the species block, except that
where you would normally specify a functional form for a variable, you now
specify a filename. Other variables such as {\bf minrho} and {\bf maxrho}
still work as in the {\it species} block. It is possible to specify
multiple variables in a single file using the {\bf offset} element. When the
offset is set, the next data read from a file is read with an offset of {\bf
offset} bytes from the start of the file. {\bf offset} is reset to zero at the
start of a new {\it species\_external} block, but is not reset within a given
{\it species\_external} block. Therefore, in the example above, {\bf temp\_x}
and {\bf temp\_y} are assigne the same values. The data is immediately loaded
into the requested variable, so it is valid to do the following:
\begin{boxverbatim}
begin:species_external1
   rho=Data/data.file
   offset=80000
   temp_x=Data/data.file
end:species_external1

begin:species1
   rho=rho(1)*partdens
   temp_y=temp_x(1)
end:species1
\end{boxverbatim}
It is also possible to load data from external files using the internal
autoloader routines. This is done as follows:
\begin{boxverbatim}
SUBROUTINE ic_early

  INTEGER :: ispecies
  INTEGER :: error
  INTEGER(KIND=MPI_OFFSET_KIND) :: offset

  DO ispecies = 1, n_species
    offset = 0
    CALL load_single_array_from_data_file("Data/data.file", &
        initial_conditions(species_id)%rho, offset, error)
    offset = 80000
    CALL load_single_array_from_data_file("Data/data.file", &
        initial_conditions(species_id)%temp(:,:,1), offset, error)
    CALL load_single_array_from_data_file("Data/data.file", &
        initial_conditions(species_id)%temp(:,:,2), offset, error)
  ENDDO

END SUBROUTINE ic_early
\end{boxverbatim}

\subsection{Deferred Evaluation Objects (DEOs)}
In addition to custom constants it is also possible to set up deferred
execution objects. The difference between the two is that constants are
immediately evaluated when defined, so they cannot include the spatial
information used in defining initial conditions. DEOs are evaluated at the
point in the deck where they are used so they can include spatial
information. DEOs are relatively memory intensive, so should only be used when
required. An example would be:
\begin{boxverbatim}
begin:deo
   r=sqrt(x^2+y^2)
end:deo

begin:species1
   # first set density in the range 0->1
   # cut down density in x direction
   rho=if ((r lt 1),1.0,0.2)

   # multiply density by real particle density
   rho=rho(1)*partdens

   # Set the temperature to be zero
   temp_x=0.0
   temp_y=temp_x(1)

   # Set the minimum density for this species
   minrho=0.3*partdens
end:species1
\end{boxverbatim}
This example produces a circular rather than a square density enhancement.

\subsection{Lasers}
In the latest version of {\EPOCH}, the ability to add EM wave sources such as
lasers at boundaries has been added. To use lasers, set the boundary that you
wish to have a laser on to be of type \inlinecode{simple\_laser} and then
specify one or more lasers attached to that boundary. Lasers may be specified
anywhere initial conditions are specified.

\subsubsection{Lasers in the external initial conditions file}
To introduce a laser into the code from the input deck, you simply add a
``laser'' block for every laser that you want. It is perfectly valid to add as
many lasers as required to a boundary. A laser block looks like:
\begin{boxverbatim}
begin:laser
   boundary=x_min
   amp=3.88191e13
   profile=exp(-(y^2))
   freq=1.78e15
   pol_angle=0.0
   phase=0.0
   t_profile=1.0
   t_start=0.0
   t_end=end
   id=1
end:laser
\end{boxverbatim}

\begin{itemize}
\item boundary - The boundary on which to attach the laser. Options are x\_min,
  x\_max, y\_min, y\_max, z\_min and z\_max. This must be the first element of
  the block. ``left'', ``right'', ``down'', ``up'', ``back'' and ``front'' are
  accepted as synonyms.
\item amp - The amplitude of the laser.
\item profile - The spatial profile of the laser. This should be a spatial
  function not including any values in the direction normal to the boundary
  on which the laser is attached, and the expression will be evaluated at the
  boundary. This is calculated at t=0, so no time dependant part may be used.
\item freq - The frequency of the laser.
\item pol\_angle - The polarisation angle of the magnetic field of the laser
  from the ignorable direction. If pol\_angle=0.0 then the magnetic field
  perturbation is only in bz.
\item phase - The phase profile of the laser wavefront. Once again this is
  calculated at t=0, so no time dependant part may be used.
\item t\_profile - The temporal profile of the laser. This is calculated at
  each timestep, but you cannot specify spatial information. If this is not
  specified then the code will attempt to get a temporal profile using the
  \inlinecode{custom\_laser\_time\_profile} subroutine inside the code.
\item t\_start - The time at which to start applying the laser. Can be
  ``start'' to just start at the beginning of the simulation.
\item t\_end - The time at which to stop applying the laser. Can be ``end'' to
  just end when the simulation finishes.
\item id - A user supplied id which identifies a laser. This does not have to
  be unique, and is only used when specifying temporal profiles inside the
  code.
\end{itemize}

If you add multiple laser blocks to the initial conditions file then the
multiple lasers will be additively combined on the boundary.

\subsubsection{Lasers in the internal initial conditions}
You can add lasers at any point in the internal initial conditions routines,
using the internal representation of a laser in the code, which is a Fortran
TYPE called \inlinecode{laser\_block}.
\begin{boxverbatim}
TYPE :: laser_block
  ! Boundary to which laser is attached
  INTEGER :: direction
  ! A unique id number for the laser (not used directly by EPOCH)
  ! Only used if hard coding time profiles
  INTEGER :: id
  REAL(num), DIMENSION(:), ALLOCATABLE :: profile
  REAL(num), DIMENSION(:), ALLOCATABLE :: phase

  LOGICAL :: use_time_function
  TYPE(primitive_stack) :: time_function

  REAL(num) :: amp = 0.0_num, freq = 1.0_num
  REAL(num) :: pol_angle = 0.0_num, angle = 0.0_num
  REAL(num) :: t_start = 0.0_num, t_end = 0.0_num
END TYPE laser_block
\end{boxverbatim}
Most of these items are the same as their equivalents in the input deck and
are set in the same way. The detailed description is
\begin{itemize}
\item direction - The boundary on which the laser is attached. This is
  automatically set when the laser is attached to a boundary.
\item id - Exactly the same as the id property from the input deck.
\item profile - The spatial profile for the laser. The array is allocated by
  the initial call to \inlinecode{init\_laser} and must be populated between
  (1:nx) for a boundary on the top or bottom, or (1:ny) for a boundary on the
  left or right.
\item phase - The phase of the wavefront for the laser. See profile.
\item use\_time\_function - A logical variable which tests whether to use an
  input deck specified time profile or a hard coded profile. If you're setting
  up a laser inside the code, this should be false.
\item time\_function - The Parser Stack which represents the temporal profile of
  the laser if specified in the input deck. Don't set it to anything if
  specifying an internal initial condition.
\item amp, freq, pol\_angle etc. - The same as in the input deck.
\end{itemize}

To actually set up a laser is fairly simple, as shown below. The following
code creates a laser and attaches it to the left hand boundary.
\begin{boxverbatim}
TYPE(laser_block), POINTER :: new_laser

ALLOCATE(new_laser)
CALL init_laser(new_laser, c_bd_x_min)
new_laser%amp = 3.88191e13_num
new_laser%freq = 1.78e15_num
new_laser%id = 1
new_laser%phase = 0.0_num
DO iy = 1, ny
  new_laser%profile(iy) = exp(-y(iy)**2)
ENDDO
CALL attach_laser(new_laser)
\end{boxverbatim}
After the call to \inlinecode{attach\_laser} the laser is attached to the
correct boundary. Since new\_laser is a pointer, it is valid to then reallocate
the same variable again to create and attach further lasers. It is valid to
delete a laser without attaching it, although obviously the laser is lost. It
is not valid to call \inlinecode{laser\_init} on an already initialised laser.

\subsubsection{Internal laser time profiles}
If you specify a laser's time profile in the input deck then you need take no
further action. However, if you specify a laser within the code or wish to
have more control, you can specify the laser time profile internally. In this
case, the relevant function is in
\inlinecode{user\_interaction/custom\_laser.f90}.
\begin{boxverbatim}
FUNCTION custom_laser_time_profile(laser)

  TYPE(laser_block), INTENT(IN) :: laser
  REAL(num) :: custom_laser_time_profile

  IF (laser%id .EQ. 1) THEN
    custom_laser_time_profile = 1.0
  ELSE
    custom_laser_time_profile = EXP(-((time-1.0e-12_num)/1.0-e13_num)**2)
  ENDIF

END FUNCTION custom_laser_time_profile
\end{boxverbatim}
The function returns just the time profile part of the laser and the
amplitude specified when the laser is set up is still applied, so normally you
would want custom\_laser\_time\_profile to run between 0 and 1.

\subsection{Distribution functions}
PIC codes effectively represent a Lagrangian Monte-Carlo sampling of the phase
space of Vlasov's equation. Sometimes it is useful to reconstruct part of the
full phase space. This is controlled by the distribution function option.
Distribution function output can be specified in any of the initial conditions
sections of the main code.

\subsubsection{Distribution function units}
Calculating distribution functions requires some degree of integration of data
leading to various possible ways of normalising the resulting distribution
function. In {\EPOCH}, distribution functions are normalised so that the value
at every point of the distribution function is the number of particles within
that cell of the distribution function, ignoring all phase space directions
which are not considered as an axis of the distribution function. Summing the
distribution function should give the total number of real particles in the
simulation.

\subsubsection{Distribution function limitations}
At present, the code to calculate the distribution functions has one
limitation: it ignores particle shape functions when calculating properties
on the spatial axis, meaning that the result is less smooth than normal
properties from the code. This will be improved in a future release.

\subsubsection{Distribution functions in the input deck}
\begin{boxverbatim}
begin:dist_fn
   name=x_px
   ndims=2
   dumpmask=always

   direction1=dir_x
   direction2=dir_px

   # range is ignored for spatial coordinates
   range1=(1,1)
   range2=(-3.0e-20,3.0e-20)

   restrict_py=(-3.0e-20,3.0e-20)

   # resolution is ignored for spatial coordinates
   resolution1=1
   resolution2=100

   include_species_1=T
   include_species_2=T
end:dist_fn
\end{boxverbatim}
\begin{itemize}
\item name - The name that the distribution function should have in output
  dumps.
\item ndims - The number of dimensions that the distribution function should
  have. Currently only 2 or 3 are valid numbers.
\item direction\{n\} - For each dimension specified in {\bf ndims} it is
  necessary to specify the direction of the full phase space which corresponds
  to the axis. Valid directions are: dir\_x, \{dir\_y\}, \{dir\_z\}, dir\_px,
  dir\_py, dir\_pz. Spatial directions are only valid in high enough dimension
  codes, i.e. EPOCH1D only has a valid dir\_x.
\item range\{n\} - The range of this dimension, specified as ({\it lower},{\it
    upper}). This is ignored for spatial dimensions. Any particle which exceeds
  the range is ignored.
\item resolution\{n\} - The number of gridpoints in each dimension. This
  number is ignored for any specified spatial dimension, due to the
  requirements of the parallelism scheme of {\EPOCH}.
\item restrict\_\{x,y,z,p\_x,p\_y,p\_z\} - Restrictions are specified in the
  same way as ranges, but have a subtly different behaviour. Ranges specify
  the range of a visible axis on the resulting distribution function, whereas
  restrictions allow you to specify a valid window of particle properties even
  for properties which are not being used as axes. It is possible to set a
  restriction that is more restrictive than the range applied. This is not
  trapped as an error and such parts of the distribution function are
  guaranteed to be empty.
\item include\_species\_\{n\} - Whether to calculate the distribution function
  for this species.
\end{itemize}

\subsubsection{Distribution functions in the internal initial conditions}
Using distribution functions within {\EPOCH} is similar to using them from an
input deck. To add a distribution function output in any of the initial
conditions subroutines, the sequence looks like:
\begin{boxverbatim}
  TYPE(distribution_function_block), POINTER :: work

  ALLOCATE(work)
  CALL setup_dist_fn(work)
  work%name = "x_px"
  work%ndims = 2
  work%dumpmask = c_io_always
  work%directions(1) = c_dir_x
  work%directions(2) = c_dir_px
  ! range is ignored for spatial directions, so don't bother setting
  work%range(2,:) = (/-3.0e-20, 3.0e-20/)
  work%resolution(2) = 100
  ! To get the first index for a restriction, just count through this list,
  ! ignoring directions which do not apply to your dimensionality
  ! (i.e. c_dir_z only in EPOCH3D)
  ! c_dir_x, c_dir_y, c_dir_z, c_dir_px, c_dir_py, c_dir_pz
  work%restrictions(4,:) = (/-3.0e-20, 3.0e-20/)
  work%use_species(1:2) = .TRUE.
  CALL attach_dist_fn(work)
\end{boxverbatim}
This code would create exactly the same distribution function as the
preceding input deck section.

\subsection{Particle Probes}
Sometimes it is useful to consider all the properties of particle which pass
through a point/line/plane (depending on dimension) in the simulation. To
allow this, it is possible to specify one or more {\it Particle Probe} blocks
in the input deck. These record copies of all particles which cross a
point/line/plane in a given direction which meet minimum and maximum kinetic
energy criteria and output the particle properties into the normal output
files. Particle probes record the positions, momenta and weight of all
particles passing through the plane. To use particle probes, the code must be
compiled with the \inlinecode{-DPARTICLE\_PROBES} compiler option. It is not
currently possible to have a particle probe which is parallel to the x-axis
(although arbitrarily close is acceptable). \\

At present, particle probes exist in all dimensionalities of the code, but the
way in which they are specified has only been finalised in 2D, so only this
version of the input deck parameters will be specified. To see how to use the
versions in other dimensions, please either contact the code maintainer or
look at the file \inlinecode{src/deck/deck\_eio\_particle\_probe\_block.F90}.\\

In the 2D code, the probe is specified in terms of two control nodes which
specify the ends of a line. These are specified in terms of four control
values \inlinecode{x1, y1, x2, y2}. Since particles are only recorded if they
cross the line in one direction, it should be explained how this
works. Consider a vertical line where \inlinecode{x1,y1} is the bottom node
and \inlinecode{x2,y2} is the top node, the code will then record particles
travelling from left to right. If these are reversed so that
\inlinecode{x1,y1} are the top node and \inlinecode{x2,y2} are the bottom node
then particles will be recorded if they are travelling from right to left. If
you want to record particles travelling in both directions then use two
particle probes.

\subsubsection{Particle probes in the input deck}
\begin{boxverbatim}
begin:probe
   name=electron_back_probe

   x1=50.0e-6
   y1=-50.0e-6

   x2=50.0e-6
   y2=50.0e-6

   probe_species=1
   ek_min=0.0
   ek_max=-1.0

   dump=always
end:probe
\end{boxverbatim}
\begin{itemize}
\item name - The name that the probe should have in output dumps. Output
  variables are then named this as a prefix. For example, the block shown
  above will result in the name \inlinecode{electron\_back\_probe\_px} for
  the x momentum. The particle positions would just be called
  \inlinecode{electron\_back\_probe}.
\item x1 - The x position of the first probe node.
\item y1 - The y position of the first probe node.
\item x2 - The x position of the second probe node.
\item y2 - The y position of the seconds probe node.
\item probe\_species - The species number to which this probe should be
  applied. To probe several species, use several probe blocks in the input
  deck.
\item ek\_min - The minimum kinetic energy of particles to store information
  about. Set to 0 for no minimum kinetic energy.
\item ek\_max - The maximum kinetic energy of particles to store information
  about. Set to -1 for no maximum kinetic energy.
\item dump - The dump code for this particle probe. This is the same as that
  for the main output controls in \inlinecode{input.deck}. Note that the code
  has to store copies of particles which pass through the probe until a dump
  occurs. This means that the code's memory requirements can increase
  drastically if this code only dumps probe information infrequently. If this
  is set to \inlinecode{never} then the code effectively never uses the probe.
\end{itemize}

\subsection{Restarting {\EPOCH} from previous output dumps}
Another possible way of setting up initial conditions in {\EPOCH} is to load in
a previous output dump and use it to specify initial conditions for the
code. The effect of this is to restart the code from the state that it was in
when the dump was made. To do this, you just
set the field ``restart\_snapshot'' to
the number of the output dump from which you want the code to restart. Because
of the way in which the code is written you cannot guarantee that the code will
successfully restart from any output dump. To restart properly, the
following{\it must} have been dumped
\begin{itemize}
\item Particle positions.
\item Particle momenta.
\item Particle species.
\item Particle weights.
\item Relevant parts of the electric field (If for example it is known that ez
  == 0 then it is not needed).
\item Relevant parts of the magnetic field.
\end{itemize}
Since the autoloader completely changes the position of particles etc. it is
not possible to combine restart dumps with {\it any} of the autoloader
routines. It is possible to use the manual particle control part of the
initial conditions to make changes to a restarted initial condition after the
restart dump is loaded. The output files don't include all of the information
needed to restart the code fully, but a restart dump will contain a full copy
of the relevant input deck files which can be unpacked before running the
code. In future the code may well be changed to allow full restart from
output files.\\

If specific ``restart'' dumps are specified in the input deck, or the
``force\_final\_to\_be\_restartable'' flag is set then in some cases the
output is forced to contain enough information to output all the data. These
restart dumps can be very large, and also override the ``dump'' parameter
specified for a species and output the data for that species anyway.

\subsection{Visualising {\EPOCH} output data}

\subsubsection{IDL}
{\EPOCH} is supplied with routines to allow the visualisation of data from
within the ITT IDL data analysis and visualisation language. To start IDL with
the routines loaded, just type\\
\indent\inlinecode{idl Start}\\
in the main {\EPOCH} directory. To load an output file, just type\\
\indent\inlinecode{data=getdata(\{dumpnum\},wkdir='Data')}\\
the return value from this function is an IDL structure which contains all the
data which is available in the output dump. Some of the data which has been
loaded is itself a structure because of the need to store some metadata as
well as the primary data. For example particle position information in the
x direction would be accessed using\\
\indent\inlinecode{data.particles.particlepositions(*,0)}\\
and in the y direction,\\
\indent\inlinecode{data.particles.particlepositions(*,1)}\\
etc. It is also possible to get more information about the data held in a
given data dump by typing\\
\indent\inlinecode{data=getdata(\{dumpnum\},wkdir='Data',/variables)}\\
which prints the name and type of data held in the file, but loads no data. To
load a specific variable, just add a /\{variable\_name\} to the load command\\
\indent\inlinecode{data=getdata(\{dumpnum\},wkdir='Data',/vx)}\\

\subsubsection{LLNL VisIt}
LLNL's VisIt software is a parallel data visualisation package
(\inlinecode{https://wci.llnl.gov/codes/visit/}). {\EPOCH} comes with source
code for the plug-in needed to allow VisIt to load the CFD output files which
are generated by {\EPOCH}. There are full manuals for VisIt which can be
downloaded from the above link so no further details will be given here. To
build the plug-in when the VisIt bin directory is in the user's path, just
type ``make visit'' at the root of the {\EPOCH} directory. For more experienced
users of VisIt, the xml file which is used to generate the plug-in is supplied
in the VisIt subdirectory, called cfd.xml.

\subsubsection{Mathworks MatLab}
There are also routines to allow the loading of CFD files from Mathworks
MatLab software. These are still under development so are not shipped with the
code. If they are required, they can be supplied on demand.

\section{{\EPOCH} for advanced end users}

\subsection{{\EPOCH} internal representation of particles}
Since not all problems in plasma physics can be described in terms of an
initial distribution of thermal plasma, it is also possible to manually
control properties of each individual pseudoparticle for an initial
condition. This takes place in the subroutine \inlinecode{manual\_load} in the
user\_interaction/ic\_module.f90. Manual loading takes place after
all the autoloader phases, to allow manual tweaking of autoloader specified
initial conditions.

Inside the code, particles are represented by a Fortran90 TYPE called
``particle''. The current definition of this type (in 2D) is:
\begin{boxverbatim}
! Object representing a particle
  TYPE :: particle
    REAL(num), DIMENSION(3) :: part_p
    REAL(num), DIMENSION(2) :: part_pos
#ifdef PER_PARTICLE_WEIGHT
    REAL(num) :: weight
#endif
#ifdef PER_PARTICLE_CHARGEMASS
    REAL(num) :: charge
    REAL(num) :: mass
#endif
    TYPE(particle), POINTER :: next, prev
#ifdef PART_DEBUG
    INTEGER :: processor
    INTEGER :: processor_at_t0
#endif
  END TYPE particle
\end{boxverbatim}
Note the presence of the preprocessor directives, meaning that charge and mass
only exist if the \inlinecode{-DPER\_PARTICLE\_CHARGEMASS} define was put in
the makefile. If you want to access a property that does not seem to be
present, check the preprocessor defines.

The ``particle'' properties can be explained as follows:
\begin{itemize}
\item part\_p - The momentum in 3 dimensions for the particle. This is always
  of size 3.
\item part\_pos - The position of the particle in space. This is of the same
  size as the dimensionality of the code.
\item weight - The weight of this particle. The number of real particles
  represented by this pseudoparticle.
\item charge - The particle charge. If the code was compiled without per
  particle charge, then the code uses the charge property from
  TYPE(particle\_family).
\item mass - The particle rest mass. If the code was compiled without per
  particle mass, then the code uses the mass property from
  TYPE(particle\_family).
\item next, prev - The next and previous particle in the linked list which
  represents the particles in the current species. This will be explained in
  more detail later.
\item processor - The rank of the processor which currently holds the
  particle.
\item processor\_at\_t0 - The rank of the processor on which the particle
  started.
\end{itemize}

Collections of particles are represented by another Fortran TYPE, called
\inlinecode{particle\_list}. This type represents all the properties of a
collection of particles and is used behind the scenes to deal with
inter-processor communication of particles. The definition of the type is:
\begin{boxverbatim}
  TYPE :: particle_list
    TYPE(particle), POINTER :: head
    TYPE(particle), POINTER :: tail
    INTEGER(KIND=8) :: count
    ! Pointer is safe if the particles in it are all unambiguously linked
    LOGICAL :: safe

    TYPE(particle_list), POINTER :: next, prev
  END TYPE particle_list
\end{boxverbatim}
\begin{itemize}
\item head - The first particle in the linked list.
\item tail - The last particle in the linked list.
\item count - The number of particles in the list. Note that this is NOT MPI
  aware, so reading count only gives you the number of particles on the local
  processor.
\item safe - Any particle\_list which a user should come across will be a safe
  particle\_list. Don't change this property.
\item next, prev - For future expansion it is possible to attach particle\_lists
  together in another linked list. This is not currently used anywhere in the
  code.
\end{itemize}

An entire species of particles is represented by another Fortran TYPE, this
time called \linebreak\inlinecode{particle\_family}. This represents all the
properties which are common to all particles in a species. The definition is:
\begin{boxverbatim}
  TYPE :: particle_family
    CHARACTER(entry_length) :: name

    TYPE(particle_list) :: attached_list
    TYPE(particle_family), POINTER :: next, prev
    INTEGER :: id
    LOGICAL :: dump

    REAL(num) :: charge
    REAL(num) :: mass
    INTEGER(KIND=8) :: count
  END TYPE particle_family
\end{boxverbatim}

\begin{itemize}
\item name - The name of the particle species, used in the output dumps etc.
\item next, prev - Particle species are also linked together in a linked
  list. This is used internally by the output dump routines, but should not be
  used by end users.
\item id - The species number for this species. This is the same number as is
  used in the input deck to designate the species.
\item charge - The charge in Coulombs. Even if PER\_PARTICLE\_CHARGEMASS is
  specified, this is still populated from the input deck, and now refers to
  the reference charge for the species.
\item mass - The mass in kg.
\item count - The global number of particles of this species (NOTE may not
  be accurate). This will only ever be the number of particles on this
  processor when running on a single processor. While this property will be
  accurate when setting up initial conditions, it is only guaranteed to be
  accurate for the rest of the code if the code is compiled with the correct
  preprocessor options.
\end{itemize}

\subsection{Linked Lists}
Linked lists are a standard computer programming technique which is still
slightly unusual in Fortran, and may well be unfamiliar to many Fortran
programmers. They effectively allow you to have an array of arbitrary length,
although this comes with various trade-offs about memory locality and speed of
accessing elements. The general concept is that of a chain where each link in
the chain only knows about the previous link in the chain and the next link in
the chain. Although there are schemes for doing this in languages which don't
have pointers, the normal method of implementing linked lists is to use
pointers to point to previous and next elements in the list, and this is how
they are implemented in {\EPOCH}. Since both linked lists and Fortran pointers
are slightly esoteric concepts, while being key to the operation of {\EPOCH} a
brief overview of them is presented here.\\

The simplest possible form of a linked list element would be a TYPE which
looks like:
\begin{boxverbatim}
TYPE :: linked_list
  TYPE(linked_list), POINTER :: next
  TYPE(linked_list), POINTER :: prev
END TYPE linked_list
\end{boxverbatim}

You also have to have a pointer to the start of the list, and to speed up
adding new elements to the list, you normally also keep a pointer to the
last element of the list. Therefore, you would also have variables which look
like:
\begin{boxverbatim}
TYPE(linked_list) :: head, tail
\end{boxverbatim}

Since Fortran pointers are not initialised in any particular state, you have
to remember to set the head and tail pointers to explicitly point nowhere
(normally called a null pointer by analogy with the older C style
pointers). This is done using the nullify command.
\begin{boxverbatim}
NULLIFY(head)
NULLIFY(tail)
\end{boxverbatim}

The same thing is important when creating a new linked list element, so you
would normally have a creation function for linked list elements.
\begin{boxverbatim}
SUBROUTINE create_element(element)

  TYPE(linked_list), POINTER, INTENT(INOUT) :: element

  ALLOCATE(element)
  NULLIFY(element%next)
  NULLIFY(element%prev)

END SUBROUTINE create_element
\end{boxverbatim}
Note that the allocate function can be used on pointers in the same way that
it can be used with variables which have the allocatable attribute. However,
there is one important difference between a pointer and an allocatable
variable. If you attempt to allocate an already allocated variable which has
the allocatable attribute then the code will fail, whereas allocating an
already allocated pointer is perfectly valid, and will allocate the new
variable and point the pointer to it. This does not deallocate the memory that
the pointer previously pointed to, and Fortran does not have a ``garbage
collector'' which deallocates memory no longer accessible. So if you
allocate a pointer which already points to a variable, it is very important
that you have another pointer somewhere which points to the same memory. Once
you no longer have a pointer to an area of memory, that area of memory is
completely inaccessible and cannot even be deallocated. This is termed a
memory leak and for programs which run for many cycles and have a memory leak
on each cycle, the entire memory can very quickly be used up.

So, to add a new element to the list you would have a subroutine which looks
like:
\begin{boxverbatim}
SUBROUTINE add_element(element)

  TYPE(linked_list), POINTER, INTENT(IN) :: element

  IF (.NOT. ASSOCIATED(head)) THEN
    ! Adding first element to list, so just set
    ! both head and tail to the element
    head=>element
    tail=>element
    RETURN
  ENDIF

  tail%next=>element
  element%prev=>tail
  tail=>element

END SUBROUTINE add_element
\end{boxverbatim}
This subroutine adds the new Fortran operator of ``$=>$'' which means ``points
to''. Unlike C or similar languages, Fortran pointers try to be partially
transparent to the end user, so the following code would fail:
\begin{boxverbatim}
PROGRAM test

  REAL, TARGET :: a = 10.0
  REAL, POINTER :: b

  b = a

END PROGRAM test
\end{boxverbatim}

This happens because Fortran will try to copy the value of ``a'' into ``b''.
However, ``b'' is a pointer which hasn't been initialised, so the code will
crash when it tries to copy the data in (in theory, the code may not crash if
the uninitialised ``b'' pointer happens to point somewhere in memory which is
a valid target, but this is very unlikely). Note also that ``a'' has the
attribute ``TARGET''. The target attribute means that it is possible to point
a pointer to this variable. You can only point a pointer to a variable which
is either a pointer itself or has the target attribute. This is to try and
keep Fortran pointers ``safer'' than C style pointers. The correct code would
use \inlinecode{b$=>$a}, at which point ``b'' is set to point to ``a'' and
can then be used everywhere in place of ``a''.

So, to set up a linked list of n elements, you would use the following code:
\begin{boxverbatim}
TYPE(linked_list), POINTER :: new
NULLIFY(new)

DO i = 1,n
  CALL create_element(new)
  CALL add_element(new)
ENDDO
\end{boxverbatim}

To then run through the elements of your newly created linked list, you would
use code like:
\begin{boxverbatim}
TYPE(linked_list), POINTER :: current

current=>head
DO WHILE(ASSOCIATED(current))
  ! Do stuff
  current=>current%next
ENDDO
\end{boxverbatim}

This code snippet introduces one new function ``ASSOCIATED'', which tells you
whether a pointer is a null pointer or not (this is why it is so important to
nullify new pointers, because ASSOCIATED on its own doesn't check whether a
pointer is valid, just whether or not it is a null pointer). You can also use
ASSOCIATED to check whether a pointer points to a particular object or not, in
which case the syntax is #RESULT = ASSOCIATED(b, TARGET=a)#, which
returns true if ``b'' points to ``a'', or false if it doesn't, even if ``b''
is a valid pointer pointing to something else. It also introduces the way in
which you must use linked lists in {\EPOCH}. The execution flow is as follows
\begin{itemize}
\item Point current to the current element to the start of the linked list
  (head).
\item Iterate while current points to a valid element.
\item Perform whatever actions you want on current.
\item Point current to the next element in the chain.
\end{itemize}
This leads to the slightly counter intuitive behaviour where even though the
loop only acts on the variable named ``current'', all of the elements in the
list are operated on. Although there are many tricks which can be performed
with linked lists, the only other aspect which needs to be explained is how
to delete elements. A subroutine to remove a single element from a linked list
would look like:
\begin{boxverbatim}
SUBROUTINE remove_element(element)

  TYPE(linked_list), POINTER, INTENT(INOUT) :: element

  IF (ASSOCIATED(element%prev)) THEN
    ! Previous element exists
    element%prev%next=>element%next
  ELSE
    ! Previous element does not exist therefore element is the head. When
    ! element is removed the head is the element after the one being removed
    head=>element%next
  ENDIF

  IF (ASSOCIATED(element%next)) THEN
    ! next element exists
    element%next%prev=>element%prev
  ELSE
    ! next element does not exists therefore element is the tail. When element
    ! is removed the head is the element before the one being removed
    tail=>element%prev
  ENDIF

END SUBROUTINE remove_element
\end{boxverbatim}

Once again, this code looks slightly counter-intuitive, but if you go through
step by step, it's fairly simple. In the following discussion the element
being removed is called ``C'', the element before ``C'' (if it exists) is
called ``P'' and the element after ``C'' (if it exists) is called ``N''
\begin{itemize}
\item Check whether \inlinecode{C}'s prev element exists, this means that
  \inlinecode{P} exists.
\item If \inlinecode{P} exists then the element to be removed isn't at the
  start of the chain. When \inlinecode{C} is removed, we need
  \inlinecode{P}\%next to point to \inlinecode{C}\%next. This leads to the odd
  looking element\%prev\%next$=>$ element\%next syntax.
\item If \inlinecode{P} does not exist then \inlinecode{C} is at the the start
  of the chain. In order to not leave the chain orphaned when \inlinecode{C}
  is removed, we need head to point to \inlinecode{C}\%next.
\item Exactly the same logic applies for updating the element after
  \inlinecode{C}.
\item Check whether \inlinecode{C}'s next element exists, this means that
  \inlinecode{N} exists.
\item If \inlinecode{N} exists then the element to be removed isn't at the end
  of the chain. When \inlinecode{C} is removed, we need \inlinecode{N}\%prev
  to point to \inlinecode{C}\%prev.
\item If \inlinecode{P} does not exist then \inlinecode{C} is at the the start
  of the chain. In order to not leave the chain orphaned when \inlinecode{C}
  is removed, we need head to point to \inlinecode{C}\%next.
\end{itemize}

Therefore, code to remove some elements from a linked list would look like:
\begin{boxverbatim}
TYPE(linked_list), POINTER :: current, next

current=>head
DO WHILE(ASSOCIATED(current))
  next=>current%next
  IF (dealloc) THEN
    CALL remove_element(current)
    DEALLOCATE(current)
  ENDIF
  current=>next
ENDDO
\end{boxverbatim}
Note that ``current'' must be deallocated explicitly even after it has been
removed from the linked list to prevent a memory leak. Note also that the
pointer to the ``next'' element is saved before ``current'' is deallocated.
This is not necessary but means that there is only one IF statement rather
than the two otherwise required.

\subsection{Setting the particle properties}
After all the descriptions of the types, actually setting the properties of
the particles is fairly simple. The following is an example which positions
the particles correctly in 2D space, but doesn't set any momentum.
\begin{boxverbatim}
SUBROUTINE manual_load

  TYPE(particle), POINTER :: current
  INTEGER :: ispecies
  INTEGER :: idum, clock
  REAL(num) :: rpos

  ! Seed the random number generator off the system clock
  CALL SYSTEM_CLOCK(clock)
  ! Correct using the rank to ENSURE that different
  ! processors have different random numbers
  idum = -(clock+rank+1)

  DO ispecies = 1,n_species
    current=>particle_species(ispecies)%attached_list%head
    DO WHILE(ASSOCIATED(current))
      rpos = (random(idum) - 0.5_num)*2.0_num
      current%part_pos(1) = rpos
      rpos = random(idum)*length_x + x_min

      current%weight = 1.0_num
      current=>current%next
    ENDDO
  ENDDO

END SUBROUTINE manual_load
\end{boxverbatim}

This produces output identical to that shown in Figure~\ref{slab}. Note that
I completely ignored the question of domain decomposition when setting up the
particles. The code automatically moves the particles onto the correct
processor without user interaction.

In the above example, note that particle momentum was not specified and
particle weight was set to be a simple constant. Setting particle weight can
be very simple if you can get the pseudoparticle distribution to match the
real particle distribution, or quite tricky if this isn't possible. Remember
that the weight of each pseudoparticle is the number which locally transforms
the pseudoparticle number density into the real particle number density. If
the pseudoparticle distribution matches the real particle distribution then
this is simply a constant, i.e the ratio of the number of pseudoparticles in
the domain to the number of real particles in the domain. In more complicated
cases, it is probably better to use the autoloader than to manually set up the
density distribution.

If you want to set up thermal distributions manually, then there is a helper
function {\bf momentum\_from\_temperature} which returns a momentum sampled from
the correct distribution for a given temperature and particle mass. At present
{\bf momentum\_from\_temperature} uses a simple Maxwellian distribution to
describe the thermal distribution of the particles. This is correct for
particles which do not have a strongly relativistic thermal speed, where the
Maxwell-Juttner distribution is correct. The next version of {\EPOCH} will have
an option to use the Maxwell-Juttner distribution rather than a Maxwellian
distribution. An example of using {\bf momentum\_from\_temperature} is given
below.
\begin{boxverbatim}
SUBROUTINE manual_load

  TYPE(particle), POINTER :: current
  INTEGER :: ispecies
  INTEGER :: idum, clock
  REAL(num) :: rpos, temp

  ! Seed the random number generator off the system clock
  CALL SYSTEM_CLOCK(clock)
  ! Correct using the rank to ENSURE that different
  ! processors have different random numbers
  idum = -(clock+rank+1)

  ! Simple uniform temperature
  temp = 1.0e6_num

  DO ispecies = 1, n_species
    current=>particle_species(ispecies)%attached_list%head
    DO WHILE(ASSOCIATED(current))
      rpos = (random(idum) - 0.5_num)*2.0_num
      current%part_pos(1) = rpos
      rpos = random(idum)*length_x + x_min

      current%weight = 1.0_num

      current%part_p(1) = &
          momentum_from_temperature(particle_species(ispecies)%mass, temp, idum)
      current%part_p(2) = &
          momentum_from_temperature(particle_species(ispecies)%mass, temp, idum)

      current=>current%next
    ENDDO
  ENDDO

END SUBROUTINE manual_load
\end{boxverbatim}

While the autoloader is capable of dealing with most required initial thermal
distributions, you may want to set up non-thermal initial conditions. The code
includes a helper function to select a point from an arbitrary distribution
function which can be used to deal with most non-thermal distributions. To use
the helper function, you need two to define two 1D arrays which are the x and
y axes for the distribution function. An example of using the helper function
is given below.
\begin{boxverbatim}
SUBROUTINE manual_load

  TYPE(particle), POINTER :: current
  INTEGER, PARAMETER :: nx_local = 100
  INTEGER :: ispecies
  INTEGER :: idum, clock, ix
  REAL(num) :: rpos, dx_local
  REAL(num) :: min_p, max_p, var, temperature
  REAL(num), DIMENSION(nx_local) :: x_axis, y_axis

  ! Seed the random number generator off the system clock
  CALL SYSTEM_CLOCK(clock)
  ! Correct using the rank to ENSURE that different
  ! processors have different random numbers
  idum = -(clock+rank+1)

  min_p = -3.0e-20
  max_p = 3.0e-20

  dx_local = (max_p-min_p)/REAL(nx_local, num) + min_p
  x_axis(1) = min_p
  DO ix = 2, nx_local
    x_axis(ix) = x_axis(ix-1)+dx_local
  ENDDO

  temperature = 10000.0_num
  DO ispecies = 1, n_species
    var = temperature*kb*particle_species(ispecies)%mass
    DO ix = 1, nx_local
      y_axis(ix) = EXP(-x_axis(ix)**2/var)
    ENDDO
    current=>particle_species(ispecies)%attached_list%head
    DO WHILE(ASSOCIATED(current))
      current%part_p(1) = sample_dist_function(x_axis, y_axis, idum)
      current=>current%next
    ENDDO
  ENDDO

END SUBROUTINE manual_load
\end{boxverbatim}

This is a very simple example where the distribution function is a Maxwellian
which could have been set up using the {\bf momentum\_from\_temperature}
function more easily. However, it does demonstrate how the
{\bf sample\_dist\_function} function works. It is not necessary to normalise
the distribution function, as this is done automatically by the
{\bf sample\_dist\_function} function.

\section{Customising {\EPOCH}}
This section details how to setup {\EPOCH} with additional parameters so that
it can be handed as a black box to new and/or inexperienced users. There are
various ways of doing this, ranging from simply giving the end user
parameterised input decks to adding new functions to the input deck parser. It
is assumed here that the end user will be using external (input deck) initial
conditions, otherwise simply point the end user to the internal autoloader
initial conditions in this manual.

\subsection{Parameterising input decks}
The simplest way to allow someone to use {\EPOCH} as a black box is to give them
the input.deck files that control the setup and initial conditions
of the code. The input deck is simple enough that a quick read through of the
relevant section of the manual should make it fairly easy for a new user to
control those features of the code, but the initial conditions can be complex
enough to be require significant work on the part of an unfamiliar user to
understand. In this case, it can be helpful to use the ability to specify
constants in an input deck to parameterise the file. So, to go back to a slight
variation on an earlier example:
\begin{boxverbatim}
begin:species1
   # first set density in the range 0->1
   # cut down density in x direction
   rho=if ((x gt -1) and (x lt 1),1.0,0.2)
   # cut down density in y direction
   rho=if ((y gt -1) and (y lt 1),rho(1),0.2)

   # multiply density by real particle density
   rho=rho(1)*100.0

   # Set the temperature to be zero
   temp_x=0.0
   temp_y=temp_x(1)

   # Set the minimum density for this species
   minrho=0.3*100.0
end:species1

begin:species2
   # Just copy the density for species 1
   rho=rho(1)

   # Just copy the temperature from species 1
   temp_x=temp_x(1)
   temp_y=temp_y(1)

   # Set the minimum density for this species
   minrho=0.3*100.0
end:species2
\end{boxverbatim}

The particle density (100.0) is hard coded into the deck file in several
places. It would be easier if this was given to a new user as:
\begin{boxverbatim}
begin:constant
   particle_density=100.0 # Particle number density
end:constant

begin:species1
   # first set density in the range 0->1
   # cut down density in x direction
   rho=if ((x gt -1) and (x lt 1),1.0,0.2)
   # cut down density in y direction
   rho=if ((y gt -1) and (y lt 1),rho(1),0.2)

   # multiply density by real particle density
   rho=rho(1)*particle_density

   # Set the temperature to be zero
   temp_x=0.0
   temp_y=temp_x(1)

   # Set the minimum density for this species
   minrho=0.3*particle_density
end:species1

begin:species2
   # Just copy the density for species 1
   rho=rho(1)

   # Just copy the temperature from species 1
   temp_x=temp_x(1)
   temp_y=temp_y(1)

   # Set the minimum density for this species
   minrho=0.3*particle_density
end:species2
\end{boxverbatim}

It is also possible to parameterise other elements of initial conditions in a
similar fashion. This is generally a good idea, since it makes the
initial conditions easier to read an maintain.

\subsection{Using DEOs to further parameterise initial conditions}
Again, this is just a readability change to the normal input.deck file, but it
also makes changing and understanding external initial conditions rather
simpler. In this case, entire parts of the initial conditions are moved into a
DEO in order to make changing them at a later date easier. For example:
\begin{boxverbatim}
begin:constant
   particle_density=100.0 # Particle number density
end:constant

begin:deo
   profile_x=if((x gt -1) and (x lt 1),1.0,0.2)
   profile_y=if((y gt -1) and (y lt 1),1.0,0.2)
end:deo

begin:species1
   # multiply density by real particle density
   rho=particle_density * profile_x * profile_y

   # Set the temperature to be zero
   temp_x=0.0
   temp_y=temp_x(1)

   # Set the minimum density for this species
   minrho=0.3*particle_density
end:species1

begin:species2
   # Just copy the density for species 1
   rho=rho(1)

   # Just copy the temperature from species 1
   temp_x=temp_x(1)
   temp_y=temp_y(1)

   # Set the minimum density for this species
   minrho=0.3*particle_density
end:species2
\end{boxverbatim}

This creates the same output as before. It is now trivial to modify the
profiles later. For example:
\begin{boxverbatim}
begin:constant
   particle_density=100.0 # Particle number density
end:constant

begin:deo
   profile_x=gauss(x,0.0,1.0)
   profile_y=gauss(y,0.0,1.0)
end:deo

begin:species1

   # multiply density by real particle density
   rho=particle_density * profile_x * profile_y

   # Set the temperature to be zero
   temp_x=0.0
   temp_y=temp_x(1)

   # Set the minimum density for this species
   minrho=0.3*particle_density
end:species1

begin:species2
   # Just copy the density for species 1
   rho=rho(1)

   # Just copy the temperature from species 1
   temp_x=temp_x(1)
   temp_y=temp_y(1)

   # Set the minimum density for this species
   minrho=0.3*particle_density
end:species2
\end{boxverbatim}

This changes the code to run with a Gaussian density profile rather then a step
function. Again, this can be extended as far as required.

\subsection{Adding new elements to the maths parser}
Sometimes the complexity in changing the input.deck file is due to the fact that
a function which must be used is fairly complex in form and is not supplied
with the core code. It must therefore be represented in the input deck maths
parser. This can be a significant cause of complexity for some problems, and
in this case, there are three options: Put up with it and implement in the
deck, use the internal initial conditions rather than the deck or extend the
maths parser to include your function. Extending the maths parser can either
be permanent (described later in the manual) or temporary (described
here). Temporarily adding elements to the parser is much the easier. It is
possible to add new constants and functions to the maths parser. It is hoped
that in a future release of {\EPOCH} this will be extended to allow custom
operators as well.\\

As an example, lets looking at adding a new function (lorentz) for a
Lorentzian distribution, and adding a new constant, phi.

\subsubsection{Registering your new constant/function}
All of the routines used in extending the maths parser are in the file
user\_interaction/custom\_parser.f90.  Before a new constant or function
can be defined it must be registered. In the registration phase the text
representation of the function or constant is given to the parser subroutines
and the user is returned an integer handle for the registered object. The
numerical handle must be stored so that that all of the functions in this
module can access it, so they should be placed after the \inlinecode{IMPLICIT
NONE} statement at the top of the file and defined as:
\begin{boxverbatim}
INTEGER :: c_func_lorentz
INTEGER :: c_const_phi
\end{boxverbatim}

Note that the names given to the constants is obviously at the developers
discretion, but these names comply with the {\EPOCH} style guide.
Actually registering the objects is done in the \inlinecode{register\_objects}
subroutine which should include lines to register functions and constants.
An example is given below.
\begin{boxverbatim}
SUBROUTINE register_objects

  c_func_lorentz = register_function("lorentz")
  c_const_phi = register_constant("phi")

END SUBROUTINE register_objects
\end{boxverbatim}

Note that the input deck parser is case sensitive, so the strings which are
given to \inlinecode{register\_function} and \inlinecode{register\_constant}
should be in the case that they will appear in the input deck. To follow the
{\EPOCH} style guide this should be all lowercase. At this point, the maths
parser would start to recognise the new function/constant, but would still
give error messages since they haven't yet been implemented.

\subsubsection{Setting up new constants}

Once a new constant has been registered it must be described using the
\inlinecode{custom\_constant} function. In 2D this function looks like:
\begin{boxverbatim}
FUNCTION custom_constant(opcode, ix, iy, errcode)

  INTEGER, INTENT(IN) :: opcode, ix, iy
  INTEGER, INTENT(INOUT) :: errcode
  REAL(num) :: custom_constant

  ! Leave these lines in place. They cause the code to throw an error if
  ! The opcode is unknown
  custom_constant = 0.0_num
  errcode = IOR(errcode, c_err_unknown_element)

END FUNCTION custom_constant
\end{boxverbatim}

The parameters are

\begin{itemize}
\item opcode - The operator code of the constant requested. This will be the
  integer handle returned from \inlinecode{register\_constant}.
\item ix, iy, iz - Some constants are actually evaluated at specific points in
  space and ix, iy, iz are the gridpoint number of the location currently being
  evaluated. If you are specifying a simple constant then just ignore
  these. If your constant does depend upon space then directly subscript your
  array with ix, iy, iz as needed to read the correct location.
\item errcode - The error code which should be passed back to the
  parser. If for some reason you cannot evaluate your constant then you should
  \inlinecode{IOR} errcode with the appropriate error code (all the error
  codes are listed in appendix A). Note that errcode should never be SET to
  any specific error code when extending the parser, since this might
  overwrite errors put in place earlier in the parsing sequence. This is
  different to extending the input deck where the error code is set.
\end{itemize}

The function should just return the evaluated value of the constant requested
by \inlinecode{opcode}. This might look like:
\begin{boxverbatim}
FUNCTION custom_constant(opcode, ix, iy, errcode)

  INTEGER, INTENT(IN) :: opcode, ix, iy
  INTEGER, INTENT(INOUT) :: errcode
  REAL(num) :: custom_constant

  IF (opcode .EQ. c_const_phi) THEN
    custom_constant = pi
    RETURN
  ENDIF

  ! Leave these lines in place. They cause the code to throw an error if
  ! The opcode is unknown
  custom_constant = 0.0_num
  errcode = IOR(errcode, c_err_unknown_element)

END FUNCTION custom_constant
\end{boxverbatim}

Note that when \inlinecode{opcode} is successfully recognised, the code sets
the return value and returns straight away. This is how all constants should
work, since the last line forces the function to return an error code. This
last line is in place to trap people registering constants but never defining
them. Without this line, it is possible to define a constant which is
never specified and have the code complete OK with a random value for that
constant.

The constant ``phi'' should now work fine when used anywhere in the input deck
and will return a value of $\pi$.

\subsubsection{Setting up new functions}

Setting up the new function \inlinecode{lorentz} is very similar to setting up
the new constant. The relevant function is \inlinecode{custom\_function} and
when empty looks like:
\begin{boxverbatim}
FUNCTION custom_function(opcode, ix, iy, errcode)

  INTEGER, INTENT(IN) :: opcode, ix, iy
  INTEGER, INTENT(INOUT) :: errcode
  REAL(num) :: custom_function
  REAL(num) :: values(5)

  ! Leave these lines in place. They cause the code to throw an error if
  ! The opcode is unknown
  custom_function = 0.0_num
  errcode = IOR(errcode, c_err_unknown_element)

END FUNCTION custom_function
\end{boxverbatim}

The parameters are

\begin{itemize}
\item opcode - The operator code of the constant requested. This will be the
  integer handle returned from \inlinecode {register\_function}.
\item ix, iy, iz - Some functions are evaluated differently at specific points
  in space and ix, iy, iz are the gridpoint number of the location currently
  being evaluated. If you are specifying a simple function then just
  ignore these. If your function does depend upon space then directly
  subscript your array with ix, iy, iz as needed to read the correct location.
\item errcode - The error code which should be passed back to the
  parser. If for some reason you cannot evaluate your function then you should
  \inlinecode{IOR} errcode with the appropriate error code. Note that errcode
  should never be SET to any specific error code, since this might overwrite
  errors put in place earlier in the parsing sequence.
\end{itemize}

The function should return the value of your evaluated constant. The
parameters which are passed to the function can be retrieved by the function
\inlinecode {get\_values(n, values)}, where \inlinecode {n} is the number of
parameters to be returned and \inlinecode{values} is a \inlinecode{REAL(num)}
array of length \inlinecode{n} which will hold the returned values .  In this
implementation of the Lorentzian function there are three parameters: The
dependent variable, the location parameter and the scale parameter. The code
to implement the function therefore looks like:
\begin{boxverbatim}
FUNCTION custom_function(opcode, ix, iy, errcode)

  INTEGER, INTENT(IN) :: opcode, ix, iy
  INTEGER, INTENT(INOUT) :: errcode
  REAL(num) :: custom_function
  REAL(num) :: values(5)

  IF (opcode .EQ. c_func_lorentz) THEN
    CALL get_values(3, values(1:3))
    ! values(1) - Dependent variable
    ! values(2) - location parameter
    ! values(3) - scale parameter
    custom_function = values(3)**2/((values(1)-values(2))**2+values(3)**2)
    RETURN
  ENDIF

  ! Leave these lines in place. They cause the code to throw an error if
  ! The opcode is unknown
  custom_function = 0.0_num
  errcode = IOR(errcode, c_err_unknown_element)

END FUNCTION custom_function
\end{boxverbatim}

This function is then available at any point in the input deck and if I return
to the previous example ic.deck file, it would be used as follows:
\begin{boxverbatim}
begin:constant
   particle_density=100.0 # Particle number density
end:constant

begin:deo
   profile_x=lorentz(x,0.0,1.0)
   profile_y=lorentz(y,0.0,1.0)
end:deo

begin:species1
   # multiply density by real particle density
   rho=particle_density * profile_x * profile_y

   # Set the temperature to be zero
   temp_x=0.0
   temp_y=temp_x(1)

   # Set the minimum density for this species
   minrho=0.3*particle_density
end:species1

begin:species2
   # Just copy the density for species 1
   rho=rho(1)

   # Just copy the temperature from species 1
   temp_x=temp_x(1)
   temp_y=temp_y(1)

   # Set the minimum density for this species
   minrho=0.3*particle_density
end:species2
\end{boxverbatim}

It is therefore clear that the new lorentz function is essentially the same as
the built in gauss function. Note that due to the way that the parser works,
the end user is not required to deal with parameters which are themselves
maths expressions. They have been fully evaluated by the time they are
returned by \inlinecode{get\_values}. Note that the parser is not guaranteed to
be bulletproof. If a user calls \inlinecode{get\_values} requesting more
parameters than have been passed to the function then it will scramble the
stack which is used by the parser and cause the code to fail. Note that
calling \inlinecode{get\_values(2, values)} is not the same as calling
\inlinecode{get\_values(1, values)} twice, in fact calling
\inlinecode{get\_values(1, values)} multiple times will return the parameters in
{\it reverse} order. This is normal and is a feature of how the maths parser
operates. It is possible to use this property to write functions which have a
variable number of parameters, but this is not recommended.

\subsection{Adding new elements to the input deck}

For some types of changes to the code it is more convenient to have the end
user pass new parameters into the code. This can be for several reasons, and
the section on permanent additions to the input deck is given later in this
manual. At this stage, we will describe how to temporarily add new
elements to the input deck parser routines, allowing parameterising of
internal and manual initial conditions.

Custom input deck elements are setup in the file
\inlinecode{src/user\_interaction/custom\_deck.f90}. The function
\inlinecode{handle\_custom\_block} is called when a new block is started which
the core parser is not familiar with, and once for each element of a block. The
function \inlinecode{check\_custom\_blocks} is called once the entire deck has
been parsed and is used to check that all the elements which are required for
the code to run have been specified.

\subsubsection{handle\_custom\_block}
There are three parameters passed to \inlinecode{handle\_custom\_block}, which
are:
\begin{itemize}
\item blockname - The name of the block specified in the
  \inlinecode{begin:blockname} part of the input deck.
\item element - The name of the element in an input deck
  \inlinecode{element=value} pair.
\item value - The string representation of the value in an input deck
  \inlinecode{element=value} pair.
\end{itemize}

\inlinecode{handle\_custom\_block} is first called when a new block is begun
using \inlinecode{begin:blockname} and ``blockname'' is not recognised by the
core input deck parser. The first thing that it does is test whether or not it
is a valid custom block. The code does this by passing in the blockname with
\inlinecode{element} and \inlinecode{value} set to the special constant called
``blank''. When extending the input deck, an end user should check if either
\inlinecode{element} or \inlinecode{value} are set to the special constant
``blank'', and if they are then test to see whether the blockname is known or
not. If the blockname is known then the code should return the error code
\inlinecode{c\_err\_none} (No error). If the blockname is not known then the
code should return \inlinecode{c\_err\_unknown\_block} and the deck parser will
just skip the block. In operation, this looks like:
\begin{boxverbatim}
FUNCTION handle_custom_block(blockname, element, value)

  CHARACTER(LEN=entry_length), INTENT(IN) :: blockname, element, value
  INTEGER :: handle_custom_block
  TYPE(primitive_stack) :: output
  INTEGER :: ix, iy
  IF (str_cmp(blockname, "custom")) THEN
    IF (element .EQ. blank .OR. value .EQ. blank) THEN
       ! If element or value are blank then just testing block so return c_err_none
       handle_custom_block = c_err_none
       RETURN
     ENDIF
   ENDIF

  ! The following line must always be present
  handle_custom_block = c_err_unknown_block

END FUNCTION handle_custom_block
\end{boxverbatim}

In order to simply Fortran's rather annoying string handling behaviour, several
helper functions have been defined and the most used one is
\inlinecode{str\_cmp(string1, string2)}. This is a simple routine which returns
true if string1==string2 and false otherwise. It is case sensitive but can
deal with differing string lengths etc. The next stage is to deal with the
actual \inlinecode{element=value} pairs in the deck. Each time that a new pair
is read from the deck, \inlinecode{handle\_custom\_block} is called with
\inlinecode{element} and \inlinecode{value} having the values read from the
deck. To test for known elements they should just be checked against a known
list of names using \inlinecode{str\_cmp} and return the error code
\inlinecode{c\_err\_unknown\_element} if the element isn't a known element. This
looks like:
\begin{boxverbatim}
FUNCTION handle_custom_block(blockname, element, value)

  CHARACTER(LEN=entry_length), INTENT(IN) :: blockname, element, value
  INTEGER :: handle_custom_block
  TYPE(primitive_stack) :: output
  INTEGER :: ix, iy

  IF (str_cmp(blockname, "custom")) THEN
    IF (element .EQ. blank .OR. value .EQ. blank) THEN
      ! If element or value are blank then just testing block so return c_err_none
      handle_custom_block = c_err_none
      RETURN
    ENDIF
    handle_custom_block = c_err_unknown_element
    ! Now test for the real elements
    IF (str_cmp(element, "int_element")) THEN
      handle_custom_block = c_err_none
    ENDIF
    IF (str_cmp(element, "real_element")) THEN
      handle_custom_block = c_err_none
    ENDIF
    IF (str_cmp(element, "logical_element")) THEN
      handle_custom_block = c_err_none
    ENDIF
    RETURN
  ENDIF

  ! The following line must always be present
  handle_custom_block = c_err_unknown_block

END FUNCTION handle_custom_block
\end{boxverbatim}

This version of the code will allow you to add a new block called ``custom''
with elements\linebreak
``int\_element'', ``real\_element'' and ``logical\_element'' and the
code will parse them successfully, while any other block or any other element
in the block ``custom'' will throw errors. However, at this stage the code
doesn't actually read any of the values from the deck. To make it useful, any
variable which is read from the input deck must be stored in a global
variable. Defining global variables are explained in more detail in the
relevant section of the manual, but in short, any variable defined in the
module \inlinecode{shared\_data} in the file \inlinecode{src/shared\_data.F90}
will be a global variable. After the variables have been setup, there are once
again helper functions to make converting the text from the deck into a normal
Fortran90 variable. These helper functions are:

\begin{itemize}
\item as\_integer - Attempts to convert a string to an integer. Invokes the
  maths parser.
\item as\_real - Attempts to convert a string to a REAL(num). Invokes the maths
  parser.
\item as\_logical - Attempts to convert a string to a logical. Does not invoke
  the maths parser (must be either ``T'' or ``F'').
\end{itemize}

They are used pretty much as expected, except that the return value is passed
to the functions so that they can report errors while trying to parse the
string. An example would then be:
\begin{boxverbatim}
FUNCTION handle_custom_block(blockname, element, value)

  CHARACTER(LEN=entry_length), INTENT(IN) :: blockname, element, value
  INTEGER :: handle_custom_block
  TYPE(primitive_stack) :: output
  INTEGER :: ix, iy
  IF (str_cmp(blockname, "custom")) THEN
    IF (element .EQ. blank .OR. value .EQ. blank) THEN
      ! If element or value are blank then just testing block so return c_err_none
      handle_custom_block = c_err_none
      RETURN
    ENDIF
    handle_custom_block = c_err_unknown_element
    ! Now test for the real elements
    IF (str_cmp(element, "int_element")) THEN
      handle_custom_block = c_err_none
      int_element = as_integer(value, handle_custom_block)
    ENDIF
    IF (str_cmp(element, "real_element")) THEN
      handle_custom_block = c_err_none
      real_element = as_real(value, handle_custom_block)
    ENDIF
    IF (str_cmp(element, "logical_element")) THEN
      handle_custom_block = c_err_none
      logical_element = as_logical(value, handle_custom_block)
      ENDIF
    RETURN
  ENDIF

  ! The following line must always be present
  handle_custom_block = c_err_unknown_block

END FUNCTION handle_custom_block
\end{boxverbatim}

It is possible to perform more advanced types of evaluation of maths
expressions such as reading arrays etc. but this is beyond the scope of this
manual at present.

\subsubsection{check\_custom\_blocks}
This function is called when all the blocks in the input deck have been
evaluated and is used to check that all required parameters have been set. If
all required elements have been set then you should just return
\inlinecode{c\_err\_none}, otherwise return
\inlinecode{c\_err\_missing\_elements}. How you test that required elements have
been set is up to the developer, and for testing and personal use (which is
all that the custom deck parts of the code should be used for) it is
acceptable to just not check and always return \inlinecode{c\_err\_none}. If
permanently expanding the deck, error trapping should always be written.

{
  \fontfamily{phv}\selectfont\input{./devtitle.tex}
}

\section{{\EPOCH} developers manual}
This section of the manual is aimed at people who intend to edit the {\EPOCH}
source code to extend or modify existing features, add new diagnostics or
develop new physics packages. It is expected that anyone reading this part of
the manual will be familiar with the material covered in the main manual,
particularly the notices about the layout of particle, particle\_list and
particle\_family structures in the ``{\EPOCH} for advanced end users'' section
of the main manual. A good working knowledge of linked lists is also critical to
the understanding of the code and again an introduction to linked lists is
given in the ``{\EPOCH} for advanced end users'' section.

A quick note: Some files have the normal Fortran file extension .f90, while
some have the slightly unusual .F90. The difference is that files with the
.F90 extension are passed through the preprocessor before they are compiled
allowing the use of precompiler directives (the \#ifdef commands).

\section{General layout of the {\EPOCH} code}

The names of the source files in {\EPOCH} are fairly self explanatory but, for
clarity, they are explained here.

\subsection{Directories}
All source files are contained in the \inlinecode{src} directory and its
subdirectories. There is a stylistic reason for the layout of the files, which
is explained here

\begin{itemize}
\item \inlinecode{src} - Files in this directory are the core files for the
  basic {\EPOCH} code, such as the field solvers, the particle pusher, the
  boundary conditions and the lasers.
\item \inlinecode{src/deck} - Files in this directory are responsible for
  dealing with the permanent input deck parser and include the core parts of
  the deck handler, and also the routines which deal with the blocks in the
  input deck files.
\item \inlinecode{src/housekeeping} - Files in this directory deal with those
  parts of the code operation which are not physics; including the load
  balancer, the MPI setup routines and the moving window.
\item \inlinecode{src/io} - The files involved in all I/O activities, including
  the distribution functions and the particle probes.
\item \inlinecode{src/parser} - The files for the maths expression parser are
  in this directory, including both the core implementation of the shunting yard
  algorithm and the routines for implementing the permanent functions,
  constants and operators for the input deck.
\item \inlinecode{src/physics\_packages} - Contains routines which implement
  additional physics for the code.
\item \inlinecode{src/user\_interaction} - Contains any Fortran routines which
  a user has to modify to use the code with internal initial conditions, or to
  temporarily extend the maths parser or the input deck.
\end{itemize}

\subsection{The files in \inlinecode{src}}
\begin{itemize}
\item boundary.f90 - Includes all boundary conditions except laser and
  transmissive boundaries; including field and particle MPI boundaries, and
  field and particle domain boundaries.
\item epoch\{n\}d.F90 - Main driver for the code. Reading this routine gives
  the basic layout of the code flow.
\item fields.f90 - The Maxwell field solver.
\item laser.f90 - Includes laser and transmissive boundary conditions for each
  boundary and also the housekeeping routines for the laser objects.
\item particles.F90 - The particle pusher.
\item shared\_data.F90 - This file includes all the global variable and type
  definitions. Usually new variables should be defined in this file.
\end{itemize}

\subsection{The files in \inlinecode{src/deck}}
\begin{itemize}
\item deck.F90 - The main input deck routines. Deals with opening files,
  reading data and MPI distribution of the data to all processes. Also
  includes the routines which deal with calling the right reader routines to
  deal with a given block.
\item deck\_boundaries\_block.f90 - Reader routine for the ``boundaries'' block
  of the input deck.
\item deck\_constant\_block.f90 - Reader routine for ``constants'' blocks in the
  input deck.
\item deck\_control\_block.f90 - Reader routine for ``control'' block in the
  input deck.
\item deck\_deo\_block.f90 - Reader routine for ``deo'' blocks in the input
  deck.
\item deck\_eio\_dist\_fn\_block.f90 - Reader routine for ``dist\_fn'' blocks in
  the input deck.
\item deck\_eio\_particle\_probe\_block.f90 - Reader routine for
  ``particle\_probe'' blocks in the input deck.
\item deck\_ic\_external\_block.f90 - Reader routines for ``species\_external''
  and ``field\_external'' blocks in the initial conditions deck.
\item deck\_ic\_fields\_block.f90 - Reader routine for ``fields'' blocks in the
  initial conditions deck.
\item deck\_ic\_laser\_block.f90 - Reader routine for ``laser'' blocks in the
  initial conditions deck.
\item deck\_ic\_species\_block.f90 - Reader routine for ``species'' blocks in
  the initial conditions deck.
\item deck\_io\_block.F90 - Reader routine for the ``io'' block in the input
  deck.
\item deck\_species\_block.F90 - Reader routine for the ``species'' block in the
  input deck.
\item deck\_window\_block.f90 - Reader routine for the ``window'' block in the
  input deck.
\item strings.F90 - Basic string handling routines such as ``str\_cmp'' and
  routines for converting strings to numbers WITHOUT using the maths parser
  are covered in this routine.
\item strings\_advanced.F90 - The routines which pass maths along to the maths
  parser routines are here.
\end{itemize}

\subsection{The files in \inlinecode{src/housekeeping}}
\begin{itemize}
\item balance.F90 - Contains the routines for the load balancer and related
  routines.
\item current\_smooth.F90 - Contains the current smoothing routines.
\item mpi\_routines.F90 - Contains the routines dealing with the setup of the
  MPI layer and the creation of the communicator. Also allocates all arrays
  for the first time before load balancing.
\item mpi\_subtype\_control.F90 - Contains the routines that setup the mpi
  types required by the I/O subsystem.
\item particle\_pointer\_advance.F90 - Contains subroutines which walk through
  the lists of particles and species for I/O purposes.
\item partlist.F90 - Contains the routines which deal with the particle lists
  which are used for inter-processor communication of particles.
\item setup.F90 - Deals with the setup of the grids and domains and restarting
  from previous output dumps.
\item shape\_functions.F90 - Contain the particle shape functions used for
  calculating the particle weighting.
\item split\_particles.F90 - Is the implementation of a demonstration of
  particle splitting routines.
\item version\_data.F90 - Contains version information for the current {\EPOCH}
  code.
\item welcome.F90 - The routine which prints the banner message and compiler
  options info.
\item window.F90 - The routines which deal with the moving window.
\end{itemize}

\subsection{The files in \inlinecode{src/io}}
\begin{itemize}
\item calc\_df.F90 - Despite the slightly confusing name, this subroutine
  deals with derived functions like number density, charge density and mass
  density.
\item diagnostics.F90 - Contains the routines which actually dump the data,
  decide what to dump and also the routine to calculate the timestep.
\item dist\_fn.F90 - Contains the routines to calculate the distribution
  functions, and also the routines handling the requests for distribution
  functions.
\item input.f90 - Core part of the CFD file format. Contains the general
  routines needed for reading data from CFD files.
\item input\_arb.f90 - Core part of the CFD file format. Contains the routines
  needed to read arbitrary data blocks from CFD files.
\item input\_cartesian.f90 - Core part of the CFD file format. Contains the
  routines needed to read Cartesian meshes and variables from CFD files.
\item input\_functions.f90 - Core part of the CFD file format. Contains the
  routines needed to traverse the block structure of a CFD file without
  reading any block specific metadata.
\item input\_particle.f90 - Core part of the CFD file format. Contains the
  routines needed to read particles meshes and variables from CFD files.
\item iocommon.f90 - Core part of the CFD file format. Contains constants and
  definitions needed by the CFD format. If attempting to implement a CFD
  reader then look here for values of named constants.
\item iocontrol.f90 - Core part of the CFD file format. Contains the routines
  needed to open and close files etc.
\item iterators.f90 - Contains the iterator functions used to write particle
  data into CFD files.
\item job\_info.f90 - Contains routines for generating a unique job-id for
  each simulation run.
\item output.f90 - Core part of the CFD file format. Contains the basic
  routines needed to write data to a CFD file.
\item output\_arb.f90 - Core part of the CFD file format. Contains the
  routines needed to write arbitrary data blocks to a CFD file.
\item output\_cartesian.f90 - Core part of the CFD file format. Contains the
  routines needed to write Cartesian meshes and variables.
\item output\_particles.f90 - Core part of the CFD file format. Contains the
  routines needed to write particles meshes and variables.
\item probes.F90 - Contains the routines which write the data from the
  particle probes. Also includes the routines which deal with user requests to
  add new particle probes.
\item simple\_io.F90 - Contains routines for performing the simple binary I/O
  required by species\_external and fields\_external blocks.
\end{itemize}

\subsection{The files in \inlinecode{src/parser}}
\begin{itemize}
\item evaluate.f90 - Contains the routines which actually evaluate a tokenized
  expression. The core of this is a simple implementation of an RPN
  calculator.
\item evaluator\_blocks.f90 - Contains the routines which evaluate a given
  token into a numerical values. Actually implements the functions, constants
  and operators in {\EPOCH}'s maths parser.
\item shunt.F90 - {\EPOCH}'s implementation of the ``shunting yard'' algorithm
  used to simultaneously tokenize the input and convert it from infix notation
  to RPN.
\item stack.f90 - Deals with routines for pushing onto and popping off
  stacks.
\item tokenizer\_blocks.f90 - Deal with converting strings found in a string
  being tokenized into tokens. Essentially a large collection of ``str\_cmp''
  commands testing a string against a known name.
\end{itemize}

\subsection{The files in \inlinecode{src/physics\_packages}}
\begin{itemize}
\item ionise.F90 - This is a demonstration of how to implement a physics
  package. It implements a field ionisation routine using a 1 level Saha
  equation. It is more intended of a demonstration of how a physics package
  could be implemented that a ``research ready'' ionisation routine.
\end{itemize}

\subsection{The files in \inlinecode{src/user\_interaction}}
\begin{itemize}
\item custom\_deck.f90 - This file is where and end user can temporarily
  extend the input deck. Described in the ``Extending {\EPOCH}'' section.
\item custom\_laser.f90 - The file where an end user specifies laser time
  profiles without using the input deck.
\item custom\_parser.f90 - The file where an end user can temporarily add new
  functions and constants to the input deck. It is described in the ``Extending
  {\EPOCH}'' section of this manual.
\item helper.F90 - This file contains all the internal workings of the
  autoloader. This is in user\_interaction for historical reasons, since early
  versions of the code required the end user to modify some parts of the
  functions contained in this file. As the autoloader has increased in
  complexity, this has ceased to be the case, so it is likely that soon this
  file will be move to ``housekeeping''.
\item ic\_module.f90 - This file is where the internal and manual initial
  conditions are set.
\end{itemize}

\section{{\EPOCH} makefile}
The makefile supplied with {\EPOCH} is a standard GNU make makefile, which must
be user modified to allow a developer to add new files to the code. {\EPOCH}'s
makefile is quite large, so an explanation of how to add new files and new
directories is given below.

\subsection{Adding a new file to be compiled with {\EPOCH}}
There are three things that must be done to cause {\EPOCH} to compile a new
file and link it into the final code. Assume that you're adding a file called
``newfile.F90''. First, find the line which sets the environment variable
\inlinecode{SRCFILES} and add a new parameter which reads\\
\\
newfile.F90\\
\\ This tells the makefile to compile the final code using your new file, the
next thing to do is to add a line which tells the code about the dependencies
for your file. Lower down in the makefile, you'll find a section with lines
which look like:
\begin{boxverbatim}
iocontrol.o: iocontrol.f90 input.o iocommon.o job_info.o output.o
\end{boxverbatim}
Add a new line for describing all the FILES (NOT modules) which are used by
your new file. If you USE shared\_data, mpi\_subtype\_control and stack in
your file then the line would look like:
\begin{boxverbatim}
newfile.o: newfile.F90 shared_data.o mpi_subtype_control.o stack.o
\end{boxverbatim}
Note the structure of the line with ONLY the source file for the new file
specified, all other used files specify the intermediate .o files. The
remaining element of the makefile which needs to be modified is to add your
new file as a dependency to all the files which USE modules contained in your
new file. This is achieved very simply by adding ``newfile.o'' to the dependency
list for those files which USE your modules. For example if you've written new
I/O routines and USE your modules in diagnostics, you'd just change the line
for diagnostics from:
\begin{boxverbatim}
diagnostics.o: diagnostics.F90 calc_df.o deck.o dist_fn.o encoded_source.o \
  iterators.o mpi_subtype_control.o output_particle.o probes.o
\end{boxverbatim}
to
\begin{boxverbatim}
diagnostics.o: diagnostics.F90 calc_df.o deck.o dist_fn.o encoded_source.o \
  iterators.o mpi_subtype_control.o output_particle.o probes.o \
  newfile.o
\end{boxverbatim}

Note that the backslash characters are line continuation marks in makefiles.

\subsection{Adding new directories to {\EPOCH}'s makefile}
If you want to add an entire new directory to the {\EPOCH} compile path then
you need to add it to the definition of the environment variable
\inlinecode{VPATH}. Remember to use the environment variable
\inlinecode{\$(SRCDIR)} rather than hard-coding \inlinecode{src} into the path.

\section{{\EPOCH} core programming}
{\EPOCH} is designed so that it can fairly easily be extended while still being
written in (more or less) standard Fortran90 and MPI1.2. This section details
in increasing complexity what a programmer needs to know to extend {\EPOCH} with
new diagnostics, new physics or even new core solvers. The first few entries
in this section range between style points and explanations of fairly trivial
parts of the {\EPOCH} code, but the end of this section gives an overview of how
one would perform major changes to the complete {\EPOCH} core solver.

\subsection{Physical constants}
In order to ensure that different parts of the code run at the same precision
common physical constants are defined in \inlinecode{shared\_data.F90} and any
new physical constants required by extensions to the code should be placed in
the same location. The constants available in the code are
\begin{itemize}
\item Q0 - Charge on electron.
\item M0 - Rest mass of electron.
\item C - Speed of light in vacuum.
\item kb - Boltzmann's constant.
\item epsilon0 - Permittivity of free space.
\item mu0 - Permeability of free space.
\item h\_planck - Plank's constant.
\item ev - The value of an electron volt.
\end{itemize}

Any new constants required should be specified in the same place in
\inlinecode{shared\_data.F90}.

\subsection{Important variables, arrays and array length}
As well as the physical constants, there are some important variables which
you will have to use to do any development with {\EPOCH}. As a general note,
since {\EPOCH} is written with separate 1D, 2D and 3D versions, definitions will
be given for the 3D version of the code and irrelevant dimensions should just
be left out.

\subsubsection{Shape and size variables}
\begin{itemize}
\item INTEGER :: nx. ny, nz - The number of gridpoints on the current
  processor in each direction. This may change when the load balancer
  activates, so always use these variables rather than local copies.
\item INTEGER :: nx\_global, ny\_global, nz\_global - The number of gridpoints
  in each direction of the whole domain. These numbers will never change and
  will be the numbers read in from the input deck.
\item INTEGER(KIND=8) :: npart\_global - The global number of particles
  specified in the input deck. This is not updated as particles leave the
  domain through boundaries etc. so it is not guaranteed to be accurate.
\item INTEGER :: n\_species - The number of species of particles specified.
\item INTEGER :: nsteps - The maximum number of steps that the core solver
  should take.
\item INTEGER, DIMENSION(1:nproc\_{\it \{x,y,z\}}), ALLOCATABLE :: cell\_{\it
    \{x,y,z\}}\_start, cell\_{\it \{x,y,z\}}\_end.
\item INTEGER :: data\_dir\_max\_length - The maximum number of characters in
  the name of the output directory.
\item INTEGER :: n\_zeros - The number of leading zeros in the output filenames
  from {\EPOCH}.
\end{itemize}
The variables \inlinecode{cell\_{\it \{x,y,z\}}\_start} and
\inlinecode{cell\_{\it \{x,y,z\}}\_end} represent the part of a global array
which is held by the current processor. Since {\EPOCH} is an MPI code, there
doesn't exist a single copy of any of the global arrays anywhere, but if there
did then each processor would be responsible for the slice which runs\\
(cell\_x\_min(rank):cell\_x\_max(rank),
cell\_y\_min(rank):cell\_y\_max(rank),
cell\_z\_min(rank):cell\_z\_max(rank))\\
These variables are used internally in the load balancer, where it is updated,
but is also used when calculating distribution functions. Here it is used to
define the extents of the MPI type which is used to write the distribution
function to disk.

\subsubsection{Input deck variables}
\begin{itemize}
\item CHARACTER(LEN=entry\_length) :: blank - A special string which the input
  deck parser uses to indicate that it's passing a blank string rather than a
  string read from the deck which just happens to be blank.
\item INTEGER :: deck\_state - An integer determining which type of input deck
  is being read by the deck parser.
\item INTEGER, PARAMETER :: num\_vars\_to\_dump - A variable describing the
  number of variables which should be selectable in the input deck as possible
  variables to dump.
\item CHARACTER(LEN=entry\_length) :: extended\_error\_string - String used by
  some error codes in the deck parser to give more user friendly error
  messages.
\item CHARACTER(LEN=entry\_length) :: project\_name - String read from the
  input deck and written into the output files to allow the project to be
  recognised by a reader.
\end{itemize}

\subsubsection{Initial conditions (autoloader) variables}
Initial conditions for the autoloader for a given species are described in
{\EPOCH} by the Fortran TYPE \inlinecode{initial\_conditions\_block}. The
definition (in 3D) is:
\begin{boxverbatim}
TYPE :: initial_condition_block

  REAL(num), DIMENSION(:,:,:), POINTER :: rho
  REAL(num), DIMENSION(:,:,:,:), POINTER :: temp

  REAL(num) :: minrho
  REAL(num) :: maxrho

END TYPE initial_condition_block
\end{boxverbatim}

In 1D, the arrays have one fewer index, and in 3D they have one more.
\begin{itemize}
\item REAL(num) :: rho - Number density for the particles in the species. When
  defined runs (-2:nx+3,-2:ny+3,-2:nz+3).
\item REAL(num) :: temp - Temperature in Kelvin of the species in space. When
  defined runs (-2:nx+3,-2:ny+3,-2:nz+3,1:3). The final index of the array
  is a direction index, used to give anisotropic thermal distributions.
\item minrho - The minimum density below which the autoloader should not load
  particles.
\item maxrho - The maximum density above which the autoloader should clip the
  density function.
\end{itemize}

The initial conditions themselves are in the variable
\begin{boxverbatim}
TYPE(initial_condition_block), DIMENSION(:), ALLOCATABLE :: initial_conditions
\end{boxverbatim}
which is allocated to an array of size \inlinecode{1:n\_species}.

\subsubsection{Particles and particle species}
Particles are represented as linked lists of Fortran TYPES. The definition of
the particle type is:
\begin{boxverbatim}
  ! Object representing a particle
  TYPE :: particle
    REAL(num), DIMENSION(3) :: part_p
    REAL(num), DIMENSION(2) :: part_pos
#ifdef PER_PARTICLE_WEIGHT
    REAL(num) :: weight
#endif
#ifdef PER_PARTICLE_CHARGEMASS
    REAL(num) :: charge
    REAL(num) :: mass
#endif
    TYPE(particle), POINTER :: next, prev
#ifdef PART_DEBUG
    INTEGER :: processor
    INTEGER :: processor_at_t0
#endif
  END TYPE particle
\end{boxverbatim}
And the descriptions are
\begin{itemize}
\item REAL(num) :: part\_p(3) - The particle momentum. Always dimension 3 even
  in 1D and 2D codes.
\item REAL(num) :: part\_pos({\it ndims}) - The particle position. Has
  the same dimensions as that of the code.
\item REAL(num) :: weight - The particle weight if the code is running with
  per particle weighting.
\item REAL(num) :: charge - The particle charge in Coulombs if the code is
  running with per particle charge.
\item REAL(nun) :: mass - The particle mass in kilograms if the code is
  running with per particle mass.
\item TYPE(particle), POINTER :: next, prev - The pointers to the next and
  previous elements of the linked list.
\item INTEGER :: processor - The rank of the processor that the particle
  thinks it is on. Used for debugging.
\item INTEGER :: processor\_at\_t0 - The rank of the processor that the
  particle started on. Used for debugging.
\end{itemize}
Simply adding a new parameter to the definition of the particle type is NOT
sufficient to extend the particle type, since the communications when the
particle crosses a processor boundary do not know about the new parameter and
it will not be transmitted with the particle. How to add new properties to the
particle communication layer is described later.\\

The entire linked list of particles is encapsulated in another Fortran TYPE,
called \inlinecode{particle\_list}, which is defined as:
\begin{boxverbatim}
! Object representing a collection of particles
! Used internally by the MPI particle transfer code
TYPE :: particle_list
  TYPE(particle), POINTER :: head
  TYPE(particle), POINTER :: tail
  INTEGER(KIND=8) :: count
  ! Pointer is safe if the particles in it are all unambiguously linked
  LOGICAL :: safe

  TYPE(particle_list), POINTER :: next, prev
END TYPE particle_list
\end{boxverbatim}
And its properties are:
\begin{itemize}
\item TYPE(particle), POINTER :: head - The first particle in the linked list.
\item TYPE(particle), POINTER :: tail - The last particle in the linked
  list. New particles added to the end of the list are added onto the end of
  the tail element, and the new last particle becomes the new tail element.
\item INTEGER(KIND=8) :: count - The number of particles in this particle
  list. Note that the \inlinecode{particle\_list} type is not directly MPI
  aware, so this is literally the number of particles in {\it this} particle
  list, not the number of particles of this species on all processors.
\item LOGICAL :: safe - A particle list is {\it safe} if the particles in it
  are unambiguously linked. That is that the \inlinecode{count}th particle is
  guaranteed to have its \inlinecode{next} property be null. Most particle
  lists within {\EPOCH} are safe, but sometimes it is useful to be able to have
  particle lists which are subsets of longer particles lists, and these
  particle lists are not {\it safe}.
\item TYPE(particle\_list), POINTER :: next, prev - At present, {\EPOCH} does
  not use these pointers, which are intended to allow multiple particle lists to
  be attached together. Certain parts of {\EPOCH}, such as the I/O system are
  aware of these pointers and will automatically use them if they are ever
  set. They are reserved for future use.
\end{itemize}

The \inlinecode{particle\_list} objects are used to abstract all the functions
of the linked list, including adding and removing particles and transporting
particles between processors.\\
\\
The particle species are represented by yet another Fortran TYPE, this time
called \inlinecode{particle\_family}, which is defined as:
\begin{boxverbatim}
  ! Object representing a particle species
  TYPE :: particle_family
    ! Core properties
    CHARACTER(entry_length) :: name
    TYPE(particle_family), POINTER :: next, prev
    INTEGER :: id
    LOGICAL :: dump

    REAL(num) :: charge
    REAL(num) :: mass
    INTEGER(KIND=8) :: count
    TYPE(particle_list) :: attached_list

#ifdef TRACER_PARTICLES
    LOGICAL :: tracer
#endif

    ! Particle cell division
#ifdef PARTICLE_CELL_DIVISION
    LOGICAL :: split
    INTEGER(KIND=8) :: npart_max
#endif
    ! Secondary list
#ifdef USE_SECONDARY_LIST
    TYPE(particle_list), DIMENSION(:,:,:), POINTER :: secondary_list
#endif

    ! Injection of particles
    INTEGER(KIND=8) :: npart_per_cell
    REAL(num), DIMENSION(:,:), POINTER :: density
    REAL(num), DIMENSION(:,:,:), POINTER :: temperature

    ! species_ionisation
#ifdef PART_IONISE
    LOGICAL :: ionise
    INTEGER :: ionise_to_species
    INTEGER :: release_species
    REAL(num) :: ionisation_energy
#endif
    ! Attached Probes for this species
#ifdef PARTICLE_PROBES
    TYPE(particle_probe), POINTER :: attached_probes
#endif
  END TYPE particle_family
\end{boxverbatim}
Again, most of these properties are self explanatory, but they are detailed
below.
\begin{itemize}
\item #CHARACTER(LEN=entry_length) :: name# - The name of the particle
  species. Used when constructing things like ``ekbar\_electron'' and similar
  names.
\item #TYPE(particle_family), POINTER :: next, prev# - Particle species are
  connected to each other as a linked list using pointers as well as being
  available through a simple array. These pointers are used behind the scenes
  in the I/O.
\item #INTEGER :: id# - The number of the species, so for the species
  \inlinecode{particle\_species(1)}, the id field would be 1. For
  \inlinecode{particle\_species(2)}, the id field would be 2 etc.
\item #LOGICAL :: dump# - Whether or not this species should be dumped in
  diagnostic output.
\item #REAL(num) :: charge# - The charge on a single particle of the species in
  Coulombs.
\item #REAL(num) :: mass# - The mass of a single particle of the species in
  kilograms.
\item #INTEGER(KIND=8) :: count# - The number of particles of this species on
  all processors. NOTE that this is only accurate if the code is compiled with
  the correct preprocessor options. Without the correct preprocessor options,
  this will be accurate at the start of the code runtime, but will not be if
  any particles enter or leave the domain. This is mainly a debugging
  parameter.
\item #TYPE(particle_list) :: attached_list# - This is the
  \inlinecode{particle\_list} object which holds the particles assigned to this
  species on this processor. Particles are attached to this list at all
  \linebreak times EXCEPT when they are explicitly split when the code is
  compiled with the\linebreak
  \inlinecode{PARTICLE\_CELL\_DIVISION} option. When the code is compiled with
  \inlinecode{PARTICLE\_CELL\_DIVISION}, the particles are attached to
  \inlinecode{attached\_list} except between the calls to
  \inlinecode{reorder\_particles\_to\_grid} and
  \inlinecode{reattach\_particles\_to\_mainlist} in
  \inlinecode{epoch{\it n}d.F90} where the particles are instead attached to
  \inlinecode{secondary\_list}. This is explained later.
\item #LOGICAL :: tracer# - Whether or not this species is a tracer particle. If
  a species is a tracer species then it moves under the fields as normal for a
  particle with its mass and charge but contributes no current.
\item #LOGICAL :: split# - {\EPOCH} includes a very early version of a particle
  splitting operator. It works mechanically but has undesirable properties at
  present. If this flag is true then the code attempts to split the particles
  when the pseudoparticle number density drops too low.
\item #INTEGER(KIND=8) :: npart_max# - Used with the particle splitting
  operator. When the total number of particles equals this number, further
  particle splitting is suppressed.
\item #TYPE(particle_list), DIMENSION(:,:,:), POINTER :: secondary_list# - When
  the code is compiled with the \inlinecode{-DPARTICLE\_CELL\_DIVISION} option,
  the code allocates \inlinecode{secondary\_list(0:nx+1,0:ny+1,0:nz+1)} and then
  loops over all particles. It calculates the cell in which each particle is and
  moves the particle from \inlinecode{attached\_list} to the correct element of
  \inlinecode{secondary\_list} for that cell. This means the particles which are
  nearby in space are now linked together in an array of linked lists. This
  allows things such as collision operators which require direct interaction
  between nearby particles.
\item #INTEGER(KIND=8) :: npart_per_cell# - The number of pseudoparticles per
  cell in the initial conditions. This is used with the moving window function
  to ensure that the same number of particles per cell are used for the new
  material introduced at the leading edge of the window.
\item #REAL(num), DIMENSION(:,:), POINTER :: density# - The density of the
  plasma at the leading edge of the window at the start of the simulation. This
  is used to structure the density of the new material introduced at the leading
  edge of the plasma.
\item #REAL(num), DIMENSION(:,:,:), POINTER :: temperature# - The temperature
  in of the plasma at the leading edge of a moving window at the start of the
  simulation. The final index of the array is the direction in which the
  temperature is set (1=x, 2=y, 3=z).
\item #LOGICAL :: ionise# - If the ionisation model is activated then this
  species should ionise.
\item #INTEGER :: ionise_to_species# - The species number for the next ionised
  state of this species.
\item #INTEGER :: release_species# - Specifies what type of particle should be
  released when this species ionises (i.e. which species is the electron).
\item #REAL(num) :: ionisation_energy# - The ionisation energy for the next
  ionisation of this species.
\item #TYPE(particle_probe), POINTER :: attached_probes# - A pointer pointing to
  the head of an attached linked list of particle probe diagnostics.
\end{itemize}

\subsubsection{EM Fields}
There are nine variables which are used in updating the EM field solver. These
are
\begin{itemize}
\item ex - Electric field in the X direction.
\item ey - Electric field in the Y direction.
\item ez - Electric field in the Z direction.
\item bx - Magnetic field in the X direction.
\item by - Magnetic field in the Y direction.
\item bz - Magnetic field in the Z direction.
\item jx - Current in the X direction.
\item jy - Current in the Y direction.
\item jz - Current in the Z direction.
\end{itemize}
The EM fields in {\EPOCH} are simple allocatable arrays, which are of size
(-2:nx+3,-2:ny+3,-2:nz+3), although this includes the ghost cells. The length of
the core domain is different for each variable due to the grid stagger.

The {\EPOCH} field solver is a Yee staggered 2nd order FDTD scheme, directly
based on the scheme in the PSC by Hartmut Ruhl and is contained in the file
\inlinecode{fields.F90}. To locate a variable on the grid there is a simple
rule.
\begin{itemize}
\item Start at the cell centre.
\item For an E field component, move the field half a grid point in the
  direction that the field points if possible.
\item For a B field component, move the field half a grid point in all
  directions {\it except} the one it points.
\end{itemize}
This is illustrated in Figure~\ref{yeegrid} for the 2D case.\\

\captionedimage{./images/stagger}{yeegrid}{The Yee grid in 2D}


The grid stagger means that you have to be careful with boundary conditions
since some variables are defined on the domain boundaries whereas others are
defined on either side of a domain boundary. This is handled automatically by
the built in boundary routines, but must be understood if developing other
boundary conditions. To explain it, consider only the left/right boundary in 1D
and consider $E_x$ and $B_x$.\\

$E_x$ is defined on the cell boundary, so \inlinecode{ex(0)} is the value of
$E_x$ on the left boundary and similarly \inlinecode{ex(nx)} is the
value on the right boundary. Conversely, in the 1D code $B_x$ is cell centred
(in reality, $B_x$ is never used in the field update and is unimportant since
any gradients in $B_x$ in 1D automatically break the solenoidal condition, but
this is still a useful example.). This means that \inlinecode{bx(1)} is the
centre of the first cell in the domain, and \inlinecode{bx(0)} is the value at
the centre of the first left hand ghost cell. This means the you must do
different things as boundary conditions for the two fields for some boundary
conditions.\\

For example, if you want to clamp the value of $E_x$ to be zero on the
boundary, then just set \inlinecode{ex(0) = 0.0\_num} since \inlinecode{ex(0)}
lies on the boundary. To do the same for $B_x$ on the boundary you have to
set \inlinecode{bx(0) = -bx(1)}. This is because if you use a linear
reconstruction of $B_x$ (i.e second order) then the point between
\inlinecode{bx(0)} and \inlinecode{bx(1)} has the value
$B_x(1/2) = \left(B_x(1)+B_x(0)\right)/2$. Similarly, if you want to set zero
gradient on the boundary then for $E_x$ you set \inlinecode{ex(-1) = ex(1)},
whereas for $B_x$ you would set \inlinecode{bx(0) = bx(1)}. This is explained
in more detail in the section which describes the existing {\EPOCH} boundary
conditions.

In the particle pusher, time centred field variables are needed for second
order accuracy, so an FDTD scheme is used to advance the fields. This looks
like

\begin{itemize}
\item $\vec{E}^{n+\frac{1}{2}} = \vec{E}^n + \frac{\Delta t}{2} \left( c^2
  \nabla \wedge \vec{B}^{n} -\vec{j}^{n} \right)$
\item $\vec{B}^{n+\frac{1}{2}} = \vec{B}^n - \frac{\Delta t}{2} \left( \nabla
  \wedge \vec{E}^{n+\frac{1}{2}} \right)$
\item Call particle pusher which calculates $j^{n+1}$ currents
\item $\vec{B}^{n+1} = \vec{B}^{n+\frac{1}{2}} - \frac{\Delta t}{2} \left(
  \nabla \wedge \vec{E}^{n+\frac{1}{2}} \right)$
\item $\vec{E}^{n+1} = \vec{E}^{n+\frac{1}{2}} + \frac{\Delta t}{2} \left( c^2
  \nabla \wedge \vec{B} ^{n+1} - \vec{j}^{n+1} \right)$
\end{itemize}
Note that all spatial derivatives are calculated using the staggered grid, so
the final derivatives in the code appear one sided. However, this is not the
case, and all spatial derivatives are second order accurate. Higher order
spatial derivatives schemes for {\EPOCH} are being developed to improve the
dispersion properties of the code when resolving small timescales.

\subsection{The particle pusher}
{\EPOCH}'s particle pusher is based on the one from the PSC by Hartmut Ruhl, and
is a Birdsall and Landon type PIC scheme using Villasenor and Buneman current
weighting. It is contained in the file \inlinecode{particles.F90}. The
operation of the particle pusher is fairly simple, but there are a few elements
which need some clarification.
\begin{itemize}
\item The update to the particle momenta etc. does not explicitly include the
  particle weight function. This means that the pseudoparticle momenta etc. are
  the momentum for a single real particle of the collection of real particles
  represented by that pseudoparticle, NOT the momentum of the whole collection
  of real particles.
\item \inlinecode{root} - The variable root which appears in various places is
  essentially the multiplicative factor which is needed to convert the particle
  momentum into the particle velocity. If {\EPOCH} was not relativistic then
  this would simply be $1/part\_m$ where $part\_m$ is the particle mass.
  Since {\EPOCH} is relativistic, root is defined as $\left(part\_m^2 +
    \vec{p}.\vec{p}/c^2\right)^{-1/2}$.
\item \inlinecode{cell\_x1=cell\_x1+1} - There are lines like this after all
  the sections of the routine where the cell a particle is in is
  calculated. This is because, for a cell centred variable, the domain runs
  (1:nx,1:ny,1:nz) rather than (0:nx-1,0:ny-1,0:nz-1).
\end{itemize}

\subsubsection{Particle shape functions}
The key feature of a PIC code controlling the smoothness of the solution is the
particle shape function. That is the function that describes the assumed
distribution of the real particles making up a pseudoparticle. The simplest
solution is to assume that the pseudoparticles uniformly fill the cell in which
the pseudoparticle is located. This has the advantages of speed and simplicity
but produces very noisy solutions. The next simplest approach is to assume a
triangular shape function with the peak of the triangle located at the position
of the pseudoparticle and a width of $ 2 \Delta x$, as illustrated in
Figure~\ref{shape}. This is the approach used
in {\EPOCH} and is a good trade-off between cleanness of solution and
speed. Higher order methods based on spline interpolation can be used and do
produce smoother solutions, but they are significantly slower and the benefits
of the schemes can easily be overstated. {\EPOCH} does now include an option to
use 4th order spline interpolation in all parts of the code. This option is
enabled with the \inlinecode{-DSPLINE\_FOUR} compile time option in the
makefile.\\
\[
S(w) =
\begin{cases}
1 - \frac{x_i-w}{\Delta x}, & (x_i-w) \le \Delta x \\
0, & \mbox{otherwise}
\end{cases}
\]

Functions derived from the particle shape function appear in two places in the
core solver: when the EM fields are interpolated to the position of the
pseudoparticle and when the current is updated and properties of the
pseudoparticle are copied onto the grid. These two uses of the shape function
are conceptually similar, but have different forms.

\captionedimage{./images/shape}{shape}{Second order particle shape function}

To derive the equations for calculating the field acting on a particle,
you calculate the overlap of the particle shape function with the function
representing the fields on the grid. In {\EPOCH}, the fields are approximated at
first order so that the field is constant over each cell. Consider a particle
with position $X$, where $X$ lies in the cell centred at $x_i$ and grid
spacing $\Delta x$. The integral is split into four parts; that part of the
shape function which overlaps with the cell $x_{i-1}$, the part of the shape
function from the left boundary of $x_i$ to the point of the triangle, the part
of the shape function from the point of the triangle to the right hand edge of
$x_i$ and finally that part of the shape function which overlaps cell
$x_{i+1}$. Assuming that fields are constant inside each cell, this takes the
form
\[
\begin{aligned}
  Fpart_i~=~\frac{1}{\Delta x} & \left[ \int^{x_{i-1}
      + \frac{\Delta x}{2}}_{X-\Delta x}
      F_{i-1} \left( 1-\frac{X-x}{\Delta x} \right) dx \right.\\
    & + \int^{X}_{x_i-\frac{\Delta x}{2}}
      F_i \left( 1-\frac{X-x}{\Delta x} \right) dx \\
    & + \int^{x_i+\frac{\Delta x}{2}}_{X}
      F_i \left(  1-\frac{x-X}{\Delta x}\right) dx \\
    & \left. + \int^{X+\Delta x}_{x_{i+1} - \frac{\Delta x}{2}}
      F_{i+1} \left( 1-\frac{x-X}{\Delta x}\right) dx\right]
\end{aligned}
\]

Performing these integrals and remembering that $x_{i-1}+\frac{\Delta x}{2}$ is
equal to $x_i-\frac{\Delta x}{2}$ since the grid is uniformly spaced with
spacing $\Delta x$ this gives a final formula for the field at a particle of

\[
\begin{aligned}
  F_{part}~=~& \frac{1}{2} F_{i-1} \left( \frac{1}{2}
  + \frac{x_i-X}{\Delta x} \right)^2 \\
  & + F_i \left( \frac{3}{4} - \frac{(x_i-X)^2}{\Delta x^2} \right)\\
  & +\frac{1}{2} F_{i+1} \left( \frac{1}{2} - \frac{x_i-X}{\Delta x} \right)^2
\end{aligned}
\]

%If you are running the code with the \inlinecode{-DSPLINE\_FOUR} high order
%particle weight function option then the triangular shape function is replaced
%by a 4th order spline function which has a basis of 5 cells rather than 3
%cells. The form of this function is

In the code calculating the strength of a cell centred field on the particle
is done as follows.
\begin{boxverbatim}
  REAL(num) :: cell_x_r, cell_frac_x
  INTEGER :: cell_x
  REAL(num) :: gx(-2:2)
  TYPE(particle), POINTER :: current

  ! Work out number of grid cells in the particle is
  ! Not in general an integer
  cell_x_r = (current%particle_pos(1)-x_min_local)/dx
  ! Round cell position to nearest cell
  cell_x = NINT(cell_x_r)
  ! Calculate fraction of cell between nearest cell boundary and particle
  cell_frac_x = REAL(cell_x, num) - cell_x_r
  cell_x = cell_x+1

  CALL grid_to_particle(cell_frac_x, gx)

  f_part = 0.0_num
  DO ix = -sf_order, sf_order
    f_part = f_part + F(cell_x+ix) * gx(ix)
  ENDDO
\end{boxverbatim}

where \inlinecode{f\_part} is the field at the particle location. The variable
\inlinecode{sf\_order} contains the shape function order parameter which
indicates the number of cells each side of the cell containing the particle
which are overlapped by the particle shape function. It is defined in
\inlinecode{shared\_data.F90} and should only be changed by the developer if a
new particle shape function is being added.  In 2D or 3D, you just calculate gy
in the same manner as gx and calculate the weight over all the cells affected
by the individual 1D shape functions. In 2D this looks like:
\begin{boxverbatim}
  CALL grid_to_particle(cell_frac_x, gx)
  CALL grid_to_particle(cell_frac_y, gy)
  f_part = 0.0_num
  DO iy = -sf_order, sf_order
    DO ix = -sf_order, sf_order
      f_part = f_part + f(cell_x+ix, cell_y+iy) * gx(ix) *gy(iy)
    ENDDO
  ENDDO
\end{boxverbatim}

Inside the particle pusher the E and B fields are not cell centred fields, but
Yee staggered. This means that there is a small change to the above mentioned
example. In 1D this change looks like

\begin{boxverbatim}
  REAL(num) :: cell_x_r, cell_frac_x
  INTEGER :: cell_x1, cell_x2
  REAL(num) :: gx(-2:2), hx(-2:2)
  TYPE(particle), POINTER :: current

  ! Work out number of grid cells in the particle is
  ! Not in general an integer
  cell_x_r = (current%particle_pos(1)-x_min_local)/dx
  ! Round cell position to nearest cell
  cell_x = NINT(cell_x_r)
  ! Calculate fraction of cell between nearest cell boundary and particle
  cell_frac_x = REAL(cell_x, num) - cell_x_r
  cell_x = cell_x+1

  CALL grid_to_particle(cell_frac_x, gx)

  cell_x_r = (current%particle_pos(1)-x_min_local)/dx - 0.5_num
  cell_x2  = NINT(cell_x_r)
  cell_frac_x = REAL(cell_x2, num) - cell_x_r
  cell_x2 = cell_x2+1

  CALL grid_to_particle(cell_frac_x, hx)

  ! bx is cell centred
  bx_part = 0.0_num
  DO ix = -sf_order, sf_order
    bx_part = bx_part + bx(cell_x1+ix) * gx(ix)
  ENDDO

  ! ex is staggered 1/2 a cell to the right
  ex_part = 0.0_num
  DO ix = -sf_order, sf_order
    ex_part = ex_part + ex(cell_x2+ix) * hx(ix)
  ENDDO
\end{boxverbatim}

In 2D and 3D, you just combine the shifted and unshifted shape functions and
associated cell positions depending on the position of the variable in the
cell. Therefore, in 3D and using the loop notation for clarity you would get:
\begin{boxverbatim}
  DO iz = -sf_order, sf_order
    DO iy = -sf_order, sf_order
      DO ix = -sf_order, sf_order
        ex_part = ex_part + hx(ix)*gy(iy)*gz(iz) * &
            ex(cell_x2+ix, cell_y1+iy, cell_z1+iz)
      ENDDO
    ENDDO
  ENDDO

  DO iz = -sf_order, sf_order
    DO iy = -sf_order, sf_order
      DO ix = -sf_order, sf_order
        bx_part = bx_part + gx(ix)*hy(iy)*hz(iz) * &
            bx(cell_x2+ix,cell_y1+iy,cell_z1+iz)
      ENDDO
    ENDDO
  ENDDO
\end{boxverbatim}

Since $E_x$ is staggered half a grid cell in the x direction, whereas $B_x$ is
staggered by half a grid cell in the y and z directions.

The next stage is to consider how to copy pseudoparticle
properties on the grid. This is very similar to the function for calculating
grid variables at the particle location and, for each grid point $x_i$,
consists of integrating the part of the particle shape function which overlaps
the $i^{th}$ cell. That is

\[
F(i) = Data  \int^{x_i+\frac{\Delta x}{2}}_{x_i-\frac{\Delta x}{2}} S(X-x) dx
\]

Where $Data$ is the particle property to be copied onto the grid. In {\EPOCH},
since the particle shape function is known to go to zero outside a distance of
$2 \Delta x$ from the maximum, the maximum number of cells that can possibly be
overlapped by a given particle shape function is 3; the cell containing the
particle maximum and the two cells to either side. Performing the integration
using the triangular shape function given above gives the result

\[
  F(i) =
\begin{cases}
  \frac{3}{4} - \frac{|X-x_i|^2}{\Delta x^2}, & |X-x_i|
    \le \frac{\Delta x}{2}\\
  \frac{1}{2} \left(\frac{3}{2}
    - \frac{|X - x_i|}{\Delta x} \right)^2, & \frac{\Delta x}{2} < |X-x_i|
    \le \frac{3 \Delta x}{2}\\
  0, & |X-x_i| > \frac{3 \Delta x}{2}\\
\end{cases}
\]

%Again, when using high order particle shape functions using the
%\inlinecode{-DSPLINE\_FOUR} option this is replaced with an equivalent form for
%a 4th order spline.

When this is translated into the code, it looks very similar to that presented
for the case where grid properties are interpolated to the particle
position. This form is used in the particle pusher to perform the current
update and in the routines in \inlinecode{src/io/calc\_df.F90} to copy particle
properties onto the grid for output. The form from calc\_df is rather clearer
and easier to see in operation. In 1D it looks like:
\begin{boxverbatim}
  cell_x_r = part_x / dx
  cell_x  = NINT(cell_x_r)
  cell_frac_x = REAL(cell_x, num) - cell_x_r
  cell_x = cell_x+1

  CALL particle_to_grid(cell_frac_x, gx)

  DO ix = -sf_order, sf_order
    data_array(cell_x+ix) = data_array(cell_x+ix) + gx(ix)  * data
  ENDDO
\end{boxverbatim}

Once again multi-dimensional codes just have the weighting functions multiplied
together.
\begin{boxverbatim}
  DO iy = -sf_order, sf_order
    DO ix = -sf_order, sf_order
      data_array(cell_x+ix,cell_y+iy) = data_array(cell_x+ix,cell_y+iy) + &
          gx(ix) *gy(iy) * data
    ENDDO
  ENDDO
\end{boxverbatim}

\subsubsection{Current calculation}
{\EPOCH} uses the Villasenor and Buneman (Villasenor and Buneman, Computer
Physics Communications 69(1992) 306-316) current calculating scheme which
solves the additional equation
${\partial \rho}/{\partial t} = \nabla\cdot\vec{J}$ to
calculate the current at each timestep. The main advantage of this scheme is
that it conserves charge {\it on the grid} rather than just globally conserving
charge on the particles. This means that the error in the solution of Poisson's
equation is conserved, so if Poisson's equation is satisfied for $t=0$ it
remains satisfied for all time.\\

The Villasenor and Buneman scheme works because exactly the same charge added
to one cell is subtracted from another cell, which in turn means that exactly
the same current added to one cell is subtracted from another cell. This is
intuitively correct since a point particle crossing a cell boundary would
represent the loss of that particle's contribution to the current from the
source cell and the gain of that particle's contribution to the current by the
destination cell. In fact this simple type of cell boundary crossing
current calculation was used in classical Buneman type PIC codes.\\

The scheme is messy, in practise, but simple. After the main particle push, the
particle is advanced a further half timestep into the future to first order
using the velocities calculated at the end of the particle push. The particle
position at $t + dt/2$ were stored earlier, and combined with the newly
calculated particle position at $t + {3dt}/{2}$ this allows a time centred
evaluation of ${\partial \rho}/{\partial t}$ meaning that the current
update is second order accurate in time. The spatial order of the scheme
matches the spatial order of the particle weight function.\\

The weight functions for transferring particle properties onto the grid at the
two timesteps are calculated including a shift when necessary to allow for the
particle having crossed a cell boundary. Since the charge associated with the
particle is spatially distributed using the weight function, all that is
necessary to calculate ${\partial \rho}/{\partial t}$ is to subtract the
two functions, multiply by the charge on the pseudoparticle and the
pseudoparticle weight and finally divide by $dt$. The spatial derivative of
$\vec{J}$ is then converted to a one sided finite difference form and solved
directly. In multiple dimensions this is slightly complicated by the effects of
offsets in directions other than the direction that a given current component
is pointing in, with this adding additional weight factors based on the overlap
of the shape functions in other directions. This is explained in full in the
Villasenor and Buneman paper already quoted.\\

Currents in ignorable directions are simply calculated using $J = n\rho\vec{v}$
with the correct shape functions to ensure that the current is placed in the
correct places.

\subsection{The timestep}

The timestep is calculated in the subroutine \inlinecode{set\_dt} in the file
\inlinecode{src/io/diagnostics.F90}. All that the subroutine has to do is set
the variable \inlinecode{dt} to set the timestep for the whole code. Any
additional timestep constraints should be coded into this subroutine. This
should be implemented after the existing \inlinecode{dt=} lines but before the
line \inlinecode{dt = dt\_multiplier * dt}. Such a modification should be set so
that it only changes the timestep if the timestep is MORE restrictive than that
calculated from the core code. An example would be:
\begin{boxverbatim}
   dt = dtx*dty/SQRT(dtx**2+dty**2)
   dt = MIN(dt, my_new_dt)
\end{boxverbatim}

In the core {\EPOCH} code the timestep can be calculated identically on each
processor, so there is no requirement to synchronise the timestep across
multiple processors. If your new timestep restriction uses information local to
each processor then some additional lines must be added to the
\inlinecode{set\_dt} routine after the timestep has been calculated which
should read:
\begin{boxverbatim}
  REAL(num) :: dt_global
          .
          .
          .
  CALL MPI_ALLREDUCE(dt_global, dt, 1, mpireal, MPI_MIN, comm, errcode)
  dt = dt_global
\end{boxverbatim}

This uses another MPI command to determine the most restrictive timestep across
all processors. {\EPOCH} is not written in a way that permits operation with
different timesteps on different processors, and the behaviour of the code is
undefined (and likely wrong) if the code runs with different timesteps on
different processors.

\subsection{Boundary conditions}
Boundary conditions in {\EPOCH} are split into three types
\begin{itemize}
\item Simple field boundaries.
\item Laser and outflow boundaries.
\item Particle boundaries.
\end{itemize}

These boundaries can be combined in different ways to give different
effects. From the end user perspective there are 4 boundaries which can be
applied to each edge of the simulation domain. These are
\begin{itemize}
\item Periodic
  \subitem Particles periodic
  \subitem Fields periodic
  \subitem Lasers off
\item Other
  \subitem Particles reflect
  \subitem Fields clamped zero
  \subitem Lasers off
\item Simple Laser
  \subitem Particles transmissive
  \subitem Fields clamped zero
  \subitem Lasers applied at half timestep for B field
\item Simple outflow
  \subitem Particles transmissive
  \subitem Fields clamped zero
  \subitem No lasers applied at half timestep, but outflow conditions applied
  to B field at half timestep
\end{itemize}
The boundary conditions are applied in too many places in the code to give a
full description of them, but the laser boundaries are only applied in
\inlinecode{src/fields.f90}. The boundaries requested by the user are converted
into the conditions on the fields and particles in the routine
\inlinecode{setup\_particle\_boundaries} in
\inlinecode{src/boundaries.F90}. For each of the six possible boundaries
(x\_min, x\_max, y\_min, y\_max, z\_min, z\_max) there is a variable which will
be named something like \inlinecode{bc\_x\_min\_particle} or
\inlinecode{bc\_y\_max\_field} which controls the boundary condition which will
be applied to either the field or the particles.

\subsubsection{Simple field boundaries}
There are two subroutines which apply the standard boundary conditions:
\inlinecode{field\_zero\_gradient} and \inlinecode{field\_clamp\_zero}. The
type of boundary condition that the two apply is obvious from the name, but
the two functions have different calling conventions.

\pagebreak
\begin{codedef}
SUBROUTINE field_zero_gradient
\HRule
REAL(num), DIMENSION(-2:,-2:,-2:), INTENT(INOUT) :: field
LOGICAL, INTENT(IN) :: force
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/boundary.F90}\\
\\[0.5cm]
{\Large Description\\}
#field_zero_gradient# is the routine which applies zero gradient
boundary conditions to a field variable passed in the parameter
\inlinecode{field}. It can be used as a global field boundary condition by
setting one of the field boundary conditions to c\_bc\_zero\_gradient in
\inlinecode{setup\_particle\_boundaries}, but it is mostly used to give
boundary conditions for the autoloader, where the \inlinecode{force} parameter
is set to \inlinecode{.TRUE.} to override the boundary condition values.
\\[0.5cm]
{\Large Notes\\ \\}

\pagebreak
\begin{codedef}
SUBROUTINE field_clamp_zero
\HRule
REAL(num), DIMENSION(-2:,-2:,-2:), INTENT(INOUT) :: field
INTEGER, DIMENSION(ndims), INTENT(IN) :: stagger
\end{codedef}
\vspace{1cm}
{\Large Description\\}
\inlinecode{field\_clamp\_zero} is the routine which clamps the field given by
the \inlinecode{field} variable to zero on the boundary. Since this routine
explicitly sets the field to zero {\it on} the boundary it needs to know about
the grid stagger which is provided in the parameter \inlinecode{stagger}. At
present stagger is simply an array of 1/0 values which indicates whether a
given field is staggered in that direction, so a cell centred field has
stagger \inlinecode{(/0, 0, 0/)} while ex has stagger \inlinecode{(/1, 0, 0/)}
and bx has stagger \inlinecode{(/0, 1, 1/)}.
\\[0.5cm]
{\Large Notes\\}
\pagebreak

Additional boundary conditions should follow the same basic principle as these
routines. Note that all of the routines test for things like \inlinecode{x\_min
.EQ. MPI\_PROC\_NULL} etc. these tests confirm that a given processor is at the
edge of the real domain, and so should have a real boundary condition applied
to it. This also explains why there are no explicit periodic boundary condition
routines, since by connecting the processors cyclically in a periodic direction
the domain boundary effectively becomes another internal processor boundary.

\subsubsection{Laser and outflow boundaries}
Setting up lasers is explained in the relevant section in the first part of the
manual and the way in which the laser specification in the input deck works is
described later, so this section just describes how the actual laser and
outflow routines in \inlinecode{laser.F90} work.\\

The laser boundaries in {\EPOCH} are based on a rewriting of Maxwell's
equations in a new form which expresses the fields explicitly in terms of waves
propagating in both directions along each co-ordinate axis with both S and P
polarisation states. In the X direction,
\[
\partial_t(E_y \pm B_z) \pm \partial_x(E_y \pm B_z) = \pm \partial_yE_x
+ \partial_zB_x -\frac{j_y}{\epsilon_0}
\]
\[
\partial_t(E_z \mp B_y) \pm \partial_x(E_z \mp B_y) = \pm \partial_yE_x
- \partial_zB_x -\frac{j_y}{\epsilon_0}
\]
It is then possible to rewrite these equations to provide a boundary condition
on $B_z$ and $B_y$ to give propagating EM waves at the boundary. For waves
travelling into the boundary, this gives a transmissive boundary, and if the
components for waves propagating out from the boundary are set to be non-zero
then it also introduces an EM wave propagating from the left boundary.\\

This boundary condition is found in the file \inlinecode{laser.F90} which also
includes the routines for handling the \inlinecode{laser\_block} objects which
represent how lasers are represented in {\EPOCH}.

\subsubsection{Particle boundaries}
Due to the time that is required to loop over all the particles the particle
boundary conditions in {\EPOCH} combine the inter-processor boundary conditions
with the real boundary conditions. The boundary conditions for particles are in
the routine \inlinecode{particle\_bcs} in the file \inlinecode{boundary.f90} \\
Currently {\EPOCH} includes only three particle boundary conditions
\begin{itemize}
\item c\_bc\_periodic - Particles which leave one side of the box reappear on
  the other side.
\item c\_bc\_reflect - Particles reflect off the boundary as if it was a hard
  boundary.
\item c\_bc\_open - Particles pass through the boundary and are destroyed. Total
  pseudoparticle number is not conserved in this mode.
\end{itemize}

Although the routine looks rather messy, it is fairly easy to understand. The
sequence goes:
\begin{itemize}
\item Loop over all species.
\item Create particle list objects for particles to be sent to and received from
  other processors.
\item Loop over all particles.
\item If the particle has crossed the domain boundary and that boundary has
  reflecting boundary conditions then reflect the particle.
\item If the particle has crossed the processor boundary then set the variables
  \inlinecode{xbd} and \inlinecode{ybd} which are used to identify which
  processor relative to the current processor the particle should be moved to.
\item Check whether the particle has crossed an open domain boundary.
\item If the particle has crossed a processor boundary or an open domain
  boundary then remove the particle from the particle list for its species.
\item If the particle has crossed a processor boundary but not an open domain
  boundary then add it to the list to be sent to its new processor using
  \inlinecode{xbd} and \inlinecode{ydb} to identify which processor it should
  be on.
\item If the particle has crossed an open domain boundary then either add it to
  another list to be dumped to disk if the user has requested this, or
  otherwise just deallocate the particle to reclaim memory.
\item End particle loop.
\item Loop over all possible neighbouring processors for the current processor
  and exchange particle lists with that processor.
\item Add any received particles onto the particle list for the current
  species.
\item Although particles which have crossed a periodic boundary are now on the
  correct processor, they have the wrong position in space, so subtract the
  length of the domain from the particles position.
\item End species loop.
\end{itemize}

Note that, unlike for fields, there is explicit periodic boundary code. This is
because although the MPI routines place the particle on the correct processor
after the MPI routines, the particle's position variable still places it beyond
the other end of the domain. The MPI parallelism for exchanging particles is
hidden in the routines which deal with the particle list objects and are
described in the next section.
\pagebreak

\subsubsection{MPI Boundaries}
There are three routines which deal with MPI exchange for field variables in
{\EPOCH}. Two are closely related and will be considered together. The third
deals with using MPI to sum variables at processor boundaries rather than
synchronise ghost cells.\\\\

\begin{codedef}
SUBROUTINE field_bc
\HRule
REAL(num), DIMENSION(-2:,-2:,-2:), INTENT(INOUT) :: field
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/boundary.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{field\_bc} exchanges the information in the ghost cells between
adjacent processors. Any field variable which is used in a calculation that
requires operations involving information from points other than the
current point should call this routine each time the variable is updated. This
will ensure that the ghost cells are populated from adjacent processors.
(i.e. if you only need to access field(ix,iy,iz) there is no need to update
ghost cells, whilst if you use field(ix-1,iy,iz) you do).
\\[0.5cm]
{\Large Notes\\}
The \inlinecode{field\_bc} routine just calls the
\inlinecode{do\_field\_mpi\_with\_lengths} routine which is a more general
routine that allows ghost cell information to be exchanged for fields with
an arbitrary number of cells, rather than fields which are
(-2:nx+3,-2:ny+2,-2:nz+3). This routine is used internally in the load
balancing routine when fields with both the old and new sizes must be handled
at the same time.

\pagebreak
\begin{codedef}
SUBROUTINE processor_summation_bcs
\HRule
REAL(num), DIMENSION(-2:,-2:,-2:), INTENT(INOUT) :: field
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/boundary.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{processor\_summation\_bcs} is a routine which is used to deal with
variables, like $\vec{j}$ or number density that should be added at boundaries
to include contributions from particles on both sides of a processor boundary.
The routine is used for the current inside the particle pusher and inside most
of the routines for calculating derived variables. If you have a variable
which needs to add contributions from adjacent processors then you should
calculate the quantity on each processor, including contributions from the
particles to the ghost cells and then call this routine.
\\[0.5cm]
{\Large Notes\\}
\pagebreak

These routines can be used for most MPI calls required by all but the most
extreme modifications to {\EPOCH}. More details on the parallel
implementation of {\EPOCH} is given in the section {\it MPI in \EPOCH}, which
includes information on more extreme changes to {\EPOCH}. However,
realistically one should already be a proficient MPI programmer before
attempting to perform extensive modification to {\EPOCH}.

\subsection{Particle List control functions}
Collections of particles in {\EPOCH} are represented by the #particle_List#
object. These objects abstract much of the operation of the linked lists,
including adding and removing particles and sending particles to other
processors. The functions are as follows:

\pagebreak
\begin{codedef}
FUNCTION create_empty_partlist(partlist)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{create\_empty\_partlist} is a function which takes a particle\_list
object and sets it up so that it points to no particles at all. It should be
used on newly allocated particle\_list objects and when a particle\_list has
served its purpose. It DOES NOT destroy the particles linked in the list at
the point that it is called. If the user wishes to delete all the particles in a
particle\_list then the routine \inlinecode{destroy\_partlist} should be used
instead.
\\[0.5cm]
{\Large Notes\\}

\pagebreak
\begin{codedef}
FUNCTION create_unsafe_partlist(partlist, a_particle, &
    n_elements)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
TYPE(particle), POINTER :: a_particle
INTEGER(KIND=8), INTENT(IN) :: n_elements
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{create\_unsafe\_partlist} is a routine which allows the creation of
a particle\_list which represents a subset of another particle list. This subset
is defined as starting at the particle pointed to by \inlinecode{a\_particle}
and extending for \inlinecode{n\_elements} elements. The new particle\_list is
then flagged as ``unsafe'' because if it is destroyed for any reason then it
will affect other particle lists. Many particle\_list functions can only work
on safe particle lists.
\\[0.5cm]
{\Large Notes\\}

\pagebreak
\begin{codedef}
FUNCTION create_unsafe_partlist_by_tail(partlist, head, &
    tail)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
TYPE(particle), POINTER :: head, tail
\end{codedef}
\vspace{1cm}
{\Large Description\\}
\inlinecode{create\_unsafe\_partlist\_by\_tail} is almost identical to
\inlinecode{create\_unsafe\_partlist}, but instead of specifying the first
particle and a number of elements, the user specifies the first and last
elements in the subset of the particle list. If the particle objects specified
for head and tail are not in the same partlist or tail actually comes before
head then the routine will fail in an undefined manner.
\\[0.5cm]
{\Large Notes\\}

\pagebreak
\begin{codedef}
FUNCTION create_allocated_partlist(partlist, n_elements)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
INTEGER(KIND=8), INTENT(IN) :: n_elements
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{create\_allocated\_partlist} is a helper routine to setup a new
particle\_list and create \inlinecode{n\_elements} new particle objects already
in place in the list.
\\[0.5cm]
{\Large Notes\\}
You should always use this routine when creating large numbers of new particle
objects since there is no guarantee that the internal structure of the
particle\_list objects will not change in the future. This routine will
be modified to reflect any changes in the underlying code.

\pagebreak
\begin{codedef}
FUNCTION create_filled_partlist(partlist, data_in, &
    n_elements)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
REAL(num), DIMENSION(:), INTENT(IN) :: data_in
INTEGER(KIND=8), INTENT(IN) :: n_elements
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{create\_filled\_partlist} is a helper routine to setup a new
particle\_list and create \inlinecode{n\_elements} new particle objects already
in place in the list. These new particle objects are then assigned properties
from the array \inlinecode{data\_in} where the particle properties are contained
in packed form. The particle data is unpacked from the array using the
\inlinecode{unpack\_particle} routine.
\\[0.5cm]
{\Large Notes\\}
You should always use this routine, if possible, when copying particles out of
packed format since there is no guarantee that the internal structure of the
particle\_list objects will not change in the future. This routine will
be modified to reflect any changes in the underlying code.

\pagebreak
\begin{codedef}
FUNCTION test_partlist(partlist)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{test\_partlist} is a routine which tests for various possible types
of error within a particle\_list object. It has a number of possible return
codes for different errors, using negative values for errors so severe that
the main tests cannot be run, or with a bitmask for errors in the main tests.
The return codes are:
\begin{itemize}
\item 0 - No error, particle\_list has passed all tests.
\item -1 - Either the head or tail of the particle\_list object is NULL. This is
  a serious error and usually means that there is a serious error inside the
  particle\_list routines.
\end{itemize}
The other error codes are returned as a bitmask and mean the following
\begin{itemize}
\item 1 - A particle\_list marked as safe has a head element which is linked to
  a preceding particle object.
\item 2 - A particle\_list marked as safe has a tail element which is linked to
  a proceding particle object.
\item 4 - The count property of a particle\_list does not correspond to the
  actual number of objects linked between the head and tail objects. This error
  code on its own usually means that the count property has been modified
  improperly.
\end{itemize}
\vspace{0.5cm}
{\Large Notes\\}
Note that this routine is only intended for debugging and is very slow. It
should never be used by the code in normal operation and all routines should be
written in such a way that it is impossible for a particle\_list object to
become corrupted.

\pagebreak
\begin{codedef}
FUNCTION destroy_partlist(partlist)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{destroy\_partlist} is a helper routine to delete all the particles
attached to a particle\_list and free up the memory that they use. It also
guarantees to leave the particle\_list object itself in a blank state where new
particles can be added to it. It DOES NOT delete the particle\_list object
itself, since it does not know whether or not the particle\_list is dynamically
allocated. If using dynamically allocated particle\_list objects then it is up
to the user to deallocate them AFTER the attached particles are destroyed using
\inlinecode{destroy\_partlist}.
\\[0.5cm]
{\Large Notes\\}
If a particle\_list is deleted without deleting the attached particle objects,
either using this routine or explicitly by the user, then the particles will
become orphaned and sit around using memory until the code ends. If this
happens regularly then the code will quickly crash, usually with a SIG\_SEGV
error.

\pagebreak
\begin{codedef}
FUNCTION copy_partlist(partlist1, partlist2)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist1, partlist2
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{copy\_partlist} is a routine which sets \inlinecode{partlist2} to
point to the same linked list of particles as \inlinecode{partlist1}. It does
not copy the particles, just sets the head and tail pointers of
\inlinecode{partlist1} to point to the same particle objects as
\inlinecode{partlist1}.
\\[0.5cm]
{\Large Notes\\}

\pagebreak
\begin{codedef}
FUNCTION append_partlist(head, tail)
\HRule
TYPE(particle_list), INTENT(INOUT) :: head, tail
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{append\_partlist} is a routine which takes the particles
attached to the particle\_list object \inlinecode{tail} and adds them to the
end of the linked list for particle\_list \inlinecode{head}. The particle\_list
\inlinecode{tail} is then set to be an empty particle\_list.
\\[0.5cm]
{\Large Notes\\}
This routine can only append one safe particle\_list to another safe
particle\_list.

\pagebreak
\begin{codedef}
FUNCTION add_particle_to_partlist(partlist, new_particle)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
TYPE(particle), POINTER :: new_particle
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{add\_particle\_to\_partlist} adds a new
particle (\inlinecode{new\_particle}) to the end of the linked list of
particles in the particle\_list object \inlinecode{partlist}. It deals with
cases of empty particle\_list objects automatically.
\\[0.5cm]
{\Large Notes\\}
If you want to add a new particle to the end of a particle list you should
always use this routine.

\pagebreak
\begin{codedef}
FUNCTION remove_particle_from_partlist(partlist, &
    a_particle)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
TYPE(particle), POINTER :: a_particle
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{remove\_particle\_from\_partlist} removes the particle object
specified by \inlinecode{a\_particle} from the particle\_list object given by
\inlinecode{partlist}. Be very careful that \inlinecode{a\_particle} is indeed
in the linked list pointed to by \inlinecode{partlist}, otherwise it is possible
for the particle\_list object which really does contain \inlinecode{a\_particle}
to be left with an invalid pointer as its head or tail element if
\inlinecode{a\_particle} is either the head or tail element.
\\[0.5cm]
{\Large Notes\\}
Although this routine does work with unsafe particle\_list objects, you should
be very careful using it in this case as it can break the head or tail element
of the primary particle\_list which the unsafe particle\_list is a subset of.
As a general rule, you should only use this routine to remove particles from a
simple particle\_list which is a singly referenced primary, safe particle\_list.

\pagebreak
\begin{codedef}
SUBROUTINE setup_partlists()
\HRule

\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{setup\_partlists} is a routine which is called once when {\EPOCH}
first starts. It sets the variable \inlinecode{nvars} which is the number of
REAL(num) values required to contain all the information about a
single particle object needed when a particle is transferred to another
processor. How the information is packed and unpacked from the particle object
into an array of REAL(num) values is controlled in the functions
\inlinecode{pack\_particle} and \inlinecode{unpack\_particle}.
\\[0.5cm]
{\Large Notes\\}
If the particle type gains additional properties as the result of preprocessor
directives then there should be a line which increments \inlinecode{nvars} by
the correct number when that preprocessor directive is active. For example:
\begin{boxverbatim}
#ifdef PER_PARTICLE_CHARGEMASS
  nvar = nvar+2
#endif
\end{boxverbatim}

\pagebreak
\begin{codedef}
SUBROUTINE   pack_particle(data, a_particle)
SUBROUTINE unpack_particle(data, a_particle)
\HRule
REAL(num), DIMENSION(:), INTENT(INOUT) :: data
TYPE(particle), POINTER :: a_particle
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{pack\_particle} and \inlinecode{unpack\_particle} are subroutines
which are used to copy all the information about a particle
necessary for the particle to be transferred to another processor into a
temporary array before sending to another processor. If a new particle property
has been added to the particle then these routines must be modified to allow
the copying of the new data into the array. The parameter \inlinecode{data} is
a REAL(num) array of length \inlinecode{nvars} and is the array into which the
data either must be packed or from which it must be
unpacked. \inlinecode{a\_particle} is the particle object which must either have
its data copied into the array or be
populated with data from the array. No restriction is placed on how the
data should be packed into the data array, but obviously
\inlinecode{pack\_particle} and \inlinecode{unpack\_particle} must be inverse
operations so that particles packed by one processor can be unpacked correctly
by another processor.
\\[0.5cm]
{\Large Notes\\}
Since it is very unlikely that {\EPOCH} will be run on anything other than a
homogeneous cluster, it is acceptable to use the Fortran \inlinecode{TRANSFER}
function to pack incompatible data types into the \inlinecode{data} array. Just
make sure that \inlinecode{nvars} is defined in \inlinecode{setup\_partlists}
to be long enough to contain all the information. More documentation on the
\inlinecode{TRANSFER} function (which is rarely used and dangerous!) can be
found at \url{http://www.macresearch.org/%
advanced_fortran_90_callbacks_with_the_transfer_function}.

\pagebreak
\begin{codedef}
SUBROUTINE display_particle(a_particle)
\HRule
TYPE(particle), POINTER :: a_particle
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
Displays the key information about a particle given by the parameter
\inlinecode{a\_particle}. Used by\linebreak \inlinecode{compare\_particles}.
\\[0.5cm]

\begin{codedef}
FUNCTION compare_particles(particle1, particle2)
\HRule
TYPE(particle), POINTER :: particle1, particle2
LOGICAL :: compare_particles
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
Compares all the properties of two particle objects and displays the
information if they don't match. Used internally by
\inlinecode{test\_packed\_particles}. If the particle object is extended then
this routine should also be modified to test for equivalence of the new
properties.
\\[0.5cm]

\begin{codedef}
SUBROUTINE test_packed_particles(partlist, data, &
    npart_in_data)
\HRule
TYPE(particle_list), INTENT(IN) :: partlist
REAL(num), DIMENSION(:), INTENT(IN) :: data
INTEGER(KIND=8), INTENT(IN) :: npart_in_data
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{test\_packed\_particles} is a routine which checks that a packed
array of particles can be successfully unpacked back into particle objects. The
parameters are:
\begin{itemize}
\item \inlinecode{partlist} - The particle\_list corresponding to the original
  unpacked particles.
\item \inlinecode{data} - The REAL(num) array containing the packed data.
\item \inlinecode{npart\_in\_data} - The number of particles which were packed
  into the \inlinecode{data} array.
\end{itemize}
The routine tests that the number of particles in the particle\_list match the
number believed to be in the data array, that the length of the data array is
correct and then unpacks each particle in turn from the data array and uses the
\inlinecode{compare\_particles} function to compare the particles with the
original versions in the particle\_list. If any particles fail the comparison
then an error is output to stdout. The error message includes the processor
rank on which the problem occurs but the routine does not specifically include
any MPI commands, so it is possible to call the routine on a subset of
processors.
\\[0.5cm]

\pagebreak
\begin{codedef}
FUNCTION partlist_send(partlist, dest)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
INTEGER, INTENT(IN) :: dest
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{partlist\_send} is a routine for sending all the particles in the
particle\_list object \inlinecode{partlist} to another processor. The
destination processor is identified by its rank which is given by the
\inlinecode{dest} parameter. The routine does not destroy the particle\_list
object which is given to it.
\\[0.5cm]
{\Large Notes\\}
\inlinecode{partlist\_send} uses MPI blocking sends, so unless a matching
\inlinecode{partlist\_recv} has been posted on \inlinecode{dest} then the
routine will deadlock. It would be fairly simple to write a non-blocking
version of \inlinecode{partlist\_send}, but at present no need for such a
routine has been found.

\pagebreak
\begin{codedef}
FUNCTION partlist_recv(partlist, src)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist
INTEGER, INTENT(IN) :: src
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{partlist\_recv} is a routine for receiving particles sent by a call
to \inlinecode{partlist\_send} and loading them into the particle\_list object
\inlinecode{partlist}. The source processor is identified by its rank which is
given by the \inlinecode{src} parameter. The routine destroys the particle\_list
which it is given and indeed will leave orphaned particles if it is not given
an empty particle\_list to receive the data.
\\[0.5cm]
{\Large Notes\\}
\begin{itemize}
\item \inlinecode{partlist\_recv} uses MPI blocking receives, so unless a
  matching \inlinecode{partlist\_send} has been posted on \inlinecode{src} then
  the routine will deadlock. It would be fairly simple to write a non-blocking
  version of \inlinecode{partlist\_recv}, but at present no need for such a
  routine has been found.

\item Although it is not possible to directly use \inlinecode{partlist\_recv}
  to add new particles onto an existing particle\_list, it is only two lines to
  do this. First call \inlinecode{partlist\_recv} with a temporary
  particle\_list to receive the data and then use \inlinecode{append\_partlist}
  to attach the particles to the end of the already populated list.
\end{itemize}

\pagebreak
\begin{codedef}
FUNCTION partlist_send_recv(partlist_send, partlist_recv, &
    dest, src)
\HRule
TYPE(particle_list), INTENT(INOUT) :: partlist_send
TYPE(particle_list), INTENT(INOUT) :: partlist_recv
INTEGER, INTENT(IN) :: dest, src
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/partlist.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{partlist\_sendrecv} is a routine equivalent to
\inlinecode{MPI\_SENDRECV} in that it allows overlapping sends and receives to
be written in a single line rather than the end user having to split processors
into red/black ordered pairs for communication. It sends the particle data in
\inlinecode{partlist\_send} to the processor with rank \inlinecode{dest} and
receives particle data sent by processor \inlinecode{src} and stores it in the
particle\_list \inlinecode{partlist\_recv}. The routine is destructive to both
sending and receiving particle\_lists, and can lead to orphaned particles if a
filled particle\_list is passed as \inlinecode{partlist\_recv}. this is the
routine which is used in the particle boundary conditions.
\\[0.5cm]
{\Large Notes\\}
\begin{itemize}
\item \inlinecode{partlist\_sendrecv} uses MPI blocking sendrecv commands, so
  should be used in matching pairs or the routine will deadlock.

\item Although it is not possible to directly use
  \inlinecode{partlist\_sendrecv} to add new particles onto an existing
  particle\_list, it is only two lines to do this. First call
  \inlinecode{partlist\_sendrecv} with a temporary particle\_list to receive the
  data and then use \inlinecode{append\_partlist} to attach the particles to
  the end of the already populated list.
\end{itemize}
\pagebreak


\subsection{MPI in {\EPOCH}}
{\EPOCH} is a massively parallel code written using standard MPI1.2, and likely
to be upgraded to MPI2 in the near future. Due to the massively parallel nature
of {\EPOCH}, there are MPI commands scattered throughout many parts of the code,
although the MPI has been hidden as far as possible from the end user. The main
use of MPI occurs during I/O, in the boundary conditions and during load
balancing. The MPI setup routines are all in
\inlinecode{src/housekeeping/mpi\_routines.f90}, and the routines which are
used to create the MPI types used by MPI-IO are in
\inlinecode{src/housekeeping/mpi\_subtype\_control.F90}.

\subsubsection{General MPI in {\EPOCH}}
{\EPOCH} uses Cartesian domain decomposition for parallelism and creates an MPI
Cartesian topology using \inlinecode{MPI\_CART\_CREATE}. The use of MPI in
{\EPOCH} is deliberately kept as simple as possible, but there are some points
which must be made and some variables which must be explained.
\begin{itemize}
\item MPI decomposition is reversed compared to array ordering. Due to the
  layout of arrays in Fortran, you get slightly faster performance if you split
  arrays so that the first index remains as long as possible. Since {\EPOCH}
  uses \inlinecode{MPI\_DIMS\_CREATE} to do array subdivision, this means that
  the MPI coordinate system is ordered backwards compared to the main arrays.
  This means that the \inlinecode{coordinates} array which holds the
  coordinates of the current processor in the Cartesian topology is ordered
  as \{coord\_z, coord\_y, coord\_x\}.
\item To make this easier, there are some helper variables. The simplest of
  these just gives the processors attached to each face of the domain on the
  current processor. These variables are named \inlinecode{x\_min, x\_max,
  y\_min, y\_max, z\_min} and \inlinecode{z\_max}.
\item Since it is possible for particles to cross boundaries diagonally there
  is another variable \inlinecode{neighbour} which identifies every possible
  neighbouring processor including those meeting at single edges and at
  corners. \inlinecode{neighbour} is an array which runs (-1:1,-1:1,-1:1) and,
  perhaps inconsistently, is ordered in normal order rather than reversed
  order. This means that \inlinecode{x\_min==neighbour(-1,0,0)} and
  \inlinecode{z\_max==neighbour(0,0,1)}.
\item The variable \inlinecode{comm} is the handle for the Cartesian
  communicator returned from\linebreak MPI\_CART\_CREATE.
\item The variable \inlinecode{errcode} is the standard error variable for all
  MPI communications. However, {\EPOCH} uses the standard
  MPI\_ERRORS\_ARE\_FATAL error handler so this variable is never tested.
\item {\EPOCH} uses a single variable, \inlinecode{status}, to hold all MPI
  status calls. Since there is no non-blocking communication this variable
  is never checked.
\item The rank of the current processor is stored in the variable
  \inlinecode{rank}.
\item The number of processors is stored in \inlinecode{nproc}.
\item The number of processors assigned to any given direction of the Cartesian
  topology is given by \inlinecode{nproc\_\{x,y,z\}}.
\end{itemize}

There are some other variables which are not technically part of the MPI
implementation, but which only exist because the code is running in
parallel. These are
\begin{itemize}
\item #REAL(num) :: {x,y,z}_start_local# - The location of the start of the
  domain on the local processor in real units.
\item #REAL(num) :: {x,y,z}_end_local# - The location of the end of the
  domain on the local processor in real units.
\item #INTEGER, DIMENSION(1:nproc{x,y,z}) :: cell_{x,y,z}_start# - The cell
  number for the start of the local part of the global array in each direction.
\item #INTEGER, DIMENSION(1:nproc{x,y,z}) :: cell_{x,y,z}_end#
  - The cell number for the end of the local part of the global array in each
  direction.
\end{itemize}

\subsubsection{\inlinecode{mpi\_routines.f90}}
\inlinecode{mpi\_routines.f90} is the file which contains all the MPI setup
code. It contains the following routines:
\begin{itemize}
\item mpi\_minimal\_init - Contains code to start MPI enough to
  allow the input deck reader to work. The default {\EPOCH} code setup means
  that it needs to initialise MPI, obtain the rank and the number of processors.
\item setup\_communicator - Routine which creates the Cartesian communicator
  used by the code after the input deck has been parsed. It also populates
  \inlinecode{x\_min, x\_max} etc. It is in its own subroutine so that it can be
  recalled after the start of the window move when the code is using a moving
  window. This is needed since it is valid to have a non-periodic boundary
  before the window starts to move and a periodic boundary afterwards.
\item mpi\_initialise - This routine calls \inlinecode{setup\_communicator} and
  then allocates all the arrays to do with fields, etc. It also sets up the
  particle list objects for each species. If the code is running with only
  manual initial conditions then this routine loads the requested number of
  particles on each processor. Otherwise either the restart or the autoloader
  code load the particles.
\item mpi\_close - This routine performs all the needed cleanup before the
  final call to \inlinecode{MPI\_FINALIZE}.
\end{itemize}

\subsubsection{\inlinecode{mpi\_subtype\_control.F90}}
This file contains all the routines which are used to create the MPI types
which are used in the CFD I/O system. Most of the routines in this section are
used to create the types used for writing the default variables and,
when modifying the code, it is possible to output anything which has the same
shape and size on disk as the default variables without ever having to use the
routines in this file. However, if you are creating more general modifications
which can include variables of different sizes with different layouts across
processors then you may wish to use these routines to create new MPI types which
match your data layout. Any valid MPI type describing the data layout will work
with the CFD library, so there is no absolute need to use these routines. Only
the general purpose subroutines are described here, since most of the other
routines are fairly clear and use these routines internally.

\pagebreak
\begin{codedef}
FUNCTION create_particle_subtype(npart_local)
\HRule
INTEGER(KIND=8), INTENT(IN) :: npart_local
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/mpi\_subtype\_control.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{create\_particle\_subtype} is a routine which creates an MPI type
representing particles which are spread across different processors with
\inlinecode{npart\_local} particles on each
processor. \inlinecode{npart\_local} does not have to be the same number on all
processors.
\\[0.5cm]
{\Large Notes\\}
If you use this routine to write data from multiple species of particle then
the data will be written out in a way which is unhelpful for data analysis,
since the layout will be ``All particles on processor 1, All Particles on
processor 2 etc.'' rather than ``All of species 1, All of species 2''. To create
this type of layout use \inlinecode{create\_ordered\_particle\_subtype}

\pagebreak
\begin{codedef}
FUNCTION create_ordered_particle_subtype(n_species_dump, &
    npart_local)
\HRule
INTEGER, INTENT(IN) :: n_species_dump
INTEGER(KIND=8), DIMENSION(n_species_dump), &
    INTENT(IN) :: npart_local
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/mpi\_subtype\_control.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{create\_ordered\_particle\_subtype} is a routine which creates an
MPI type representing particles from \inlinecode{n\_species\_dump} which are
spread across different processors with \inlinecode{npart\_local(ispecies)}
particles of each species on each processor. \inlinecode{npart\_local} does not
have to be the same number on all processors and does not have to be the same
number for each species.
\\[0.5cm]
{\Large Notes\\}
This routine causes the data to be written to disk as ``All of species 1, All of
species 2 etc.'' as opposed to \inlinecode{create\_particle\_subtype}. It does,
however, need rather more memory and is slower.

\pagebreak
\begin{codedef}
FUNCTION create_field_subtype(n{x,y,z}_local, &
    cell_start_{x,y,z}_local)
\HRule
INTEGER, INTENT(IN) :: n_x_local, n_y_local, n_z_local
INTEGER, INTENT(IN) :: cell_start_x_local
INTEGER, INTENT(IN) :: cell_start_y_local
INTEGER, INTENT(IN) :: cell_start_z_local
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/housekeeping/mpi\_subtype\_control.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{create\_field\_subtype} is a routine which creates an MPI type
representing a field that is defined across some or all of the processors. The
\inlinecode{n\_\{x,y,z\}\_local} parameters are the number of points in the x,
y, z directions (if they exist in the version of the code that you are working
on) that are on the local processor. The
\inlinecode{cell\_start\_\{x,y,z\}\_local} parameters are the offset of the
top, left, back corner of the local subarray in the global array that would
exist if the code was running on one processor. This is an {\it offset}, not a
position and so it begins at \{0,0,0\} NOT \{1,1,1\}.
\\[0.5cm]
{\Large Notes\\}
In EPOCH3D there is also a routine called
\inlinecode{create\_field\_subtype\_2d} which is exactly equivalent to
\inlinecode{create\_field\_subtype} in EPOCH2D and is used for writing the 2D
distribution functions. At present, there are not equivalent 1D functions
except in EPOCH1D, but these could easily by added if required.

\pagebreak

\subsection{The load balancer}
One of the major limiting factors in the scalability of PIC codes is load
balancing. Due to the synchronisation of the currents required for the update
of the EM fields the entire code runs at the speed of the slowest
process. Since most of the time in the main {\EPOCH} cycle is taken by the
particle pusher, this equates to the process with the highest number of particles
being the slowest. Since the location of particles is dependent upon the
solution of the problem under consideration, in general the code will not have
exactly the same number of particles on each processor. The load balancer is
used to move the inter-processor boundaries so that the number of particles is
as close to the same on each processor as possible. The load balancer is
invoked at the start of the code and when the ratio of the least loaded
processor to the most loaded processor falls below a user specified critical
point.\\

{\EPOCH}'s load balancer works by rearranging the processor boundaries in 1D
sweeps in each direction, rather than attempting to perform multidimensional
optimisation. Also, at present the MPI in {\EPOCH} requires each processor to be
simply connected at every point, so it must have one processor to the left, one
to the front etc. which introduces a further restriction on the load
balancer. Otherwise, the load balancer is fully general. The load balancing
sweep is illustrated in Figure~\ref{sweep}.
The load balancer is
implemented in the file \inlinecode{src/housekeeping/balance.F90} and is called
by the routine \inlinecode{balance\_workload(override)}. The parameter
\inlinecode{Override} is used to force the code to perform a load balancing
sweep even when it would normally determine that the imbalance is not large
enough to force a load balancing sweep. Although the load balancer is hard
coded to load balance in all available directions, the code is written in such
a way that it is possible to modify the code to load balance in only one
direction, or to automatically determine which single direction gives the best
performance.

\captionedimage{./images/sweep}{sweep}{Illustration of the load balancing sweep}

The details of the load balancer are fairly intricate, and if major
modification to the load balancer is required, it is recommended that the
original authors be contacted for detailed advice on how to proceed. However,
the general layout of the routine is as follows.

\begin{itemize}
\item Use MPI\_ALLGATHER to get the total number of particles on each processor
  and determine the global minimum and maximum number of particles. If the
  ratio of the minimum to the maximum is above the load balance threshold then
  just return from the subroutine.
\item The code uses the routines \inlinecode{get\_density\_in\_x},
  \inlinecode{get\_density\_in\_y} and \inlinecode{get\_density\_in\_z} to
  determine the pseudoparticle number density along each direction. Each of
  these routines integrates in both other directions.
\item This section of the code fills the arrays \inlinecode{starts\_\{x,y,z\}}
  and \inlinecode{ends\_\{x,y,z\}}. These arrays contain the starting and
  ending cell numbers of the hypothetical global array in each direction for
  each processor.
\item The routine \inlinecode{redistribute\_fields} is then called to move the
  information about field variables which cannot be recalculated. If new field
  variables are created that cannot be recalculated after the load balancing is
  completed then \inlinecode{redistribute\_fields} has to be modified for these
  new variables.
\item The next section of the routine deals with those variables which can be
  recalculated after the load balance sweep is complete, such as the coordinate
  axes and the arrays which hold the particle kinetic energy.
\item The penultimate section of the routine then changes the variables which
  tell the code where the edges of its domain lie in real space to reflect the
  changed shape of the domains.
\item The final part is the call to \inlinecode{distribute\_particles} which
  moves the particles to the new processor. Once this is
  finished, the code should have as near as possible the same number of
  particles on each processor.
\end{itemize}

Most of the load balancer is purely mechanical and should only be changed if
the way in which the code is to perform load balancing is fundamentally
altered. The redistribution of particles that takes place in
\inlinecode{distribute\_particles} uses the standard particle\_list objects, so
that if the necessary changes have been made to the routines in
\inlinecode{src/housekeeping/partlist.F90} to allow correct
boundary swaps of particles then the load balancer should work with no further
modification. The only part of the load balancer which should need changing is
\inlinecode{redistribute\_fields} which requires explicit modification if new
field variables are required. For fields which are the same shape as the main
array there is significant assistance provided within the code to make the
re-balancing simpler. There are also routines which can help with re-balancing
variables which are the size of only one edge or face of the domain. Variables
which are of a completely different size but still need to be rebalanced when
coordinate axes move have to have full load balancing routines implemented by
the developer. This is beyond the scope of this manual and any developer who
needs assistance with development such a modification should contact the original
author of the code. The field balancer is fairly simple and mostly calls one of
three routines: \inlinecode{redistribute\_field} and either
\inlinecode{redistribute\_field\_2d} or \inlinecode{redistribute\_field\_1d}
depending on the dimensionality of your code. To redistribute full field
variables the routine to use is \inlinecode{redistribute\_field}, and an
example of using the code looks like:
\begin{boxverbatim}
  temp = 0.0_num
  CALL redistribute_field(new_domain, bz, temp)
  DEALLOCATE(bz)
  ALLOCATE(bz(-2:nx_new+3,-2:ny_new+3))
  bz = temp
\end{boxverbatim}

In this calling sequence the
\inlinecode{redistribute\_field} subroutine is used to redistribute the field
\inlinecode{bz}, and the newly redistributed field is copied
into \inlinecode{temp}; an array which is already allocated to the
correct size. The \inlinecode{new\_domain} parameter is an array indicating the
location of the start and end points of the new domain for the current processor
in gridpoints offset from the start of the global array. It is passed into the
\inlinecode{redistribute\_fields} subroutine as a parameter from the
\inlinecode{balance\_workload} subroutine and should not be changed. The
\inlinecode{temp} variable is needed since Fortran standards before Fortran2000
do not allow the deallocation and reallocation of parameters passed to a
subroutine. There is a more elegant solution, where \inlinecode{temp} is
hidden inside the \inlinecode{redistribute\_field} subroutine. However, support
for this in current Fortran2000 implementations is unreliable.\\

The routine for re-balancing variables which lie along an edge of the domain are
very similar and are demonstrated in the \inlinecode{redistribute\_fields}
subroutine for lasers attached to different boundaries. It is
recommended that a developer examine this code when developing new routines.

\subsection{{\EPOCH} I/O}

{\EPOCH} uses a file format called {\it CFD} which was custom developed for use
with codes developed by CFSA at the University of Warwick. There is documentation
for the internal structure of CFD files available from
CFSA, but it is not necessary to have a full understanding of the file format
to add the output of new variables to {\EPOCH}. To add a new variable to
{\EPOCH}'s output, you simply have to use the supplied subroutines of the CFD
library which is part of {\EPOCH}. The file output from {\EPOCH} takes place in
{\it diagnostics.F90}, so to add new variables to the output you must
add additional code there. Looking through the listings, you will see two lines:
\begin{boxverbatim}
  CALL cfd_open(filename, rank, comm, MPI_MODE_CREATE + MPI_MODE_WRONLY)

  CALL cfd_close()
\end{boxverbatim}

These, as may be expected, are the commands which open and close the CFD
file. It is perfectly possible to create new CFD files containing only your own
data, but currently the CFD routines only permit one open file at a time. There
are various commands in-between which actually write the data into the
file. Most of these commands start with \inlinecode{cfd\_} which marks them as
CFD file commands. Some more complex areas of I/O, such as the particle probes
and the distribution function routines call other subroutines in their
respective source files, but these too make use of the CFD routines to actually
write data. A user should never try to write data directly to the output file,
since this will cause problems with internal parts of the CFD format and
generate a nonsensical file. There are routines in CFD which allow the writing
of arbitrary data into a CFD file which are described later.

\subsubsection{dumpmask}
Looking through {\it diagnostics.F90} there are many lines with commands which
begin \inlinecode{cfd\_}, but they are all prepended with a command which looks
like:
\begin{boxverbatim}
  IF (IAND(dumpmask(28), code) .NE. 0) THEN
\end{boxverbatim}
This is the method by which {\EPOCH} allows the end user to specify whether a
variable should be dumped, and whether it should only be dumped at
full/partial/restart dumps. #dumpmask# is an integer array, the length of which is
defined by the variable \inlinecode{num\_vars\_to\_dump} in
\inlinecode{shared\_data.f90} and contains the bitmask representing all the
types of output which should be written for the associated variable. The
possible values in the bitmask are:

\begin{itemize}
\item \inlinecode{c\_io\_never} - Never dump this variable.
\item \inlinecode{c\_io\_always} - Dump this variable at every output dump.
\item \inlinecode{c\_io\_full} - Dump this variable at full dumps.
\item \inlinecode{c\_io\_restartable} - Dump this variable for restart dumps.
\item \inlinecode{c\_io\_species} - If meaningful for this variable, write
  information for each species rather than integrated over all species.
\end{itemize}

When adding a new variable to be written to disk, the value of
\inlinecode{num\_vars\_to\_dump} should be increased to match the number of new
written variables. Next, open the file \inlinecode{src/deck/deck\_io\_block.F90}
and find the line:
\begin{boxverbatim}
  CHARACTER(LEN=entry_length), DIMENSION(io_block_elements) :: io_block_name =  ...
\end{boxverbatim}

Simply add new strings for your new variables to the end of the
definitions. These new variable names should then be placed in your input decks
in the same place as the existing I/O information and take the same parameters.

\subsubsection{Actually writing the variables}
There is a full manual for the CFD file format describing all the commands
already in existence and all planned commands for the future, so in this
manual CFD commands will only be presented in the section where they are used.

\subsection{Adding a derived variable}

As already stated, derived variables are variables which are defined on the
Cartesian spatial grid but are not directly updated by the solver. They are
calculated when needed for output or for use in other physics packages. The
final form of a derived variable is an array on each processor with the same
size as the field arrays.

\subsubsection{Writing your variable to disk}
A derived variable is defined on the same grid as the main simulation variables
and must be written to disk in such a way as to stitch the parts of the grid
from each processor together. This is achieved using one of the routines:
\begin{boxverbatim}
CALL cfd_write_1d_cartesian_variable_parallel(name, group_name, dims, &
    stagger, grid_name, grid_group, variable(1:nx), subtype_field)

CALL cfd_write_2d_cartesian_variable_parallel(name, group_name, dims, &
    stagger, grid_name, grid_group, variable(1:nx,1:ny), subtype_field)

CALL cfd_write_3d_cartesian_variable_parallel(name, group_name, dims, &
    stagger, grid_name, grid_group, variable(1:nx,1:ny,1:nz), subtype_field)
\end{boxverbatim}
The parameters have the following types and meanings:

\begin{itemize}
\item name - The name of the variable.
\item group\_name - The group of the variable. In IDL and Matlab a variable can
  be loaded either by group\_name or name, and in VisIt, the variable name
  appears under a submenu of the variable groupname.
\item dims - An nD integer array containing the GLOBAL length of the variable
  across all processors. In {\EPOCH} a variable actually called ``dims'' exists
  for variables which are the same size as the default field variables.
\item stagger - An nD REAL(num) array containing the stagger of a variable from
  the cell centre in fractions of a cell. This property is not used in {\EPOCH},
  so the default variable ``stagger'' can be used safely.
\item grid\_name - The name of the grid to which the variable is attached. In
  {\EPOCH}, the main grid is just called ``Grid''. Note that this property is
  case sensitive.
\item grid\_group - The groupname of the grid to which the variable is
  attached. In {\EPOCH}, the main grid\_group is just called ``Grid''. Note that
  this property is case sensitive.
\item variable - The actual variable to be written to disk. The variable passed
  here must be a REAL(num) array(1:nx,1:ny,1:nz),(1:nx,1:ny),(1:nx) depending
  on dimensionality of code. It is acceptable to pass an array subsection to
  this routine.
\item subtype\_field - This is an MPI type representing the layout of the data
  across the processors. For a standard field variable, there is an
  automatically created type called ``subtype\_field'' which should be used
  here.
\end{itemize}

It's probably easiest to read the diagnostics.f90 file and see how the code
implements the output of simple variables like ex or ey for an example of how
this works. Once the appropriate cfd\_write call has
been added to the code, there is no further work
to be done. The IDL, Matlab and VisIt routines will all read the existence of
the variable from the metadata in the CFD file, and it will now be available to
view in all CFD reading packages.

There is a working variable called \inlinecode{data} which is large enough to
contain the data which is large enough to store a derived variable. It is
therefore recommended that to calculate derived variables a new subroutine
should be created which populates \inlinecode{data} with the required variable
and then write it to disk. An example would look like:
\begin{boxverbatim}
IF (IAND(dumpmask(30), code)) THEN
  CALL calc_my_variable(data)
  CALL cfd_write_2d_cartesian_variable_parallel("my_variable", "derived", &
      dims, stagger, grid_name, grid_group, data(1:nx,1:ny), subtype_field)
ENDIF
\end{boxverbatim}
where \inlinecode{calc\_my\_variable} is a function which calculates the
variable which you wish to write. The form of this function depends on the type
of variable to be calculated and is given in the next section.


\subsection{Adding a particle variable}
The next simplest type of output to add is a new property for all particles. To
add a new particle variables dump two things are needed: a call to the CFD
command to write the data and an iterator function to iterate through all the
particles. The iterators are stored in the file {\it iterators.F90}. An example
iterator is:
\begin{boxverbatim}
! Iterator for particle processor
SUBROUTINE iterate_species(data, n_points, start)

  REAL(num), DIMENSION(:), INTENT(INOUT) :: data
  INTEGER(8), INTENT(INOUT) :: n_points
  LOGICAL, INTENT(IN) :: start
  TYPE(particle), POINTER, SAVE :: cur
  INTEGER(8) :: partcount
  REAL(num) :: root

  TYPE(particle_list), POINTER, SAVE :: current_list
  TYPE(particle_family), POINTER, SAVE :: current_family

  IF (start)  THEN
    CALL start_particle_family(current_family, current_list, cur)
  ENDIF
  partcount = 0
  DO WHILE (ASSOCIATED(current_family) .AND. (partcount .LT. n_points))
    DO WHILE (ASSOCIATED(current_list) .AND. (partcount .LT. n_points))
      DO WHILE (ASSOCIATED(cur) .AND. (partcount .LT. n_points))
        partcount = partcount+1
        data(partcount) = REAL(current_family%id, num) ! This is
           ! the line which writes the data
        cur=>cur%next
      ENDDO
      ! If the current partlist is exhausted, switch to the next one
      IF (.NOT. ASSOCIATED(cur)) CALL Advance_particle_list(current_list, cur)
    ENDDO
    ! If the current particle_family is exhausted then switch to the next one
    DO WHILE (.NOT. ASSOCIATED(cur))
      CALL advance_particle_family(current_family, current_list, cur)
      IF (.NOT. ASSOCIATED(current_family)) EXIT
      IF ((.NOT. current_family%dump) .AND. (.NOT. iterator_settings%restart)) &
         NULLIFY(cur)
    ENDDO
  ENDDO
  n_points = partcount

END SUBROUTINE iterate_species
\end{boxverbatim}

This is a fairly complicated routine, and includes routines to deal with the
possibility of particle species not being dumped, and other similar
effects. Luckily, there is only one line in the routine which needs to change
to output a new routine. The is the line:
\begin{boxverbatim}
  data(partcount) = REAL(current_family%id, num)
\end{boxverbatim}

To write a new iterator, you just have to copy the skeleton of an existing
iterator and change this line to copy your particle property into the ``data''
array. The details of how this routine works is explained in the ``particles,
particle\_lists and particle\_families'' of the ``deep internals'' section of
the developer guide. Once your new iterator has been written and added into the
{\it iterators.F90} file, it's time to add the CFD routine to actually write
the data. The routine is:
\begin{boxverbatim}
CALL cfd_write_nd_particle_variable_with_iterator_all(name, groupname, &
    iterator_name, npart_dump_global, n_part_per_it, gridname, gridgroup, &
    subtype_particle_var)
\end{boxverbatim}

The parameters this time are
\begin{itemize}
\item name - The name of the variable to be output.
\item groupname- The name of the group that the variable is in. Normally all
  {\EPOCH} particle variables are in group ``particle'', but there is no reason
  to enforce this.
\item iterator\_name - The name of the iterator function that you created in
  the previous step. Note that this is not a string but simply the name of the
  function.
\item npart\_dump\_global - The number of particles to be written to disk
  across all processors. When writing a standard particle variable this is the
  named variable ``npart\_dump\_global''.
\item npart\_per\_it - The number of particles to write for each loop of the
  iterator. This should normally be the named variable ``npart\_per\_it''.
\item gridname - The name of the grid (particle positions) that the variable
  corresponds to. In {\EPOCH}, this is ``Particles''.
\item gridgroup - The group of the grid (particle positions) that the variable
  corresponds to. In {\EPOCH}, this is ``part\_grid''.
\item subtype\_particle\_var - An MPI type representing the layout of particles
  across processors species by species. For a default particle variable, this
  is the named variable ``subtype\_particle\_var''.
\end{itemize}

Once again, looking at how this is implemented for one of the existing
variables (e.g. Px) is probably the most enlightening way to see how it
works. As for the fluid variables, the new variable will appear in IDL, Matlab
and VisIt.

At this point, it is possible to write any property which is similar to the
default field variables or the default particle properties. It becomes slightly
more challenging if you want to write other types of variable into an output
file. Before you can do this, you really need to fully understand how the MPI
parallelism is implemented in {\EPOCH} by reading the section {\it MPI in
{\EPOCH}}. A full description of how to do this is beyond the scope of this
manual, but the same basic commands should be used to write to the CFD file as
are used to write more normal variables, and the CFD manual should be consulted
if more sophisticated changes are needed.

\pagebreak

\subsection{Precompiler directives}
{\EPOCH} uses precompiler directives to switch certain features of the code on
or off. The precompiler directives all begin with a ``\#'' character and look
like:
\begin{boxverbatim}
#ifdef MY_PRECOMPILER_DIRECTIVE
  SOME_FORTRAN_OF_SOME_KIND
#else
  SOME_OTHER_FORTRAN
#endif
\end{boxverbatim}
They behave in a very simple manner. The precompiler runs BEFORE the
Fortran compiler and, until it reaches a precompiler directive, it just creates
a temporary file which is an exact copy of the source file. When it reaches a
precompiler directive of this kind it treats the \#ifdef commands as
if/then/else statements. If
\inlinecode{MY\_PRECOMPILER\_DIRECTIVE} was defined in the makefile then
\inlinecode{SOME\_FORTRAN\_OF\_SOME\_KIND} is pushed out to the temporary
file. Otherwise \inlinecode{SOME\_OTHER\_FORTRAN} is written instead.
The precompiler directives themselves are never output to the temporary
file. Once then preprocessor has finished, it passes this temporary file
to the Fortran compiler which can then compile it just like any other
standard Fortran file.

\subsubsection{When to use precompiler directives}
\begin{itemize}
\item When adding properties to the \inlinecode{particle} structure.
\item When adding time consuming calculations to the particle pusher.
\end{itemize}
Precompiler directives should be avoided when there is no significant
performance gain or memory reduction to be made. Wherever possible, optional
features should be controlled by parameters in the input deck.

\subsubsection{The directive printing routine on code startup}
When {\EPOCH} starts it prints the precompiler directives that it was built with
and what they mean. This isn't required, but has proved very useful and is
implemented in a very simple way. Just open the file
\inlinecode{src/housekeeping/welcome.F90} and find the subroutine
\inlinecode{compiler\_directives}. There are a large block of precompiler
directives which read:
\begin{boxverbatim}
#ifdef TRACER_PARTICLES
    defines = IOR(defines, c_def_tracer_particles)
    WRITE(*, *) "Tracer particle support -DTRACER_PARTICLES"
#endif
\end{boxverbatim}

Simply add a new element to the end of the list.
\begin{boxverbatim}
#ifdef MY_PRECOMPILER_DIRECTIVE
    defines = IOR(defines, c_def_my_precompiler_directive)
    WRITE(*,*) "My new physics -DMY_PRECOMPILER_DIRECTIVE"
#endif
\end{boxverbatim}

You will also need to add #c_def_my_precompiler_directive# to the list of
constants in #src/shared_data.F90#.

\subsubsection{Precompiler directives and the input deck}
In theory, it is possible for someone to request a feature of the code in the
input deck which this version hasn't been compiled with. In this
case, there is a special error code \inlinecode{c\_err\_pp\_options\_wrong}
which causes the input deck parser to give a meaningful error. You should also
set the string\linebreak \inlinecode{extended\_error\_string} to be the define
command for the missing preprocessor directive i.e
\linebreak\inlinecode{extended\_error\_string = "-DMY\_PRECOMPILER\_DIRECTIVE"}

\section{{\EPOCH} front end programming}

\subsection{Strings in {\EPOCH}}
Fortran is not a language famous for its string handling capabilities, but due
to the presence of the input deck {\EPOCH} has fairly extensive string handling
routines. Strings used are all of the standard Fortran \inlinecode{CHARACTER}
type and are defined as:
\begin{boxverbatim}
CHARACTER(LEN=entry_length) :: string
\end{boxverbatim}
\inlinecode{entry\_length} is a global constant defined in
\inlinecode{src/shared\_data.F90} which can be increased to allow {\EPOCH} to
handle longer strings. There may be reasons to increase this length if you wish
to use long complex expressions in the input deck. Note that many Fortran
compilers do not allow strings to exceed 512 characters in length.

\subsubsection{{\EPOCH} string handling routines}

Listed here are the string handling routines (other than those in the core
maths parser routine which are documented elsewhere) which are currently used
in {\EPOCH}.

\pagebreak
\begin{codedef}
FUNCTION str_cmp(str_in, str_test)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in, str_test
LOGICAL :: str_cmp
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{str\_cmp} is the routine which does all the string comparisons in
{\EPOCH}. It deals with leading and trailing whitespace automatically and
tests for length differences. It does not test for strings being
valid substrings of each other, only for full equality.
\\[0.5cm]
{\Large Notes\\}
A developer should always use \inlinecode{str\_cmp} rather than doing their own
string testing to ensure consistent behaviour across the entire {\EPOCH} code
base.

\pagebreak
\begin{codedef}
FUNCTION as_real_simple(str_in, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
INTEGER, INTENT(INOUT) :: err
REAL(num) :: as_real_simple
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{as\_real\_simple} is a routine to convert a string into a real
number without invoking the maths parser. It can cope with standard form as
well as simple decimal reals. It is significantly faster than the maths parser,
but should only be used when the user explicitly {\it shouldn't} be able to use
a mathematical expression. If the string cannot be parsed then the routine sets
the bitmask \inlinecode{c\_err\_bad\_value} on the parameter \inlinecode{err}
\\[0.5cm]
{\Large Notes\\}
If you have a string which has to be converted into a real quickly then this is
the routine to use. You probably shouldn't use it when parsing a string from
the input deck, since there is no reason to restrict the user from specifying a
mathematical expression. The routine is used inside the maths parser to parse
simple numbers.

\pagebreak
\begin{codedef}
FUNCTION as_integer_simple(str_in, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
INTEGER, INTENT(INOUT) :: err
INTEGER :: as_integer_simple
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{as\_integer\_simple} is a routine to convert a string into an
integer without invoking the maths parser. It can cope with standard form as
well as simple decimal integers. It is significantly faster than the maths
parser, but should only be used when the user explicitly {\it shouldn't} be
able to use a mathematical expression. If the string cannot be parsed then the
routine sets the bitmask \inlinecode{c\_err\_bad\_value} on the parameter
\inlinecode{err}
\\[0.5cm]
{\Large Notes\\}
This routine is used internally in several parts of the code when parsing
things like numbers which are parts of strings (i.e. the 1 in
\inlinecode{begin:species1}). It probably shouldn't be used to directly parse
input deck parameters, since there is no reason to restrict the user from
specifying mathematical expressions.

\pagebreak
\begin{codedef}
FUNCTION as_long_integer_simple(str_in, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
INTEGER, INTENT(INOUT) :: err
INTEGER(KIND=8) :: as_long_integer_simple
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{as\_long\_integer\_simple} is equivalent to
\inlinecode{as\_integer\_simple}, but returns the larger
\inlinecode{INTEGER(KIND=8)} rather than a normal \inlinecode{INTEGER(KIND=4)}.
\\[0.5cm]
{\Large Notes\\}

\pagebreak
\begin{codedef}
FUNCTION as_direction(str_in, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
INTEGER, INTENT(INOUT) :: err
INTEGER :: as_direction
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{as\_direction} is used when assigning a laser to a boundary and
recognises the strings
\begin{itemize}
\item x\_min or left - c\_bd\_x\_min.
\item x\_max or right - c\_bd\_x\_max.
\item y\_min or down - c\_bd\_y\_min.
\item y\_max or up - c\_bd\_y\_max.
\item z\_min or back - c\_bd\_z\_min.
\item z\_max or front - c\_bd\_z\_max.
\end{itemize}
It returns the associated direction code (given after the dash in the
definition).
\\[0.5cm]
{\Large Notes\\}
If you're writing code which requires attaching something to a boundary,
whether a boundary condition, a diagnostic or some other routine, then this is
the routine that should be used. Note that in order to prevent confusion when
moving input decks between different dimension versions of {\EPOCH}, each code
only recognises the strings for boundaries that it actually has.

\pagebreak
\begin{codedef}
FUNCTION as_logical(str_in, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
INTEGER, INTENT(INOUT) :: err
Logical :: as_logical
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{as\_logical} simply tests for the strings ``T'' and ``F'' to
determine a boolean value. The default behaviour of \inlinecode{as\_logical} is
to treat any string that isn't ``T'' as a false value.
\\[0.5cm]
{\Large Notes\\}
You should use this rather than using a 0/1 boolean flag in the input deck for
consistency.

\pagebreak
\begin{codedef}
SUBROUTINE split_off_int(str_in, str_out, int_out, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
CHARACTER(LEN=entry_length), INTENT(OUT) :: str_out
INTEGER, INTENT(OUT) :: int_out
INTEGER, INTENT(INOUT) :: err
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings\_advanced.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{split\_off\_int} is a routine which splits a string of the format
{\bf string{\it n}} into a string {\bf string} and an integer {\it n} which are
returned separately in the \inlinecode{str\_out} and \inlinecode{int\_out}
parameters respectively.  If it can't split the string successfully then it
sets the \inlinecode{c\_err\_bad\_value} bitfield of the err parameter.
\\[0.5cm]
{\Large Notes\\}
This is used in the core of the deck parser to deal with blocks like the
numbered species blocks in the initial conditions, and also in some of the
specific block parsers. Again, this routine should be used to split strings
like this rather than coding a new routine.

\pagebreak
\begin{codedef}
SUBROUTINE split_range(str_in, real1, real2, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
REAL(num), INTENT(OUT) :: real1, real2
INTEGER, INTENT(INOUT) :: err
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings\_advanced.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{split\_range} is a routine which splits a string of the format
{(\bf{\it n}, {\it m})} into two reals {\it n} and {\it m} which are returned
separately in the \inlinecode{real1} and \inlinecode{real2} parameters
respectively.  If it can't split the string successfully then it sets the
\inlinecode{c\_err\_bad\_value} bitfield of the err parameter.
\\[0.5cm]
{\Large Notes\\}
This is used when specifying ranges in the input decks at present. Any
ranges which should be specified in a single parameter should be specified in
this form and this routine used to split the string.\\ {\bf IMPORTANT NOTE. This
routine has proved somewhat unsatisfactory and may be replaced by another
routine in the future. It is expected that the function will have the same name
and behaviour, but might have subtle differences.}

\pagebreak
\begin{codedef}
FUNCTION as_integer(str_in, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
INTEGER, INTENT(INOUT) :: err
INTEGER :: as_integer
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings\_advanced.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{as\_integer} is the routine which returns integers from strings
using the maths parser. If a mathematical expression resolves to a non-integer
result then this routine rounds to the NEAREST integer. There are explicit
rounding routines in the maths parser to force other behaviour.
\\[0.5cm]
{\Large Notes\\}
This routine should be used when evaluating most strings into integers. Note
that this routine evaluates spatially dependent quantities at (0,0) on each
processor, so will give unpredictable results when spatially dependent
quantities are given to it (like rho, bx etc.). To evaluate a spatially varying
quantity use \inlinecode{evaluate\_string\_in\_space}.

\pagebreak
\begin{codedef}
FUNCTION as_long_integer(str_in, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
INTEGER, INTENT(INOUT) :: err
INTEGER(KIND=8) :: as_long_integer
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings\_advanced.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{as\_long\_integer} is the routine which returns long integers from
strings using the maths parser. If a mathematical expression resolves to a
non-integer result then this routine rounds to the NEAREST integer. There are
explicit rounding routines in the maths parser to force other behaviour.
\\[0.5cm]
{\Large Notes\\}
This routine should be used when evaluating strings which are likely to be too
large to be stored in an INTEGER(KIND=4).

\pagebreak
\begin{codedef}
FUNCTION as_real(str_in, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
INTEGER, INTENT(INOUT) :: err
REAL(num) :: as_real
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings\_advanced.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{as\_real} is the routine which returns reals from strings using the
maths parser.
\\[0.5cm]
{\Large Notes\\}
This routine should be used when evaluating most strings into reals. Note that
this routine evaluates spatially dependent quantities at (0,0) on each
processor, so will give unpredictable results when spatially dependent
quantities are given to it (like rho, bx etc.). To evaluate a spatially varying
quantity use \inlinecode{evaluate\_string\_in\_space}.

\pagebreak
\begin{codedef}
SUBROUTINE evaluate_string_in_space(str_in, data_out, &
    xrange, {yrange}, {zrange}, err)
\HRule
CHARACTER(LEN=entry_length), INTENT(IN) :: str_in
REAL(num), DIMENSION(1:,{1:},{1:}), INTENT(OUT) :: data_out
INTEGER, DIMENSION(2), INTENT(IN) :: xrange, {yrange}
INTEGER, DIMENSION(2), INTENT(IN) :: {zrange}
INTEGER, INTENT(INOUT) :: err
\end{codedef}
\vspace{1cm}
{\Large Location\\}
\inlinecode{src/deck/strings\_advanced.F90}\\
\\[0.5cm]
{\Large Description\\}
\inlinecode{evaluate\_string\_in\_space} is a routine which is used to evaluate
a tokenized maths expression over a region of the domain. The dimensionality of
\inlinecode{data\_out}, and the presence or absence of \inlinecode{yrange} and
\inlinecode{zrange} depend on the dimensionality of the code being used. The
\inlinecode{{\it $\alpha$}range} parameters represent the indices in that
direction over which the expression should be evaluated. For example, in 2D to
evaluate an expression over the entire domain, the code would look like:
\begin{boxverbatim}
REAL(num), DIMENSION(:,:), ALLOCATABLE :: data
ALLOCATE(data(0:nx+1,0:ny+1)
CALL evaluate_string_in_space(string, data, (/0,nx+1/), (/0:ny+1/), err)
\end{boxverbatim}
{\Large Notes\\}
This routine is suitable to evaluate expressions over a subsection or all of
the domain, and is used in this way in the initial condition deck parser
routines. However, the routine does have one significant weakness, which is
that it tokenizes the string each time it is called. Tokenizing the string is a
time consuming process, so if the string is to be evaluated several times for
different reasons (for example, the time profile for the laser) then a
different procedure should be followed using the lower level parser routines,
which are described in the section on the maths parser.
\pagebreak

\subsection{Permanently adding blocks to the input deck}

While using the \inlinecode{custom\_deck} subroutine is a good way of passing
parameters into the code or for temporary additions, it is not suitable for
permanent additions to the code. Adding new blocks to the code permanently is
very similar to doing it temporarily, but requires changes to some of the
subroutines in \inlinecode{deck.F90}.\\

There are four subroutines which may need to be changed to add new blocks to
the deck. These are
\begin{itemize}
\item \inlinecode{start\_block(block\_name)} - Called when the deck directive
  \inlinecode{begin:{\it block\_name}} appears in a deck file.
\item \inlinecode{end\_block(block\_name)} - Called when the deck directive
  \inlinecode{end:{\it block\_name}} appears in a deck file.
\item \inlinecode{handle\_block(block\_name, block\_element, block\_value)} -
  Called once for each element in a block.
\item \inlinecode{check\_compulsory\_blocks(errcode\_deck)} - Called once when
  the deck file has been read to check that all necessary blocks have been
  populated.
\end{itemize}

There is one final variable which is important for modifying the input deck,
\inlinecode{deck\_state}. The input deck parser routine used to read the main
input deck uses the variable \inlinecode{deck\_state} to
determine which stage of parsing the deck is required. The possible values of
\inlinecode{deck\_state} are

\begin{itemize}
\item c\_ds\_deck - The main input deck.
\item c\_ds\_ic - The initial conditions.
\item c\_ds\_eio - The extended I/O.
\end{itemize}
These constants are defined in \inlinecode{shared\_data.F90} and if new files
need to be read, then new variables should be created in the same place with
similar names using the \inlinecode{c\_ds\_} prefix.

The layout of \inlinecode{start\_block} and \inlinecode{end\_block} is very
simple, and just tests \inlinecode{deck\_state} to make sure that the deck
reader is reading the right file. It then uses \inlinecode{str\_cmp} to test the
block name and if necessary calls the correct routine to deal with the
start or finish of a block.

The main routine which needs to be explained is \inlinecode{handle\_block},
which is the function that determines which subroutine to call for parsing the
elements of a given block. The function is very simple and has the same basic
sequence as the \inlinecode{start\_block} and \inlinecode{end\_block}
functions. First determine what file is being parsed by checking
\inlinecode{deck\_state}, then use \inlinecode{str\_cmp} to compare the
\inlinecode{block\_name} parameter with the list of known block names. What
happens next is dependent on exactly what a developer wants their block to do.
At the simplest level the routine simply calls another function which
takes the element\_name and element\_value as
parameters and returns an error code from Appendix A determining the success or
failure of reading the element. Some blocks have slightly more sophisticated
entries in \inlinecode{handle\_deck}, such as the \inlinecode{species} block in
the initial conditions file reader. This strips the species number off the end
of the block name and passes it to the handler routine. The routines that deal
with constants and DEOs are implemented before any tests to
\inlinecode{deck\_state} since it is valid to set constants and DEOs in any
type of file.

The final routine is \inlinecode{check\_compulsory\_blocks} which is used to
check that all the needed elements of the input deck have been set. A single
parameter \inlinecode{errcode\_deck} is passed in. Once again, the routine
checks \inlinecode{deck\_state} to make sure that it is testing the correct
type of input deck file. It then goes through and calls functions to check that
all the necessary parts of a block have been set. The subroutines are contained
in the same file as the routine which is called in \inlinecode{handle\_block} to
handle elements of the block. The error handler functions should return an
error code from Appendix A, usually \inlinecode{c\_err\_missing\_elements}. The
return code from the error handler function should then be \inlinecode{IOR}ed
with \inlinecode{errcode\_deck} to allow error codes to be returned from
several different checks with errors occurring.

\subsubsection{The element handler routines for deck elements}
The exact form of the handler routines is up to the end user. The only {\it
requirements} are that the routine should return an error code detailing
whether or not there are any problems with reading the block and that the error
code should be \inlinecode{c\_err\_none} if either the element name or element
value are the special constant \inlinecode{blank}. The typical implementation
of an element handler routine is shown in the file
\inlinecode{src/deck/deck\_control\_block.f90}, and this general layout should
be copied for compatibility if possible.\\

Sometimes, it is useful to have each new block correspond to a new instance of
an object in the code. An example of this in {\EPOCH} is in
\inlinecode{src/deck/deck\_ic\_laser\_block.f90} where each new laser block in
the input deck corresponds to a new laser being attached to a boundary. This is
accomplished by implementing the lasers as a linked list on each boundary,
with a new laser object being created when a laser block is started, the laser
information being set during the main reader routine, and then the laser being
attached to the linked list by a call to \inlinecode{attach\_laser} in
\inlinecode{src/laser.f90} when the block is ended. When a new laser block is
started the process simply repeats allowing the end user to have as many lasers
as desired.

\subsubsection{Naming new element handler files}
While the name of the function which deals with elements of a new block is up
to the end user, the file should try to follow the form of the existing
names. That is, the name should have the following form, remembering that words
should be spaced out using \_ characters.
\begin{itemize}
\item Begin with \inlinecode{deck\_}.
\item If the block is associated with initial conditions or I/O then there
  should be a code here which details
  when this block is valid. For the existing input deck files this would be
  \subitem \inlinecode{ic\_} for the initial conditions file.
  \subitem \inlinecode{eio\_} for the extended I/O file.
\item The next element should be the name of the block followed by \_.
  i.e. \inlinecode{control\_}.
\item The final element should just be the work \inlinecode{block}.
\end{itemize}

\subsubsection{Adding elements to existing blocks}
The existing blocks in the code are read in the following files
\begin{itemize}
\item \inlinecode{deck\_boundaries\_block.f90} - Input deck boundary block.
\item \inlinecode{deck\_constant\_block.f90} - Sets constants in all deck files.
\item \inlinecode{deck\_control\_block.f90} - Input deck control block.
\item \inlinecode{deck\_deo\_block.f90} - Deferred Evaluation Objects in all
  deck files.
\item \inlinecode{deck\_eio\_dist\_fn\_block.f90} - dist\_fn blocks.
\item \inlinecode{deck\_eio\_particle\_probe\_block.F90} - probe blocks.
\item \inlinecode{deck\_ic\_external\_block.f90} - Initial conditions deck for
  reading species\_external and\linebreak field\_external blocks.
\item \inlinecode{deck\_ic\_fields\_block.f90} - Initial conditions deck field
  blocks.
\item \inlinecode{deck\_ic\_laser\_block.f90} - Initial conditions deck laser
  blocks.
\item \inlinecode{deck\_ic\_species\_block.f90} - Initial conditions deck
  species\{n\} blocks.
\item \inlinecode{deck\_io\_block.F90} - Input deck output blocks.
\item \inlinecode{deck\_species\_block.F90} - Input deck species blocks.
\item \inlinecode{deck\_window\_block.f90} - Input deck window block.
\end{itemize}

The existing structure of the blocks is simple enough in most cases that it
should be fairly easy to add new elements if needed. The most likely change
needed is to change the list of variables to dump in the \inlinecode{output}
block. How to do this is detailed in the section on {\EPOCH} I/O.

\subsection{Permanently adding functions, constants to {\EPOCH}}
In much the same way that it is possible to permanently add new blocks to the
input deck, it is also possible to permanently add functions and constants to
{\EPOCH}'s maths parser. Although adding new operators is possible, it is
sufficiently likely to cause problems with the operation of the maths parser
that it is formally not recommended by the author of the program, and hence is
not documented here.

\subsubsection{Adding the new tokenizer handle}
When adding a new function or constant to the maths parser using the temporary
routines, there are two calls (\inlinecode{register\_function} and
\inlinecode{register\_constant}) which give a numerical handle. This is the
token used to represent that function or constant after the text has been parsed
(remember that {\EPOCH}'s maths parser tokenizes before evaluation!). When
permanently adding objects to the maths parser, the tokenizer handles have to
be set up manually. This takes place in \inlinecode{src/shared\_data.F90} in
the module \inlinecode{shared\_parser\_data}. There are several lines which
look like:
\begin{boxverbatim}
  INTEGER, PARAMETER :: c_const_ix = 40
  INTEGER, PARAMETER :: c_const_iy = 41
  .
  .
  .
  INTEGER, PARAMETER :: c_func_interpolate = 22
  INTEGER, PARAMETER :: c_func_tanh = 23
\end{boxverbatim}
Constants beginning with \inlinecode{c\_const\_} are tokenizer handles for
constants, and those beginning with \inlinecode{c\_func\_} are tokenizer handles
for functions. Each number must be unique and has to be less than
the lower bound of values reserved for temporary or deck
specified values. This means that any tokenizer handle for a function has to be
less than the value of the variable \inlinecode{c\_func\_custom\_lowbound} and
any handle for a constant must be less than
\inlinecode{c\_const\_deck\_lowbound}. It is acceptable to simply increase the
value of \inlinecode{c\_func\_custom\_lowbound} and
\inlinecode{c\_const\_deck\_lowbound} to
allow the use of more values for internal constants and functions, although
care should be taken since this could lead to performance issues.
If \inlinecode{c\_const\_deck\_lowbound} is increased then the constant
\inlinecode{c\_constant\_custom\_lowbound} should be increased by the same
amount (the values between \inlinecode{c\_const\_deck\_lowbound} and
\inlinecode{c\_constant\_custom\_lowbound-1} are used for constants specified
inside the input deck while values greater than or equal to
\inlinecode{c\_constant\_custom\_lowbound} are used for constants specified
by \inlinecode{register\_constant}.

Once the tokenizer handle is specified in \inlinecode{shared\_parser\_data}, it
is now possible to extend the main areas of the maths parser. Note that from
here on, you MUST always use the constant named handle, NEVER the numerical
value that you specified for the value of the handle. If this is not done
then combining functions and constants from several sources becomes much harder.

\subsubsection{Adding the new function or constant to the tokenizer}
The next stage is to add the string representation of your constant or function
to the tokenizer routines in
\inlinecode{src/parser/tokenizer\_blocks.F90}. This is very simple to do, just
find either the function \inlinecode{as\_constant} or \inlinecode{as\_function}
and look at the existing code. These functions are just long lists of
\inlinecode{str\_cmp} commands followed by code to deal with custom
functions/constants. To add the new code, create an additional line such as:
\begin{boxverbatim}
  IF (str_cmp(name, "my_const")) as_constant = c_const_my_const
  .
  .
  .
  IF (str_cmp(name, "my_func"))  as_function = c_func_my_func
\end{boxverbatim}
Note that neither routine returns immediately after recognising the name of the
function/constant. This allows users to override built in constants or
functions with custom versions using \inlinecode{register\_constant} and
\inlinecode{register\_function}. This is not significant, since tokenizing
should never be used in a speed critical part of the code.

\subsubsection{Implementing the function or constant in the evaluator}
The evaluator is the part of the code that actually takes the streams of tokens
produced by the tokenizer and evaluates them into a number. The relevant parts
of the evaluator for adding new constants or functions are in
\inlinecode{src/parser/evaluator\_blocks.F90} and the functions which may need
changing are \inlinecode{do\_constant} and \inlinecode{do\_functions}. These are
both passed up to five parameters:
\begin{itemize}
\item #INTEGER :: opcode# - The operation code, this is the tokenizer handle
  which was defined in \inlinecode{shared\_parser\_data}.
\item #INTEGER :: ix, iy, iz# - The position of the current evaluation in the
  domain. If your function or constant behaves differently at different points
  in space then you should use these parameters to reference the correct point
  of an array.
\item #INTEGER :: errcode# - This should be set to an error code from
  Appendix A, usually\linebreak \inlinecode{c\_err\_bad\_value} if for some
  reason it is not possible to evaluate your constant or function.
\end{itemize}

The rest of the routine to set a constant is as simple as testing for the
tokenizer handle already set up in \inlinecode{shared\_parser\_data} and then
calling the subroutine \inlinecode{push\_on\_eval}. This pushes the final
constant onto the evaluation stack which is used by the RPN parser. The basic
sequence for functions is similar except for the addition of a code to read
the values that the function takes. This is again the subroutine
\inlinecode{get\_values} which is also used in custom\_function. The calling
sequence in \inlinecode{do\_function} looks like:
\begin{boxverbatim}
  IF (opcode .EQ. c_func_gauss) THEN
    CALL get_values(3, values)
    CALL push_on_eval(EXP(-((values(1)-values(2))/values(3))**2))
    RETURN
  ENDIF
\end{boxverbatim}
Simply call the \inlinecode{get\_values} subroutine, passing the number of
required parameters and an array of type \inlinecode{REAL(num)} which is at
least as long as the number of required parameters. The array is populated
by the values passed into the function. Constants and maths expressions are
already evaluated by the time that this section of code is reached, so there is
no need to deal with further parsing. Next, simply call
\inlinecode{push\_on\_eval} to push the result of your function onto the
evaluation stack.

\section{Developing an extension to {\EPOCH}}

Exactly how to extend {\EPOCH} depends heavily upon what you intend to add. The
simplest things to add are new diagnostics, and this is detailed in
the section on {\EPOCH} I/O. Other places where changes are likely to be made
are the following:
\begin{itemize}
\item The field solver - Changing the field solver to add new laser-like
  boundaries, add spatial smoothing to remove noise, add high order field
  solvers etc.
\item The particle pusher - Change the basic physical model of {\EPOCH} by
  modifying the particle pusher.
\item The boundary routines - Add new boundary conditions or modify existing
  boundary conditions.
\item The laser boundary routines - Add new features to the laser boundaries in
  this routine.
\item The main driver (\inlinecode{epoch\{n\}d.F90}) - This is the routine
  where the main calling sequence of {\EPOCH} is setup, and totally new
  extensions to {\EPOCH} should be placed in here.
\end{itemize}

Changing the field solver, the particle pusher or boundary routines
is fairly easy to accomplish by reading the section of this manual that
details the relevant code. The general sequence for writing an extension
would be:
\begin{itemize}
\item Add any new global variables needed to \inlinecode{shared\_data.F90}.
\item Add the meat of your change to the code.
\item Test the changes to your code. Make absolutely sure that you can turn your
  change to the code off.
\item Add controls for your extension to the input deck reader.
\end{itemize}

\subsection{The main driver routine}
When adding completely new routines to the code, they should be added to the
file \inlinecode{src/epoch\{n\}d.F90}. This routine simply calls other routines
to perform the actual execution of the code. The first section of the code
controls basic setup, MPI initialisation and
initial conditions. If you wish to add new startup conditions then
you should find the location in this routine where the
initial conditions are setup. The code is fairly complicated, but
there are a few key points at which the code significantly changes state.
\begin{itemize}
\item After the call to \inlinecode{read\_deck} the code has read the basic
  information from the input deck files and any tests or changes which have to
  be made to input deck values should be made immediately after this line. Note
  that although the variables from the deck have been set,
  none of these values have been used so allocatable
  variables have yet to be allocated. The grid does not exist at this point.
\item After the call to \inlinecode{open\_files} the code has finished
  allocating all field variables, although particles may not yet have been set
  up. The grid now exists.
\item There are now a series of \inlinecode{IF} statements which test for
  things like \linebreak\inlinecode{IF (IOR(ictype, c\_ic\_autoload) .NE. 0)}.
  These are the lines which test for all possible states of the
  initial conditions. The last test is for the manual load routine
  (\inlinecode{c\_ic\_manual}). After this test all the particles have been
  loaded and are now on their correct processor. The load balancer has now been
  called at least once so the domains may no longer be identical.
\item The main loop is a simple do loop beginning with just the single command
  \inlinecode{DO}. Inside this loop there are several calls to routines which
  actually advance the system. Most routines which can change currents should
  take place after the particle pusher but before the final update for the $E$
  and $B$ fields. These routines are
  \subitem \inlinecode{set\_dt} - This routine sets the timestep.
  \subitem \inlinecode{update\_eb\_fields\_half} - Time centre the $E$ and $B$
    fields.
  \subitem \inlinecode{push\_particles} - The particle pusher.
  \subitem \inlinecode{reorder\_particles\_to\_grid} - Groups particles into
    linked lists at each grid point. Used for the particle splitting routine,
    and would be the right place to add a collision operator. Any routine
    which needs to have nearby particles grouped together should take place
    after the call to this routine.
  \subitem \inlinecode{split\_particles} - The very early beta particle
    splitting operator. Doesn't really work yet, so do not use!
  \subitem \inlinecode{reattach\_particles\_to\_mainlist} - Undoes the
    particle grouping and rebuilds the main list of particles used by the
    particle pusher. Any routine which needs to have nearby particles grouped
    together should take place before the call to this routine.
  \subitem \inlinecode{update\_eb\_fields\_final} - Updates the $E$ and $B$
    fields to the full timestep.
\item After the call to \inlinecode{update\_eb\_fields\_final} the code is
  ready for another timestep. Any routines which do not change the time
  integrated properties of the code (like the moving window) should come after
  this call.
\end{itemize}

\subsection{The particle reordering routine}
If the code is compiled with the right flags then during the main loop the
particles are grouped into a linked list for each cell in the domain, with all
the particles which are in that cell linked into the list. The main list
\inlinecode{species(ispecies)\%attached\_list} is empty and cannot be used
during this period. The particles should now be accessed using the variable
\inlinecode{species(ispecies)\%secondary\_list(ix,iy,iz)} which is the array of
linked lists. This array is allocated on the call to
\inlinecode{reorder\_particles\_to\_grid} and deallocated on the call to
\inlinecode{reattach\_particles\_to\_mainlist}, and should not be used outside
the section of code between these two calls. The particles themselves remain
unchanged. No attempt is made to check that particles do not cross processor
boundaries in this section, so if a particle's position is changed, it is up to
the user to ensure that the particle is transferred to another processor if
required. However, if a particle is transferred to another processor, it is
acceptable to relink it to \inlinecode{species(ispecies)\%attached\_list} since
the other lists are simply appended to that list when the particles are
reattached to the main list.\\

You should not write an extension to the code which requires the use of the
particle reordering routines if it can be avoided since these routines
have a significant impact on performance.

\pagebreak

\appendix
  \begin{center}
    {\bf APPENDICES}
  \end{center}
\section{Error Codes}
The input deck and maths parser in {\EPOCH} use various named error codes to
report on errors which occur during the evaluation of the input deck. These
codes are
\begin{itemize}
\item c\_err\_none - No error. Set an error code to c\_err\_none to state that
  no error has occurred.
\item c\_err\_unknown\_block - In the input deck a block has been found which is
  not known. This should be returned in \inlinecode{handle\_custom\_block} if
  it is passed any block that it has not been written to handle.
\item c\_err\_unknown\_element - In the input deck an element of a valid block
  has been found which is not known. This should be returned in
  \inlinecode{handle\_custom\_block} if an element is requested which is
  unknown.
\item c\_err\_preset\_element - An element of the input deck has already been
  set and is being set again. Usually this is an indication of a malformed input
  deck file, so \inlinecode{handle\_custom\_block} should try to
  identify such situations and return this error message if
  subsequent attempts to set the variable are being ignored.
\item c\_err\_preset\_element\_use\_later - An element of the input deck has
  already been set and is being set again. Usually this is an indication of a
  malformed input deck file, so \inlinecode{handle\_custom\_block} should try to
  identify such situations and return this error message if
  the subsequent attempts to set the variable override previous ones.
\item {\bf c\_err\_bad\_value} - A value which is being evaluated for the right
  hand side of an element assignment is in some way invalid. Internally to the
  code this usually means that a string which must be interpreted as a maths
  expression or numerical constant is in some way malformed. It is also
  acceptable to return this error code when a value has been passed which is
  invalid for some other reason (the value is outside an acceptable range, etc.)
\item {\bf c\_err\_missing\_elements} - This is an error code returned when the
  code is testing to make sure that all necessary elements of an input deck
  file have been specified. It should be returned when some required parameter
  is missing in the subroutine \inlinecode{check\_custom\_blocks}.
\item c\_err\_terminate - This error code means that the code is in a state
  where execution is impossible and the code must terminate once the input deck
  has been read. Some other error codes automatically set c\_err\_terminate,
  but it can always be IOR'ed with any error code to force the code to exit.
  Note that just returning c\_err\_terminate will cause the code to
  silently quit.
\item {\bf c\_err\_required\_element\_not\_set} - This means that the code
  cannot parse an input deck element since another element which must be known
  beforehand has not been set. This is intended for things like setting the
  species information where the number of species must be known in
  advance. This error code uses the extended error string to give user friendly
  feedback. If you return this error code then you should set
  extended\_error\_string to be equal to the name of the required element which
  has not been set. If multiple previous elements are required then the code
  should be set up so that it checks for the presence of the required elements
  in order and reports on missing elements so that the end user can fix them
  one by one.
\item c\_err\_pp\_options\_wrong - If you've written a section of the code that
  is controlled by preprocessor options then you should return this
  error message if someone attempts to set input deck elements which refer to
  that part when the correct preprocessor options are not used. This means that
  the user is aware of the fact that the requested feature will not be
  active. This error code also uses the extended error string to give user
  friendly feedback. If you return this error code, you should set the string
  extended\_error\_string to the define command that would turn on the
  requested feature of the code (``-DPER\_PARTICLE\_WEIGHT'', for example).
\item {\bf c\_err\_other} - This error code is a catch all error which causes
  the code to quit with a sarcastic error message. It's mainly intended for
  debugging and is used before the final error code is implemented.
\end{itemize}

Those error codes with names in {\bf BOLD} are those which will cause the code
to terminate.

\end{document}
